from django.core.exceptions import ObjectDoesNotExist
from django.shortcuts import get_object_or_404
from django.http import JsonResponse, HttpResponse, HttpResponseNotFound
from django.db.models import Avg, Count, Sum, Case, When, F
from django.db.models.functions import ExtractWeek, ExtractMonth, ExtractYear
from django.contrib.auth import authenticate, login
from django.views.decorators.csrf import csrf_exempt
from django.utils.timezone import now
from rest_framework.views import APIView
# from rest_framework.response import Responsebot
from rest_framework import status
from .serializers import *
from .models import *
from django.core import serializers
from django.utils import timezone 
from collections import Counter, defaultdict
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
import datetime
from datetime import datetime, timedelta
from datetime import time as datetime_time
import time
import os
import http.client
import urllib.parse
from backtesting import Backtest, Strategy
from backtesting.lib import crossover, resample_apply
import asyncio
from backtesting.test import SMA, GOOG, EURUSD
import pandas as pd
# import patch_pandas_ta
import numpy as np
np.NaN = np.nan

# import pandas_ta as ta
# import MetaTrader5 as mt
from datetime import datetime
# from matplotlib import pyplot as plt
import yfinance as yf
import base64
import requests
import ast
import bokeh
from bokeh.io import export_png
from bokeh.plotting import output_file, save
from bokeh.embed import file_html
from bokeh.resources import CDN
from bokeh.embed import json_item
from asgiref.sync import sync_to_async
# import cv2
import matplotlib.pyplot as plt
from mplfinance.original_flavor import candlestick_ohlc
import matplotlib.dates as mdates
import mplfinance as mpf
from scipy.signal import argrelextrema, find_peaks
from sklearn.neighbors import KernelDensity
import pytz
import openai
# from openai import OpenAI
from django.utils import timezone
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
import cot_reports as cot
import seaborn as sns
import io
from twilio.rest import Client
import zipfile
import json
from dataclasses import dataclass
from typing import List, Optional, Dict, Any, Tuple
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle, Arrow
import pandas as pd
import re
from typing import Tuple
import numpy as np

import matplotlib.pyplot as plt
import datetime
import yfinance as yf
import mplfinance as mpf
import http.client
import urllib.parse
import urllib.request
import base64
import requests

import logging
from django.views.decorators.http import require_http_methods



from PIL import Image
import io

from dataclasses import dataclass



# Comment
# current_hour = datetime.datetime.now().time().hour



scheduler = BackgroundScheduler()
scheduler.start()


def new_york_session():
    now = datetime.now(pytz.timezone('America/New_York')).time()
    return datetime_time(8, 0) <= now <= datetime_time(17, 0)

def london_session():
    now = datetime.now(pytz.timezone('Europe/London')).time()
    return datetime_time(8, 0) <= now <= datetime_time(17, 0)

def asian_session():
    tokyo_now = datetime.now(pytz.timezone('Asia/Tokyo')).time()
    hong_kong_now = datetime.now(pytz.timezone('Asia/Hong_Kong')).time()
    return (datetime_time(8, 0) <= tokyo_now <= datetime_time(17, 0)) or (datetime_time(8, 0) <= hong_kong_now <= datetime_time(17, 0))


@csrf_exempt
def zinaida_feedback_form(request):
    try:

        if request.method == "POST":
            try:
                data = json.loads(request.body)
                feedback = data.get("feedback")
                if not feedback:
                    return JsonResponse({"message": "Feedback cannot be empty."}, status=400)
                
                FeedbackForm.objects.create(feedback=feedback)
                return JsonResponse({"message": "Feedback submitted successfully."}, status=201)
            except Exception as e:
                return JsonResponse({"message": f"An error occurred: {str(e)}"}, status=500)
        return JsonResponse({"message": "Invalid request method."}, status=405)
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return JsonResponse({"message": f"An error occurred: {str(e)}"}, status=500)



def is_bullish_run(candle1, candle2, candle3, candle4):
    if candle2.Close > candle1.Close and candle3.Close > candle2.Close and candle4.Close > candle3.Close:
        return True
    return False


def is_bearish_run(candle1, candle2, candle3, candle4):
    if candle2.Close < candle1.Close and candle3.Close < candle2.Close and candle4.Close < candle3.Close:
        return True
    return False


def is_bullish_run_3(candle1, candle2, candle3):
    if candle2.Close > candle1.Close and candle3.Close > candle2.Close:
        return True
    return False


def is_bearish_run_3(candle1, candle2, candle3):
    if candle2.Close < candle1.Close and candle3.Close < candle2.Close:
        return True
    return False


def is_bearish_candle(candle):
    if candle.Close < candle.Open:
        return True
    return False


def is_bullish_candle(candle):
    if candle.Close > candle.Open:
        return True
    return False


# def get_openai_key(request):
#     return JsonResponse({'OPENAI_API_KEY': os.environ['OPENAI_API_KEY']})


def get_news_data():
    # Get the current date
    news_data = []
    current_date = datetime.datetime.now().date()
    day_abbreviation = current_date.strftime('%a')
    day_encountered = False
    all_days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
    next_day = current_date + datetime.timedelta(days=1)
    next_day_found = False
    # Format the next day's date to get the abbreviated day name
    next_day_abbreviated = next_day.strftime('%a')
    try:
        # Check if there's existing data for the current date
        existing_news_data = NewsData.objects.filter(created_at__date=current_date)
        
        if existing_news_data.exists():
            # Fetch and return the existing news data
            existing_data = existing_news_data.first().data
            return existing_data

        chrome_options = Options()
        chrome_options.add_argument("--headless")
        # chrome_options.headless = True
        service = Service(executable_path='snowAIWeb/chromedriver.exe')
        driver = webdriver.Chrome(service=service)
        driver.get("http://www.forexfactory.com")

        table = driver.find_element(By.CLASS_NAME, "calendar__table")
        # Iterate over each table row
        for row in table.find_elements(By.TAG_NAME, "tr"):
            # list comprehension to get each cell's data and filter out empty cells
            row_data = list(filter(None, [td.text for td in row.find_elements(By.TAG_NAME, "td")]))
            if row_data == []:
                continue
            if len(row_data) > 1:
                if next_day_abbreviated in row_data[0]:
                    next_day_found = True
                if next_day_found:
                    break
                if day_abbreviation in row_data[0] and not next_day_found:
                    day_encountered = True
                if day_encountered:
                    news_data.append(row_data)

        # Save news data to the model
        news_model = NewsData(data=news_data, created_at=current_date)
        news_model.save()

        # driver.close()
        driver.quit()
        return news_data
    except Exception as e:
        print(f'Error: {e}')



class UserRegistrationView(APIView):
    def post(self, request, format=None):
        serializer = UserRegistrationSerializer(data=request.data)
        if serializer.is_valid():
            user = serializer.save()
            return Response(serializer.data, status=status.HTTP_201_CREATED)
        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)


@csrf_exempt
def check_email(request):
    if request.method == "POST":
        email = request.POST.get("email")
        existing_user = User.objects.filter(email=email).exists()
        return JsonResponse({"exists": existing_user})


@csrf_exempt
def all_trades(request, email):
    trades = Trade.objects.filter(email=email)
    serialized_trades = serializers.serialize('json', trades)
    return JsonResponse({'trades': serialized_trades})


class TellUsMoreCreateView(APIView):
    def post(self, request, *args, **kwargs):
        data = request.data
        data['tell_us_more_user'] = request.user.id  # Add the logged-in user's primary key
        # Create a serializer instance
        serializer = TellUsMoreSerializer(data=data)
        # Check if serializer is valid and print any errors
        if serializer.is_valid():
            # Save the serializer instance
            serializer.save()
            return Response(serializer.data, status=status.HTTP_201_CREATED)
        else:
            return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)


class UserLoginView(APIView):
    # @csrf_exempt
    def post(self, request, *args, **kwargs):
        global email_of_user
        try:
            email = request.data.get('email')
            password = request.data.get('password')
            test_user = User.objects.get(email=email)
        
            user = authenticate(request, email=email, password=password)
            
            if user is not None:
                email_of_user = email
                login(request, user)
                return Response({'message': 'Login successful', 'email': email}, status=status.HTTP_200_OK)
            else:
                return Response({'message': 'Invalid login credentials'}, status=status.HTTP_401_UNAUTHORIZED)
        except User.DoesNotExist:
            return Response({'message': 'No account with that email'}, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({'message': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class TradeView(APIView):
    def post(self, request, *args, **kwargs):
        # email = request.data.get('email')
        data = request.data
        email = data['email']
        initial_capital = TellUsMore.objects.get(user_email=email).initial_capital
        asset = data['asset']
        order_type = data['order_type']
        strategy = data['strategy']
        position_size = data['position_size']
        timeframe = data['timeframe']
        entry_date = data['start_date']
        exit_date = data['end_date']
        entry_point = data['entry_point']
        stop_loss = data['stop_loss']
        take_profit = data['take_profit']
        exit_point = data['exit_point']
        outcome = data['outcome']
        amount = data['amount']
        emotional_bias = data['emotional_bias']
        reflection = data['reflection']
        if outcome == 'Loss':
            amount = -amount
        roi = (amount/initial_capital) * 100
        new_trade = Trade(
            email = email,
            asset=asset,
            order_type=order_type,
            strategy=strategy,
            lot_size=position_size,
            roi=roi,
            timeframe=timeframe,
            entry_date=entry_date,
            exit_date=exit_date,
            entry_point=entry_point,
            stop_loss=stop_loss,
            take_profit=take_profit,
            exit_point=exit_point,
            outcome=outcome,
            amount=amount,
            emotional_bias=emotional_bias,
            reflection=reflection
        )
        new_trade.save()
        return Response({''}, status=status.HTTP_200_OK)


@csrf_exempt
def full_trade(request, trade_id):
    try:
        trade = Trade.objects.get(pk=trade_id)
        trade_data = {
            "model": "snowAIWeb.trade",
            "pk": trade_id,
            "fields": {
                "email": trade.email,
                "asset": trade.asset,
                "order_type": trade.order_type,
                "strategy": trade.strategy,
                "lot_size": trade.lot_size,
                "timeframe": trade.timeframe,
                "entry_date": trade.entry_date,
                "exit_date": trade.exit_date,
                "entry_point": trade.entry_point,
                "stop_loss": trade.stop_loss,
                "take_profit": trade.take_profit,
                "exit_point": trade.exit_point,
                "outcome": trade.outcome,
                "amount": trade.amount,
                "emotional_bias": trade.emotional_bias,
                "reflection": trade.reflection,
                'roi': trade.roi
            }
        }
        return JsonResponse({"trade": trade_data})
    except Trade.DoesNotExist:
        return JsonResponse({"error": "Trade not found"}, status=404)


@csrf_exempt
def user_overview(request, user_email):
    # Journal.objects.all().delete()
    initial_capital = TellUsMore.objects.get(user_email=user_email).initial_capital
    trades = Trade.objects.filter(email=user_email)
    print(f'User is {request.user}')
    
    total_profit = sum(trade.amount for trade in trades)
    equity_amount = initial_capital + total_profit
    roi = (total_profit / initial_capital) * 100 if initial_capital > 0 else 0

    equity_values = [initial_capital]
    for trade in trades:
        equity_values.append(equity_values[-1] + trade.amount)

    peak_value = max(equity_values)
    trough_value = min(equity_values)

    if peak_value > 0:
        maximum_drawdown = ((peak_value - trough_value) / peak_value) * 100
    else:
        maximum_drawdown = 0

    total_trades = Trade.objects.filter(email=user_email).count()
    winning_trades = Trade.objects.filter(email=user_email,outcome='Profit').count()
    losing_trades = Trade.objects.filter(email=user_email, outcome='Loss').count()

    win_rate = (winning_trades / total_trades) * 100 if total_trades > 0 else 0
    loss_rate = (losing_trades / total_trades) * 100 if total_trades > 0 else 0

    if total_trades > 0:
        risk_of_ruin = (losing_trades / total_trades) * 100
    else:
        risk_of_ruin = 0

     # Calculate net profit for each strategy
    strategy_net_profit = {}  # Dictionary to store strategy net profit
    for trade in trades:
        strategy = trade.strategy
        if strategy in strategy_net_profit:
            strategy_net_profit[strategy] += trade.amount
        else:
            strategy_net_profit[strategy] = trade.amount

    # Find the best and worst strategies based on net profit
    best_strategy = max(strategy_net_profit, key=strategy_net_profit.get, default="N/A")
    worst_strategy = min(strategy_net_profit, key=strategy_net_profit.get, default="N/A")

    timeframe_net_profit = {}  # Dictionary to store timeframe net profit
    for trade in trades:
        timeframe = trade.timeframe
        if timeframe in timeframe_net_profit:
            timeframe_net_profit[timeframe] += trade.amount
        else:
            timeframe_net_profit[timeframe] = trade.amount

    # Find the best and worst timeframes based on net profit
    best_timeframe = max(timeframe_net_profit, key=timeframe_net_profit.get, default="N/A")
    worst_timeframe = min(timeframe_net_profit, key=timeframe_net_profit.get, default="N/A")

    current_year = timezone.now().year
     # Calculate the day of most wins and day of most losses
    days_of_wins = [trade.entry_date.weekday() for trade in trades if trade.outcome == 'Profit']
    days_of_losses = [trade.entry_date.weekday() for trade in trades if trade.outcome == 'Loss']

    most_common_day_of_wins = Counter(days_of_wins).most_common(1)
    most_common_day_of_losses = Counter(days_of_losses).most_common(1)

    try:
        day_of_most_losses = datetime.date(current_year, 1, most_common_day_of_losses[0][0] + 1).strftime("%A")
    except:
        day_of_most_losses = 'None'
    try:
        day_of_most_wins = datetime.date(current_year, 1, most_common_day_of_losses[0][0] + 1).strftime("%A")
    except:
        day_of_most_wins = 'None'

    equity_dates = [trade.entry_date.date().strftime('%Y-%m-%d') for trade in trades]
    equity_dates.sort()
    equity_values = []
    true_amount = 0
    current_capital = initial_capital
    for trade in trades:
        equity_values.append(current_capital+trade.amount)
        current_capital += trade.amount
    equity_labels = [f"Trade{i + 1}" for i in range(len(trades))]
    
    return JsonResponse({
        'equity_amount': equity_amount,
        'profit': total_profit,
        'roi': round(roi, 2),
        'maximum_drawdown': maximum_drawdown,
        'risk_of_ruin': risk_of_ruin,
        'win_rate': win_rate,
        'loss_rate': loss_rate,
        'best_strategy': best_strategy,
        'worst_strategy': worst_strategy,
        'best_timeframe': best_timeframe,
        'worst_timeframe': worst_timeframe,
        'day_of_most_wins': day_of_most_wins,
        'day_of_most_losses': day_of_most_losses,
        'equity_over_time_data': equity_values,
        'equity_over_time_labels': equity_labels,
    })


@csrf_exempt
def save_journal(request, user_email):
    if request.method == 'POST':
        current_date = datetime.datetime.now()
        try:
            data = json.loads(request.body)
            content = data.get('content', '')
            tags = data.get('tags', '')  # Extract tags from JSON data

            if content:
                journal = Journals(user_email=user_email, content=content, created_date=current_date, tags=tags)
                journal.save()
                return JsonResponse({'message': 'Journal entry saved successfully.'})
            else:
                return JsonResponse({'error': 'Content cannot be empty.'}, status=400)
        except json.JSONDecodeError:
            return JsonResponse({'error': 'Invalid JSON data.'}, status=400)
    else:
        return JsonResponse({'error': 'Invalid request method.'}, status=405)


@csrf_exempt
def fetch_journals(request, user_email):
    journals = Journals.objects.filter(user_email=user_email)
    journal_data = []

    for journal in journals:
        journal_data.append({
            'id': journal.id,
            'content': journal.content,
            'created_date': journal.created_date.strftime('%Y/%m/%d: %H:%M'),
            'tags': journal.tags,
        })
    return JsonResponse({'journals': journal_data})


@csrf_exempt
def view_journal(request, journal_id):
    journal = Journals.objects.get(pk=journal_id)
    journal_data = {
        'id': journal.id,
        'content': journal.content,
        'created_date': journal.created_date.strftime('%Y/%m/%d %H:%M'),
        'tags': journal.tags,
    }
    return JsonResponse({'journal': journal_data})


@csrf_exempt
def upcoming_news(request, user_email):
    try:
        user_assets = TellUsMore.objects.get(user_email=user_email).main_assets
        data = NewsData.objects.all()
        current_date = datetime.datetime.now().date()
        # Assuming news_data_queryset contains the queryset of NewsData objects
        all_dates = [extract_date(news_obj.created_at) for news_obj in data]
        # Check if there is any NewsData with the given current_date
        if not NewsData.objects.filter(created_at__date=current_date).exists():
            news_data = get_news_data()  # Fetch news data
        else:
            news_data = NewsData.objects.filter(created_at__date=current_date).first().data
        news_data = clean_news_data(news_data)
        return JsonResponse(news_data)
    except ObjectDoesNotExist:
        return JsonResponse({'message': 'User not found'}, status=404)
    except Exception as e:
        print(f'Error: {e}')
        return JsonResponse({'message': str(e)}, status=500)


@csrf_exempt
def extract_date(date):
    timestamp_string = str(date)
    timestamp = datetime.datetime.strptime(timestamp_string, "%Y-%m-%d %H:%M:%S.%f%z")
    date = timestamp.date()
    return date


# Sample news data
@csrf_exempt
def extract_time(news):
    time_pattern = r'(\d{1,2}:\d{2}(?:am|pm))'
    for item in news:
        match = re.search(time_pattern, item)
        if match:
            return match.group(1)
    return None


def clean_news_data(news_data):
    final_dict = {}
    date_time = ''
    for data in news_data:
        for more_data in data:
            if 'am' in more_data or 'pm' in more_data:
                final_dict[more_data] = []
                date_time = more_data
        try:
            final_dict[date_time].append(data)
        except Exception as e:
            print(f'Error: {e}')
    return final_dict


@csrf_exempt
def get_user_data(request, user_email):
    print(request.user)
    try:
        user_data = TellUsMore.objects.get(user_email=user_email)
        data = {
            "user_email": user_data.user_email,
            "trading_experience": user_data.trading_experience,
            "main_assets": user_data.main_assets,
            "initial_capital": user_data.initial_capital,
            "trading_goals": user_data.trading_goals,
            "benefits": user_data.benefits,
        }
        return JsonResponse(data)
    except TellUsMore.DoesNotExist:
        return JsonResponse({"error": "User data not found"}, status=404)


@csrf_exempt
def save_conversation(request, user_email, identifier):
    if request.method == 'POST':
        conversation_data = request.POST.get('conversation_data')  # Assuming you send the data as a POST request
        # Or if you're using JSON in the request body: conversation_data = request.body['conversation_data']
        
        # Check if a conversation with the given identifier already exists
        existing_conversation = Conversation.objects.filter(user_email=user_email, conversation_id=identifier).first()
        if existing_conversation:
            # Update the existing conversation with new conversation data
            existing_conversation.conversation = conversation_data
            existing_conversation.save()
            return JsonResponse({'message': 'Conversation data updated successfully'})
        else:
            # Create a new conversation
            conversation = Conversation.objects.create(user_email=user_email, conversation=conversation_data, conversation_id=identifier)
            return JsonResponse({'message': 'Conversation data saved successfully'})
    else:
        return JsonResponse({'error': 'Invalid request method'})


@csrf_exempt
def fetch_conversations(request, user_email):
    all_conversations = Conversation.objects.filter(user_email=user_email).order_by('-id')
    conversations_data = [{'id': conversation.conversation_id, 'conversation': conversation.conversation} for conversation in all_conversations]
    return JsonResponse({'conversations': conversations_data})


@csrf_exempt
def fetch_conversation(request, conversation_id):
    print(request.user)
    try:
        try: 
            conversation = Conversation.objects.get(conversation_id=conversation_id)
        except:
            conversation = Conversation.objects.filter(conversation_id=conversation_id).first()
        conversation_data = {'id': conversation.conversation_id, 'conversation': conversation.conversation}
        return JsonResponse({'conversations': conversation_data})
    except Exception as e:
        return JsonResponse({'error': f'error occured in fetch converstion func, it is {e}'})


@csrf_exempt
def update_conversation(request, conversation_id):
    if request.method == 'POST':
        conversation = Conversation.objects.get(conversation_id=conversation_id)
        
        # Update the conversation data as needed
        # For example, if you want to update the conversation content
        new_conversation_data = request.POST.get('new_conversation_data')
        conversation.conversation = new_conversation_data
        
        # Save the updated conversation object
        conversation.save()
        
        return JsonResponse({'message': 'Conversation updated successfully'})
    else:
        return JsonResponse({'error': 'Only POST requests are allowed for updating conversations'})


@csrf_exempt
def delete_conversation(request, conversation_id):
    if request.method == 'POST':
        conversation = Conversation.objects.filter(conversation_id=conversation_id).first()
        conversation.delete()
        return JsonResponse({'message': 'Conversation Successfully deleted'})
    else:
        return JsonResponse({'error': 'Could not delete successfully'})


@csrf_exempt
def update_tell_us_more(request, user_email):
    try:
        # Retrieve the existing user data
        current_user_data = TellUsMore.objects.filter(user_email=user_email).first()

        # Extract the new data from the request
        data = json.loads(request.body)

        # Update the existing user data with the new data
        current_user_data.trading_experience = data["trading_experience"]
        current_user_data.main_assets = data["main_assets"]
        current_user_data.initial_capital = data["initial_capital"]
        current_user_data.trading_goals = data["trading_goals"]
        current_user_data.benefits = data["benefits"]

        # Save the updated data
        current_user_data.save()

        return JsonResponse({"message": "Data updated successfully."}, status=200)
    except TellUsMore.DoesNotExist:
        return JsonResponse({"error": "User data not found."}, status=404)
    except Exception as e:
        return JsonResponse({"error": str(e)}, status=500)
    

@csrf_exempt
def update_user_assets(request, user_email):
    if request.method == 'POST':
        try:
            # Retrieve the user's TellUsMore instance
            user_assets = TellUsMore.objects.get(user_email=user_email)

            # Parse the request data if it contains new assets
            request_data = json.loads(request.body)
            new_assets = request_data.get('new_assets', None)
            print(new_assets)

            if new_assets is not None:
                # Update the user's main_assets field with the new assets
                user_assets.main_assets = new_assets
                user_assets.save()

                return JsonResponse({'message': 'User assets updated successfully'})
            else:
                return JsonResponse({'error': 'No new assets provided'}, status=400)
        except TellUsMore.DoesNotExist:
            return JsonResponse({'error': 'User not found'}, status=404)
    else:
        return JsonResponse({'error': 'Invalid request method'}, status=405)


# Set the OpenAI API key globally
openai.api_key = os.environ['OPENAI_API_KEY']

def chat_gpt(prompt):
    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message['content'].strip()



@csrf_exempt
def set_daily_brief_assets(request):
    if request.method == "POST":
        try:
            data = json.loads(request.body)
            selected_assets = data.get('assets', [])

            # Clear existing assets and add the new ones
            DailyBriefAssets.objects.all().delete()
            for asset in selected_assets:
                DailyBriefAssets.objects.create(asset=asset)
            
            return JsonResponse({"message": "Assets updated successfully!"}, status=200)
        except Exception as e:
            return JsonResponse({"error": str(e)}, status=400)
    return JsonResponse({"error": "Invalid request method."}, status=400)


@csrf_exempt
def daily_brief(request):
    if request.method == 'POST':
        try:
            update_daily_brief()
            return JsonResponse({'message': 'Daily Brief Updated Successfully!'})
        except Exception as e:
            return JsonResponse({'message': f'Error Occurred in Daily Brief Function: {e}'})
    else:
        return JsonResponse({'message': 'Invalid request method'}, status=405)

def update_daily_brief():
    try:
        # Fetch all assets from the DailyBriefAssets model
        user_assets = DailyBriefAssets.objects.all()
        
        if not user_assets.exists():
            raise ValueError("No assets found in the DailyBriefAssets model.")

        # Retrieve all asset names into a list
        currency_list = [asset.asset for asset in user_assets]
        
        if not currency_list:
            raise ValueError("Currency list is empty.")

        # Clear all previous entries in the dailyBrief table
        dailyBrief.objects.all().delete()

        news_data_list = []
        model_replies_list = []
        
        # Establish a connection to the API
        conn = http.client.HTTPSConnection('api.marketaux.com')

        # Define query parameters
        params_template = {
            'api_token': 'xH2KZ1sYqHmNRpfBVfb9C1BbItHMtlRIdZQoRlYw',
            'language': 'en',
            'limit': 50,
        }

        for asset in currency_list:
            try:
                # Update the symbol in the query parameters
                params = params_template.copy()
                params['symbols'] = asset

                # Send a GET request
                conn.request('GET', '/v1/news/all?{}'.format(urllib.parse.urlencode(params)))

                # Get the response
                res = conn.getresponse()
                data = res.read()

                # Parse the JSON response
                news_data = json.loads(data.decode('utf-8'))

                # Validate the news data
                if 'data' not in news_data or not news_data['data']:
                    raise ValueError(f"No news data available for asset: {asset}")

                # Iterate through the news articles and save specific fields to the database
                for article in news_data['data']:
                    title = article.get('title', '')
                    description = article.get('description', '')
                    source = article.get('source', '')
                    url = article.get('url', '')
                    highlights = (
                        article['entities'][0].get('highlights', '') if article.get('entities') else ''
                    )

                    # Create a dictionary with the specific fields
                    news_entry_data = {
                        'title': title,
                        'description': description,
                        'source': source,
                        'url': url,
                        'highlights': highlights,
                    }
                    news_data_list.append(news_entry_data)

                # Generate a summary using Livingston (or your GPT model)
                livingston_response = chat_gpt(
                    f'Provide me a fundamental data summary of the news data (in paragraph format) for this asset as if you were a professional trader and analyst: {asset}\nWith this news data for the asset: {news_data_list}'
                )
                model_replies_list.append(livingston_response)

                # Get the current date and time
                now = timezone.now()

                # Save the new daily brief entry for this asset
                daily_brief = dailyBrief(asset=asset, summary=livingston_response, last_update=now)
                daily_brief.save()

            except Exception as e:
                print(f"Error processing asset {asset}: {e}")
                # Log the error but continue processing other assets
                return JsonResponse({'error': f'Error Occurred In Update DB Function: {e}'})

        # Return a success message
        return JsonResponse({'message': 'Daily brief successfully updated.'})

    except Exception as e:
        print(f"Exception occurred in update_daily_brief function: {e}")
        # Corrected JSON response
        return JsonResponse({'message': f"Exception occurred in update_daily_brief function: {e}"})

@csrf_exempt
def get_openai_key(request):
    return JsonResponse({'OPENAI_API_KEY': os.environ['OPENAI_API_KEY']})



@csrf_exempt
def reflections_summary(request, asset):
    try:
        asset = asset.upper()
        reflections_list = []
        asset_reflections = Trade.objects.filter(asset=asset)
        for entry in asset_reflections:
            reflections_list.append(entry.reflection)
        livingston_reflections_summary = chat_gpt(f'Please provide me a short one paragraph summary of my reflections for {asset}. Here is the data: {reflections_list}')
        return JsonResponse({'message': f'{livingston_reflections_summary}'})
    except Exception as e:
        print(f'Error occured in reflections_summary function: {e}')
        return JsonResponse({'message': f'Error occured in reflections_summary function: {e}'})
        

# Schedule the update_daily_brief function to run every hour
scheduler.add_job(
    update_daily_brief,
    trigger=IntervalTrigger(hours=1),
    id='update_daily_brief_job',
    name='Update daily brief every hour',
    replace_existing=True
)




@csrf_exempt
def fetch_daily_brief_data(request):
    if request.method == 'GET':
        daily_briefs = dailyBrief.objects.all().order_by('-last_update')
        data = [
            {
                'asset': brief.asset,
                'summary': brief.summary,
                'last_update': brief.last_update,
            } for brief in daily_briefs
        ]
        return JsonResponse(data, status=200, safe=False)


def save_news_data(assets, user_email):
    try:
        news = News.objects.filter(user_email=user_email).delete()
    except:
        pass
    today = timezone.localtime(timezone.now()).date()
    # List of assets to fetch news data for
    assets_to_fetch = assets

    # Establish a connection to the API
    conn = http.client.HTTPSConnection('api.marketaux.com')

    # Define query parameters
    params_template = {
        'api_token': 'xH2KZ1sYqHmNRpfBVfb9C1BbItHMtlRIdZQoRlYw',
        'langauge': 'en',
        'limit': 50,
    }

    # Iterate through the assets and make API requests
    for asset in assets_to_fetch:
        # Update the symbol in the query parameters
        params = params_template.copy()
        params['symbols'] = asset

        # Send a GET request
        conn.request('GET', '/v1/news/all?{}'.format(urllib.parse.urlencode(params)))

        # Get the response
        res = conn.getresponse()

        # Read the response data
        data = res.read()

        # Decode the data from bytes to a string
        data_str = data.decode('utf-8')

        # Parse the JSON data
        news_data = json.loads(data_str)

        # Iterate through the news articles and save specific fields to the database
        for article in news_data['data']:
            title = article['title']
            description = article['description']
            source = article['source']
            url = article['url']
            highlights = article['entities'][0]['highlights'] if article.get('entities') else ''

            # Create a dictionary with the specific fields
            news_entry_data = {
                'title': title,
                'description': description,
                'source': source,
                'url': url,
                'highlights': highlights,
            }

            # Create a News instance and save it to the database
            news_entry = News(
                user_email=user_email,
                symbol=asset,  # Set the symbol to the current asset
                data=news_entry_data,  # Store the specific fields as JSON data
                day_created=today,  # Use the current datetime as the day_created value
            )
            news_entry.save()
    return JsonResponse({'message': news_data})


@csrf_exempt
def fetch_user_news_data(request, user_email):
    try:
        # # Check if news data for the current day already exists
        today = timezone.localtime(timezone.now()).date()

        # # Check if news data for the current day already exists
        # existing_news = News.objects.filter(user_email=user_email, day_created=today)
        
        # if not existing_news.exists():
        #     # If data for the current day exists, return a message indicating it
        #     save_news_data()
            # return JsonResponse({'message': 'News data for today already exists.'})
        # else:
            # If data for the current day doesn't exist, fetch and save news data
            # return JsonResponse({'message': f'News data for today does not exist {str(today)}'})
        
        # Fetch all news data without using serializers
        news_objects = News.objects.filter(user_email=user_email)
        
        # Create a list of dictionaries representing the model instances
        news_data = []
        for news in news_objects:
            news_data.append({
                "symbol": news.symbol,
                "description": news.data,
                "created_on": news.day_created,
                'today': today,
            })
        
        # # Convert the list to JSON and return it
        return JsonResponse({"news_data": news_data}, safe=False)
    except Exception as e:
        return JsonResponse({"news_data": f"no current news data with exception: {e}"})
    


@csrf_exempt
def update_news_data(request, user_email):
    if request.method == 'POST':
        # Retrieve the array of currencies from the request body
        data = request.POST  # For form-encoded data
        # For JSON data, use request.body and decode it
        # Example for JSON data:
        json_data = json.loads(request.body)
        currencies = json_data.get('currencies', [])

        # Process the currencies array as needed
        # Perform preprocessing or any other operations here
        
        
        # Example currency list: ['EURUSD', 'GBPUSD', 'USDJPY', 'EURGBP']

        save_news_data(currencies, user_email)
        # Send back a JSON response indicating success
        return JsonResponse({'message': f'Data is: {currencies} with type: {type(currencies)}'})

    # Handle other HTTP methods or invalid requests
    return JsonResponse({'message': 'Invalid request'}, status=400)    

@csrf_exempt
async def handle_api_request(type_1, type_2, ma1, ma2, dataframe, backtest_period):

    class SmaCross(Strategy):
        n0 = 18 # Exponential Moving Average
        n1 = 50 # Exponential Moving Average
        n2 = 200 # Simple Moving Average
        equity = 100000
        risk_percentage = 30
        reward_percentage = 100
        # current_price = 0
        reward_ratio = 15
        position_size = 0.01
        current_position = ''
        range = 2
        # 200 SMA
        ma1_type = f'{type_1}_{ma1}'
        # 50 EMA
        ma2_type = f'{type_2}_{ma2}'



        def init(self):
            price = self.data.Close
            self.ma1 = self.I(SMA, price, 50)
            self.ma2 = self.I(SMA, price, 200)
            close = self.data.Close


        def check_moving_averages_for_buy(self, df, range):
            past_10_rows = df[[self.ma2_type, self.ma1_type]].tail(range)
            past_10_rows['Converge'] = past_10_rows[self.ma2_type] < past_10_rows[self.ma1_type]
            past = past_10_rows.tail(1)['Converge'].values[0]
            second_last_row = past_10_rows['Converge'].iloc[-2]
            if past == False and second_last_row == True:
                # print('True')
                return True
            else:
                # print('False')
                return False


        def check_moving_averages_for_sell(self, df, range):
            past_10_rows = df[[self.ma2_type, self.ma1_type]].tail(range)
            past_10_rows['Diverge'] = past_10_rows[self.ma2_type] > past_10_rows[self.ma1_type]
            past = past_10_rows.tail(1)['Diverge'].values[0]
            second_last_row = past_10_rows['Diverge'].iloc[-2]
            # print(past)
            if past == False and second_last_row == True:
                # print('True')
                return True
            else:
                # print('False')
                return False


        def moving_average(self, df):
            if df.tail(1)[self.ma2_type].values[0] > df.tail(1)[self.ma1_type].values[0]:
                # price = self.data.Close[-1]
                # gain_amount = self.reward_percentage * self.equity
                # risk_amount = self.risk_percentage * self.equity
                # tp_level = price + (gain_amount/self.equity)
                # sl_level = price - (risk_amount/self.equity)
                # if self.position:
                #   self.position.close()
                tp_level = self.data.Close[-1] + self.reward_percentage
                sl_level = self.data.Close[-1] - self.risk_percentage
                if self.check_moving_averages_for_buy(df=df, range=self.range):
                    if self.current_position != 'buy':
                        if self.position:
                            self.position.close()
                        self.buy()
                        self.current_position = 'buy'
            elif df.tail(1)[self.ma2_type].values[0] < df.tail(1)[self.ma1_type].values[0]:
                # price = self.data.Close[-1]
                # gain_amount = self.reward_percentage * self.equity
                # risk_amount = self.risk_percentage * self.equity
                # tp_level = price - (gain_amount/self.equity)
                # sl_level = price + (risk_amount/self.equity)
                # if self.position:
                #   self.position.close()
                # if self.current_position != 'sell':
                tp_level = self.data.Close[-1] - self.reward_percentage
                sl_level = self.data.Close[-1] + self.risk_percentage
                if self.check_moving_averages_for_sell(df=df, range=self.range):
                    if self.current_position != 'sell':
                        if self.position:
                            self.position.close()
                        self.sell()
                        self.current_position = 'sell'


        def next(self):
            df = pd.DataFrame({'Open': self.data.Open, 'High': self.data.High, 'Low': self.data.Low, 'Close': self.data.Close, 'Volume': self.data.Volume})
            # df['SMA_200'] = df['Close'].rolling(window=200).mean()
            ma1_type = f'{type_1}_{ma1}'
            ma2_type = f'{type_2}_{ma2}'
            if type_1 == 'SMA':
                df[ma1_type] = ta.sma(df["Close"], length=int(ma1))
            elif type_1 == 'EMA':
                df[ma1_type] = ta.ema(df["Close"], length=int(ma1))
            if type_2 == 'SMA':
                df[ma2_type] = ta.sma(df["Close"], length=int(ma2))    
            elif type_2 == 'EMA':
                df[ma2_type] = ta.ema(df["Close"], length=int(ma2))
            
            try:
                self.moving_average(df)
                # print('Running Algorithm...')
                # self.check_moving_averages_for_buy(df, self.range)
            except Exception as e:
                print(f'Error occured: {e}')
                pass

    return_plot = False
        
    
    if dataframe == '5Min':
        df_to_use = './XAUUSD5M.csv'
    elif dataframe == '15Min':
        df_to_use = './XAUUSD15M.csv'
    elif dataframe == '30Min':
        df_to_use = './XAUUSD30M.csv'
    elif dataframe == '1H':
        df_to_use = './XAUUSD1H.csv'
    elif dataframe == '4H':
        df_to_use = './XAUUSD4H.csv'
        return_plot = True
    elif dataframe == '1D':
        df_to_use = './XAUUSD1D.csv'
        return_plot = True
    
    if backtest_period == '0-25':
        start = 0
        end = 0.25
    elif backtest_period == '25-50':
        start = 0.25
        end = 0.5
    elif backtest_period == '50-75':
        start = 0.5
        end = 0.75
    elif backtest_period == '75-100':
        start = 0.75
        end = 1

        
    df_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), df_to_use)
    df = pd.read_csv(df_path).drop_duplicates()
    df.index = pd.to_datetime(df['Time'].values)
    del df['Time']
    length = int(len(df) * start)
    second_length = int(len(df) * end)
    bt = Backtest(df[length:second_length], SmaCross, exclusive_orders=False, cash=10000)
    output = bt.run()

    if return_plot:

        p = bt.plot()
        
        item = json_item(p, "myplot")
        # print(item)
        
        plot_json = json.dumps(item)
    else:
        plot_json = {}

    # Convert the relevant output fields to a dictionary
    result_dict = {
        "Start": str(output['Start']),
        "End": str(output['End']),
        "Duration": str(output['Duration']),
        "Exposure Time [%]": output['Exposure Time [%]'],
        "Equity Final [$]": output['Equity Final [$]'],
        "Equity Peak [$]": output['Equity Peak [$]'],
        "Return [%]": output['Return [%]'],
        "Buy & Hold Return [%]": output['Buy & Hold Return [%]'],
        "Return (Ann.) [%]": output['Return (Ann.) [%]'],
        "Volatility (Ann.) [%]": output['Volatility (Ann.) [%]'],
        "Sharpe Ratio": output['Sharpe Ratio'],
        "Sortino Ratio": output['Sortino Ratio'],
        "Calmar Ratio": output['Calmar Ratio'],
        "Max. Drawdown [%]": output['Max. Drawdown [%]'],
        "Avg. Drawdown [%]": output['Avg. Drawdown [%]'],
        "Max. Drawdown Duration": str(output['Max. Drawdown Duration']),
        "Avg. Drawdown Duration": str(output['Avg. Drawdown Duration']),
        "# Trades": output['# Trades'],
        "Win Rate [%]": output['Win Rate [%]'],
        "Best Trade [%]": output['Best Trade [%]'],
        "Worst Trade [%]": output['Worst Trade [%]'],
        "Avg. Trade [%]": output['Avg. Trade [%]'],
        "Max. Trade Duration": str(output['Max. Trade Duration']),
        "Avg. Trade Duration": str(output['Avg. Trade Duration']),
        "Profit Factor": output['Profit Factor'],
        "Expectancy [%]": output['Expectancy [%]'],
        "SQN": output['SQN'],
    }
    return result_dict, plot_json


@csrf_exempt
def moving_average_bot(request, type_1, type_2, ma1, ma2, dataframe, backtest_period):

    try:
        MovingAverageBot.objects.all().delete()
        print(f'All deleted successfully!')
    except Exception as e:
        print(f'Exception when deleting is: {e}')
        pass
    new_moving_average_backtest = MovingAverageBot(ma1_type=type_1, ma1=int(ma1), ma2_type=type_2, ma2=int(ma2))
    new_moving_average_backtest.save()

    async def inner():
        result = await handle_api_request(type_1, type_2, ma1, ma2, dataframe, backtest_period)
        return JsonResponse({'Output': result})

    # Run the asynchronous code using the event loop
    loop = asyncio.get_event_loop()
    return loop.run_until_complete(inner())


# https://backend-production-c0ab.up.railway.app/create-bot/sma/ema/200/50


@csrf_exempt
async def handle_api_request_bbands(length, std, dataframe, backtest_period):
    class BBands(Strategy):
        equity = 100000
        risk_percentage = 20
        reward_percentage = 60
        # current_price = 0
        reward_ratio = 15
        position_size = 0.01
        current_position = ''
        upper_band = f'BBU_{length}_{float(std)}'
        middle_band = f'BBM_{length}_{float(std)}'
        bottom_band = f'BBL_{length}_{float(std)}'


        def init(self):
            price = self.data.Close
            self.ma1 = self.I(SMA, price, 10)
            self.ma2 = self.I(SMA, price, 20)


        def bbands(self, df):
            if df.tail(1)['Close'].values[0] >= df.tail(1)[self.upper_band].values[0]:
                price = self.data.Close[-1]
                gain_amount = self.reward_percentage
                risk_amount = self.risk_percentage 
                tp_level = price + gain_amount
                sl_level = price - risk_amount

                # self.buy(sl=sl_level)
                if self.current_position != 'buy':
                    if self.position:
                        self.position.close()
                    self.buy()
                    self.current_position = 'buy'
            elif df.tail(1)['Close'].values[0] <= df.tail(1)[self.bottom_band].values[0]:
                price = self.data.Close[-1]
                gain_amount = self.reward_percentage
                risk_amount = self.risk_percentage 
                tp_level = price - gain_amount
                sl_level = price + risk_amount

                # self.sell(sl=sl_level)
                if self.current_position != 'sell':
                    if self.position:
                        self.position.close()
                    self.sell()
                    self.current_position = 'sell'


        def next(self):
            df = pd.DataFrame({'Open': self.data.Open, 'High': self.data.High, 'Low': self.data.Low, 'Close': self.data.Close, self.upper_band: self.data[self.upper_band], self.bottom_band: self.data[self.bottom_band]})
            # current_close = df['Close']
            # print('1')

            # print(f'first is {self.data.upper_band}')
            current_close = ta.bbands(close=df['Close'], length=int(length), std=int(std), append=True)
            # print(f'current_close is {current_close}')
            # print('2')
            try:
                # print('3')
                # print(f'Running Algorithm...')
                self.bbands(df)
            except Exception as e:
                print(f'Exception is {e}')
                pass

    return_plot = False

    if dataframe == '5Min':
        df_to_use = './XAUUSD5M.csv'
    elif dataframe == '15Min':
        df_to_use = './XAUUSD15M.csv'
    elif dataframe == '30Min':
        df_to_use = './XAUUSD30M.csv'
    elif dataframe == '1H':
        df_to_use = './XAUUSD1H.csv'
    elif dataframe == '4H':
        df_to_use = './XAUUSD4H.csv'
        return_plot = True
    elif dataframe == '1D':
        df_to_use = './XAUUSD1D.csv'
        return_plot = True
    
    if backtest_period == '0-25':
        start = 0
        end = 0.25
    elif backtest_period == '25-50':
        start = 0.25
        end = 0.5
    elif backtest_period == '50-75':
        start = 0.5
        end = 0.75
    elif backtest_period == '75-100':
        start = 0.75
        end = 1

    # comment
    df_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), df_to_use)
    df = pd.read_csv(df_path).drop_duplicates()
    df.index = pd.to_datetime(df['Time'].values)
    del df['Time']
    current_close = ta.bbands(close=df['Close'], length=int(length), std=int(std), append=True)
    upper_band = f'BBU_{length}_{float(std)}'
    middle_band = f'BBM_{length}_{float(std)}'
    bottom_band = f'BBL_{length}_{float(std)}'

    df[upper_band] = current_close[upper_band]
    df[middle_band] = current_close[middle_band]
    df[bottom_band] = current_close[bottom_band]
    length = int(len(df) * start)
    second_length = int(len(df) * end)
    bt = Backtest(df[length:second_length], BBands, exclusive_orders=False, cash=10000)
    output = bt.run()

    if return_plot:

        p = bt.plot()
        
        item = json_item(p, "myplot")
        # print(item)
        
        plot_json = json.dumps(item)
    else:
        plot_json = {}

    # image = bt.plot()
    # Convert the relevant output fields to a dictionary
    result_dict = {
        "Start": str(output['Start']),
        "End": str(output['End']),
        "Duration": str(output['Duration']),
        "Exposure Time [%]": output['Exposure Time [%]'],
        "Equity Final [$]": output['Equity Final [$]'],
        "Equity Peak [$]": output['Equity Peak [$]'],
        "Return [%]": output['Return [%]'],
        "Buy & Hold Return [%]": output['Buy & Hold Return [%]'],
        "Return (Ann.) [%]": output['Return (Ann.) [%]'],
        "Volatility (Ann.) [%]": output['Volatility (Ann.) [%]'],
        "Sharpe Ratio": output['Sharpe Ratio'],
        "Sortino Ratio": output['Sortino Ratio'],
        "Calmar Ratio": output['Calmar Ratio'],
        "Max. Drawdown [%]": output['Max. Drawdown [%]'],
        "Avg. Drawdown [%]": output['Avg. Drawdown [%]'],
        "Max. Drawdown Duration": str(output['Max. Drawdown Duration']),
        "Avg. Drawdown Duration": str(output['Avg. Drawdown Duration']),
        "# Trades": output['# Trades'],
        "Win Rate [%]": output['Win Rate [%]'],
        "Best Trade [%]": output['Best Trade [%]'],
        "Worst Trade [%]": output['Worst Trade [%]'],
        "Avg. Trade [%]": output['Avg. Trade [%]'],
        "Max. Trade Duration": str(output['Max. Trade Duration']),
        "Avg. Trade Duration": str(output['Avg. Trade Duration']),
        "Profit Factor": output['Profit Factor'],
        "Expectancy [%]": output['Expectancy [%]'],
        "SQN": output['SQN'],
    }
    return result_dict, plot_json


@csrf_exempt
def bbands_bot(request, length, std, dataframe, backtest_period):
    async def inner_bband():
        result = await handle_api_request_bbands(length, std, dataframe, backtest_period)
        return JsonResponse({'Output': result})

    # Run the asynchronous code using the event loop
    loop = asyncio.get_event_loop()
    return loop.run_until_complete(inner_bband())


@csrf_exempt
async def handle_api_request_rsi(length, overbought_level, oversold_level, dataframe, backtest_period):
    class RSI(Strategy):
        equity = 100000
        risk_percentage = 20
        reward_percentage = 50
        # current_price = 0
        reward_ratio = 15
        position_size = 0.01
        current_position = ''

        def init(self):
            price = self.data.Close
            self.ma1 = self.I(SMA, price, 10)
            self.ma2 = self.I(SMA, price, 20)


        def rsi(self, df):
            if df.tail(1)['RSI'].values[0] > int(overbought_level):
                price = self.data.Close[-1]
                gain_amount = self.reward_percentage
                risk_amount = self.risk_percentage
                tp_level = price - gain_amount
                sl_level = price + risk_amount
                # self.sell(tp=tp_level, sl=sl_level)
                if self.current_position != 'sell':
                    if self.position:
                        self.position.close()
                    self.sell(sl=sl_level)
                    self.current_position = 'sell'
            elif df.tail(1)['RSI'].values[0] < int(oversold_level):
                price = self.data.Close[-1]
                gain_amount = self.reward_percentage
                risk_amount = self.risk_percentage
                tp_level = price + gain_amount
                sl_level = price - risk_amount
                # self.buy(tp=tp_level,sl=sl_level)
                if self.current_position != 'buy':
                    if self.position:
                        self.position.close()
                    self.buy(sl=sl_level)
                    self.current_position = 'buy'


        def next(self):
            print(f'self.data is {self.data}')
            df = pd.DataFrame({'Open': self.data.Open, 'High': self.data.High, 'Low': self.data.Low, 'Close': self.data.Close, 'RSI': self.data.RSI})
            print(f'df is {df}')
            try:
                self.rsi(df)
            except Exception as e:
                print(f'df is {df}')
                print(f'Exception is {e}')
                pass        

            
    if dataframe == '5Min':
        df_to_use = './XAUUSD5M.csv'
    elif dataframe == '15Min':
        df_to_use = './XAUUSD15M.csv'
    elif dataframe == '30Min':
        df_to_use = './XAUUSD30M.csv'
    elif dataframe == '1H':
        df_to_use = './XAUUSD1H.csv'
    elif dataframe == '4H':
        df_to_use = './XAUUSD4H.csv'
    elif dataframe == '1D':
        df_to_use = './XAUUSD1D.csv'
    
    if backtest_period == '0-25':
        start = 0
        end = 0.25
    elif backtest_period == '25-50':
        start = 0.25
        end = 0.5
    elif backtest_period == '50-75':
        start = 0.5
        end = 0.75
    elif backtest_period == '75-100':
        start = 0.75
        end = 1
                
    df_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), df_to_use)
    df = pd.read_csv(df_path).drop_duplicates()
    df.index = pd.to_datetime(df['Time'].values)
    print(f'No error here')
    del df['Time']
    df['RSI'] = ta.rsi(df['Close'], length = int(length))
    print(f'df 1 is {df}')
    length = int(len(df) * start)
    second_length = int(len(df) * end)
    bt = Backtest(df[length:second_length], RSI, exclusive_orders=False, cash=10000)
    output = bt.run()
    
    # Convert the relevant output fields to a dictionary
    result_dict = {
        "Start": str(output['Start']),
        "End": str(output['End']),
        "Duration": str(output['Duration']),
        "Exposure Time [%]": output['Exposure Time [%]'],
        "Equity Final [$]": output['Equity Final [$]'],
        "Equity Peak [$]": output['Equity Peak [$]'],
        "Return [%]": output['Return [%]'],
        "Buy & Hold Return [%]": output['Buy & Hold Return [%]'],
        "Return (Ann.) [%]": output['Return (Ann.) [%]'],
        "Volatility (Ann.) [%]": output['Volatility (Ann.) [%]'],
        "Sharpe Ratio": output['Sharpe Ratio'],
        "Sortino Ratio": output['Sortino Ratio'],
        "Calmar Ratio": output['Calmar Ratio'],
        "Max. Drawdown [%]": output['Max. Drawdown [%]'],
        "Avg. Drawdown [%]": output['Avg. Drawdown [%]'],
        "Max. Drawdown Duration": str(output['Max. Drawdown Duration']),
        "Avg. Drawdown Duration": str(output['Avg. Drawdown Duration']),
        "# Trades": output['# Trades'],
        "Win Rate [%]": output['Win Rate [%]'],
        "Best Trade [%]": output['Best Trade [%]'],
        "Worst Trade [%]": output['Worst Trade [%]'],
        "Avg. Trade [%]": output['Avg. Trade [%]'],
        "Max. Trade Duration": str(output['Max. Trade Duration']),
        "Avg. Trade Duration": str(output['Avg. Trade Duration']),
        "Profit Factor": output['Profit Factor'],
        "Expectancy [%]": output['Expectancy [%]'],
        "SQN": output['SQN'],
    }
    return result_dict


@csrf_exempt
def rsi_bot(request, length, overbought_level, oversold_level, dataframe, backtest_period):
    # oversold_level = int(oversold_level.remove(f'{length}_'))
    async def inner_rsi():
        # print(f'Length is {length}. Overbought Level is {overbought_level}. Oversold Level is {oversold_level}.')
        result = await handle_api_request_rsi(length, overbought_level, oversold_level, dataframe, backtest_period)
        return JsonResponse({'Output': result})

    # Run the asynchronous code using the event loop
    loop = asyncio.get_event_loop()
    return loop.run_until_complete(inner_rsi())


@csrf_exempt
async def handle_api_request_momentum(dataframe, backtest_period):

    class Momentum(Strategy):
        equity = 100000
        risk_percentage = 20
        reward_percentage = 50
        # current_price = 0
        reward_ratio = 15
        position_size = 0.01
        current_position = ''

        def init(self):
            price = self.data.Close
            self.ma1 = self.I(SMA, price, 10)
            self.ma2 = self.I(SMA, price, 20)


        def momentum(self, df):

            if df.tail(1)['MOM'].values[0] > 80:
                price = self.data.Close[-1]
                gain_amount = self.reward_percentage
                risk_amount = self.risk_percentage
                tp_level = price - gain_amount
                sl_level = price + risk_amount

                # if self.current_position != 'sell':
                if self.position:
                    self.position.close()
                self.sell()
                    # self.current_position = 'sell'
                    # self.sell()

            elif df.tail(1)['MOM'].values[0] < 20:
                price = self.data.Close[-1]
                gain_amount = self.reward_percentage
                risk_amount = self.risk_percentage
                tp_level = price + gain_amount
                sl_level = price - risk_amount
                
                # if self.current_position != 'buy':
                if self.position:
                    self.position.close()
                self.buy()
                # self.current_position = 'buy'
                    # self.buy()


        def next(self):
            df = pd.DataFrame({'Open': self.data.Open, 'High': self.data.High, 'Low': self.data.Low, 'Close': self.data.Close, 'Volume': self.data.Volume})
            df['MOM'] = ta.mom(df['Close'])
            # if not self.position:
            try:
                self.momentum(df)
                # print('Running Backtesting Algorithm...')
            except Exception as e:
                print(f'Exception is {e}')
                pass
    

    if dataframe == '5Min':
        df_to_use = './XAUUSD5M.csv'
    elif dataframe == '15Min':
        df_to_use = './XAUUSD15M.csv'
    elif dataframe == '30Min':
        df_to_use = './XAUUSD30M.csv'
    elif dataframe == '1H':
        df_to_use = './XAUUSD1H.csv'
    elif dataframe == '4H':
        df_to_use = './XAUUSD4H.csv'
    elif dataframe == '1D':
        df_to_use = './XAUUSD1D.csv'
    

    if backtest_period == '0-25':
        start = 0
        end = 0.25
    elif backtest_period == '25-50':
        start = 0.25
        end = 0.5
    elif backtest_period == '50-75':
        start = 0.5
        end = 0.75
    elif backtest_period == '75-100':
        start = 0.75
        end = 1


    df_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), df_to_use)
    df = pd.read_csv(df_path).drop_duplicates()
    df.index = pd.to_datetime(df['Time'].values)
    del df['Time']
    # test_length = int(len(df) * 0.25)
    length = int(len(df) * start)
    second_length = int(len(df) * end)
    bt = Backtest(df[length:second_length], Momentum, exclusive_orders=False, cash=10000)
    output = bt.run()

    p = bt.plot()

    # Convert the plot to HTML
    html = file_html(p, CDN, "my plot")
    
    # Convert the relevant output fields to a dictionary
    result_dict = {
        "Start": str(output['Start']),
        "End": str(output['End']),
        "Duration": str(output['Duration']),
        "Exposure Time [%]": output['Exposure Time [%]'],
        "Equity Final [$]": output['Equity Final [$]'],
        "Equity Peak [$]": output['Equity Peak [$]'],
        "Return [%]": output['Return [%]'],
        "Buy & Hold Return [%]": output['Buy & Hold Return [%]'],
        "Return (Ann.) [%]": output['Return (Ann.) [%]'],
        "Volatility (Ann.) [%]": output['Volatility (Ann.) [%]'],
        "Sharpe Ratio": output['Sharpe Ratio'],
        "Sortino Ratio": output['Sortino Ratio'],
        "Calmar Ratio": output['Calmar Ratio'],
        "Max. Drawdown [%]": output['Max. Drawdown [%]'],
        "Avg. Drawdown [%]": output['Avg. Drawdown [%]'],
        "Max. Drawdown Duration": str(output['Max. Drawdown Duration']),
        "Avg. Drawdown Duration": str(output['Avg. Drawdown Duration']),
        "# Trades": output['# Trades'],
        "Win Rate [%]": output['Win Rate [%]'],
        "Best Trade [%]": output['Best Trade [%]'],
        "Worst Trade [%]": output['Worst Trade [%]'],
        "Avg. Trade [%]": output['Avg. Trade [%]'],
        "Max. Trade Duration": str(output['Max. Trade Duration']),
        "Avg. Trade Duration": str(output['Avg. Trade Duration']),
        "Profit Factor": output['Profit Factor'],
        "Expectancy [%]": output['Expectancy [%]'],
        "SQN": output['SQN'],
    }
    return result_dict, html


@csrf_exempt
def momentum_bot(request, dataframe, backtest_period):
    async def inner_momentum():
        result = await handle_api_request_momentum(dataframe, backtest_period)
        return JsonResponse({'Output': result})

    # Run the asynchronous code using the event loop
    loop = asyncio.get_event_loop()
    return loop.run_until_complete(inner_momentum())


@csrf_exempt
async def handle_api_request_candlesticks(engulfing, pinbar, morning_star, three_white_soldiers, doji_star, methods, dataframe, backtest_period):
    class Strat(Strategy):

        current_day = 0
        equity = 100000
        risk_percentage = 20
        reward_percentage = 50
        # current_price = 0
        reward_ratio = 15
        position_size = 0.01
        # path = '/content/trading-bot/candlestick_chart.png'
        # sr_path  = '/content/sar'
        candlestick_backtrack = 288


        def init(self):
            # super().init()
            # super().set_trailing_sl(3)
            close = self.data.Close
    

        def bullish_engulfing(self, df):

            df_test = df.tail(6)
            df_test = df_test.drop_duplicates()
            test_size = len(df)
            num_engulfing = 0

            for i in range(test_size-1):
                first_candle = df_test.iloc[i-1]
                second_candle = df_test.iloc[i-2]
                third_candle  = df_test.iloc[i-3]
                fourth_candle = df_test.iloc[i-4]
                fifth_candle = df_test.iloc[i-5]
                second_test = first_candle.Close > second_candle.Open

                if is_bearish_candle(second_candle) and is_bullish_candle(first_candle) and second_test == True and is_bearish_run(fifth_candle, fourth_candle, third_candle, second_candle):
                    # num_engulfing += 1
                    # print('Bullish Engulfing')
                    if df.tail(1)['EMA_50'].values[0] > df.tail(1)['SMA_200'].values[0]:
                        # # Set the style of the plot
                        # df.index = pd.to_datetime(df.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')
                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(df.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                        # plt.close(fig)
                        # # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')
                        # # if self.position:
                        # #     self.position.close()

                        # if process_image(self.path) == 2:
                        price = self.data.Close[-1]
                        gain_amount = self.reward_percentage
                        risk_amount = self.risk_percentage
                        tp_level = price + self.reward_percentage
                        sl_level = price - self.risk_percentage

                        # levels = get_fibonacci_levels(df=df.tail(75), trend='uptrend')
                        # thirty_eight_retracement = levels[2]
                        # sixty_one8_retracement = levels[4]
                        # if thirty_eight_retracement <= price <= sixty_one8_retracement:
                            # self.position.close()
                        if self.position:
                            self.position.close()
                        self.buy(tp=tp_level, sl=sl_level)
                break


        def bearish_engulfing(self, df):
            df_test = df.tail(6)
            df_test = df_test.drop_duplicates()
            test_size = len(df)
            num_engulfing = 0

            for i in range(test_size-1):
                first_candle = df_test.iloc[i-1]
                second_candle = df_test.iloc[i-2]
                third_candle  = df_test.iloc[i-3]
                fourth_candle = df_test.iloc[i-4]
                fifth_candle = df_test.iloc[i-5]
                # first_test = first_candle.Open < second_candle.Close
                second_test = first_candle.Close < second_candle.Open

                if is_bullish_candle(second_candle) and is_bearish_candle(first_candle) and second_test == True and is_bullish_run(fifth_candle, fourth_candle, third_candle, second_candle):
                    # num_engulfing += 1
                    # print('Bearish Engulfing')
                    price = self.data.Close[-1]

                    if df.tail(1)['EMA_50'].values[0] < df.tail(1)['SMA_200'].values[0]:
                        # df.index = pd.to_datetime(df.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # # Create the figure object without plotting
                        # fig, axes = mpf.plot(df.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                        # plt.close(fig)
                        # # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')

                        # if self.position:
                        #     self.position.close()
                            # pass
                        # if process_image(self.path) == 0:
                        gain_amount = self.reward_percentage
                        risk_amount = self.risk_percentage
                        tp_level = price - self.reward_percentage
                        sl_level = price + self.risk_percentage
                        # levels = get_fibonacci_levels(df=df.tail(75), trend='downtrend')
                        # thirty_eight_retracement = levels[2]
                        # sixty_one8_retracement = levels[4]
                            # if thirty_eight_retracement <= price <= sixty_one8_retracement:
                            # self.position.close()
                        if self.position:
                            self.position.close()
                        self.sell(tp=tp_level, sl=sl_level)
                break

            
        def bullish_pinbar(self, df):
            dataframe = df.drop_duplicates()
            df = df.tail(1)
            df = df.drop_duplicates()
            test_size = len(df)
            num_pin_bars = 0
            price = self.data.Close[-1]

            for i in range(test_size-1):
                candle = df.iloc[i]
                is_pin_bar = (candle.Close - candle.Low) > 5
                if is_pin_bar:
                    num_pin_bars += 1
                    # print('Bullish Pin Bar')
                    price = self.data.Close[-1]
                    gain_amount = self.reward_percentage
                    risk_amount = self.risk_percentage
                    tp_level = price + gain_amount
                    sl_level = price - risk_amount

                    # dataframe.index = pd.to_datetime(dataframe.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                    # plt.close(fig)
                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')

                    if df.tail(1)['EMA_50'].values[0] > df.tail(1)['SMA_200'].values[0]:
                        self.buy(tp=tp_level, sl=sl_level)


        def bearish_pinbar(self, df):
            dataframe = df.drop_duplicates()
            df = df.tail(1)
            df = df.drop_duplicates()
            test_size = len(df)
            num_pin_bars = 0

            for i in range(test_size-1):
                candle = df.iloc[i]
                is_pin_bar = abs(candle.Close - candle.High) <  5
                if is_pin_bar:
                    num_pin_bars += 1
                    # print('Bearish Pin Bar')
                    price = self.data.Close[-1]
                    gain_amount = self.reward_percentage
                    risk_amount = self.risk_percentage
                    tp_level = price - gain_amount
                    sl_level = price + risk_amount

                    # dataframe.index = pd.to_datetime(dataframe.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                    # plt.close(fig)
                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')

                    if df.tail(1)['EMA_50'].values[0] < df.tail(1)['SMA_200'].values[0]:
                        self.sell(tp=tp_level, sl=sl_level)


        def doji_star(self, df):
            # print('')
            df = df.drop_duplicates()
            dataframe = df
            df = df.tail(5)
            test_size = len(df)
            bullish_doji = 0
            bearish_doji = 0

            for i in range(test_size-4):
                first_prev_candle = df.iloc[i]
                second_prev_candle = df.iloc[i+1]
                third_prev_candle = df.iloc[i+2]
                prev_candle = df.iloc[i+3]
                testing_candle = df.iloc[i+4]
                price = self.data.Close[-1]

                if is_bullish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                    test1 = testing_candle.High - testing_candle.Close
                    test2 = testing_candle.Close - testing_candle.Low
                    if test1 == test2:
                        bullish_doji += 1
                        # print('bullish doji star')
                        price = self.data.Close[-1]
                        gain_amount = self.reward_percentage
                        risk_amount = self.risk_percentage
                        tp_level = price - gain_amount
                        sl_level = price + risk_amount

                        # dataframe.index = pd.to_datetime(dataframe.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                        # plt.close(fig)
                        # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')

                        if df.tail(1)['EMA_50'].values[0] < df.tail(1)['SMA_200'].values[0]:
                            self.sell(tp=tp_level, sl=sl_level)
                elif is_bearish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                    test1 = testing_candle.Open - testing_candle.Close
                    test2 = testing_candle.Close - testing_candle.Low
                    if test1 == test2:
                        bearish_doji += 1
                        price = self.data.Close[-1]
                        gain_amount = self.reward_percentage
                        risk_amount = self.risk_percentage
                        tp_level = price + gain_amount
                        sl_level = price - risk_amount

                        # dataframe.index = pd.to_datetime(dataframe.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                        # plt.close(fig)
                        # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')

                        if df.tail(1)['EMA_50'].values[0] > df.tail(1)['SMA_200'].values[0]:
                            self.buy(tp=tp_level, sl=sl_level)

        def three_white_soldier(self, df):
            # print('')
            dataframe = df.drop_duplicates()
            df = df.drop_duplicates()
            df = df.tail(6)
            test_size = len(df)
            three_white_soldiers = 0
            three_black_crows = 0

            for i in range(test_size-5):
                first_prev_candle = df.iloc[i]
                second_prev_candle = df.iloc[i+1]
                third_prev_candle = df.iloc[i+2]
                prev_candle = df.iloc[i+3]
                testing_candle = df.iloc[i+4]
                testing_candle_2 = df.iloc[i+5]
                price = self.data.Close[-1]

                if is_bearish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                    if testing_candle_2.Close > testing_candle.Close and testing_candle.Close > prev_candle.Close:
                        three_white_soldiers += 1
                        # print('bullish three white soldiers')
                        if df.tail(1)['EMA_50'].values[0] > df.tail(1)['SMA_200'].values[0]:
                            # dataframe.index = pd.to_datetime(dataframe.index)
                            # style = mpf.make_mpf_style(base_mpf_style='classic')

                            # Create the figure object without plotting
                            # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                            # plt.close(fig)
                            # Save the figure to a file
                            # fig.savefig('candlestick_chart.png')
                            # plt.close(fig)
                            # if self.position:
                            #   self.position.close()
                            # if process_image(self.path) == 2:
                            price = self.data.Close[-1]
                            gain_amount = self.reward_percentage
                            risk_amount = self.risk_percentage
                            tp_level = price + self.reward_percentage
                            sl_level = price - self.risk_percentage
                            # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='uptrend')
                            # thirty_eight_retracement = levels[2]
                            # sixty_one8_retracement = levels[4]
                                # if thirty_eight_retracement <= prev_candle.Close <= sixty_one8_retracement:
                                # self.position.close()
                            self.buy(tp=tp_level, sl=sl_level)
                elif is_bullish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                    if testing_candle_2.Close < testing_candle.Close and testing_candle.Close < prev_candle.Close:
                        three_black_crows += 1
                        # print('bearish three black crows')
                        if df.tail(1)['EMA_50'].values[0] < df.tail(1)['SMA_200'].values[0]:
                            # df.index = pd.to_datetime(df.index)
                            # style = mpf.make_mpf_style(base_mpf_style='classic')

                            # Create the figure object without plotting
                            # fig, axes = mpf.plot(df.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                            # plt.close(fig)
                            # Save the figure to a file
                            # fig.savefig('candlestick_chart.png')
                            # plt.close(fig)
                            # if self.position:
                            #   self.position.close()
                            # if process_image(self.path) == 0:
                            price = self.data.Close[-1]
                            gain_amount = self.reward_percentage 
                            risk_amount = self.risk_percentage
                            tp_level = price - self.reward_percentage
                            sl_level = price + self.risk_percentage
                            # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='downtrend')
                            # thirty_eight_retracement = levels[2]
                            # sixty_one8_retracement = levels[4]
                                # if thirty_eight_retracement <= prev_candle.Close <= sixty_one8_retracement:
                                # self.position.close()
                            self.sell(tp=tp_level, sl=sl_level)


        def morning_star(self, df):
            # print('')
            dataframe = df.drop_duplicates()
            df = df.drop_duplicates()
            df = df.tail(6)
            test_size = len(df)
            morning_stars = 0
            evening_stars = 0
            price = self.data.Close[-1]

            for i in range(test_size-5):
                first_prev_candle = df.iloc[i]
                second_prev_candle = df.iloc[i+1]
                third_prev_candle = df.iloc[i+2]
                prev_candle = df.iloc[i+3]
                testing_candle = df.iloc[i+4]
                testing_candle_2 = df.iloc[i+5]

                if is_bearish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                    test = testing_candle.Open - testing_candle.Close
                    if testing_candle_2.Close > testing_candle.Close and 0 < test < 2:
                        morning_stars += 1
                        # print('bullish morning star')
                        price = self.data.Close[-1]
                        gain_amount = self.reward_percentage
                        risk_amount = self.risk_percentage
                        tp_level = price + gain_amount
                        sl_level = price - risk_amount

                        # dataframe.index = pd.to_datetime(dataframe.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)

                        # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')
                        # plt.close(fig)
                        if df.tail(1)['EMA_50'].values[0] > df.tail(1)['SMA_200'].values[0]:
                            # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='uptrend')
                            # thirty_eight_retracement = levels[2]
                            # sixty_one8_retracement = levels[4]
                            # if thirty_eight_retracement <= testing_candle.Close <= sixty_one8_retracement:
                            self.buy(tp=tp_level, sl=sl_level)

                elif is_bullish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                    test = testing_candle.Open - testing_candle.Close
                    if testing_candle_2.Close < testing_candle.Close and 0 < test < 2 and testing_candle.Close < prev_candle.Close:
                        evening_stars += 1
                        # print('bearish morning star')
                        price = self.data.Close[-1]
                        gain_amount = self.reward_percentage
                        risk_amount = self.risk_percentage
                        tp_level = price - gain_amount
                        sl_level = price + risk_amount

                        # dataframe.index = pd.to_datetime(dataframe.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)

                        # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')
                        # plt.close(fig)
                        if df.tail(1)['EMA_50'].values[0] < df.tail(1)['SMA_200'].values[0]:
                            # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='downtrend')
                            # thirty_eight_retracement = levels[2]
                            # sixty_one8_retracement = levels[4]
                            # if thirty_eight_retracement <= testing_candle.Close <= sixty_one8_retracement:
                            self.sell(tp=tp_level, sl=sl_level)

            

        def methods(self, df):
            dataframe = df.drop_duplicates()
            df = df.drop_duplicates()
            df = df.tail(8)
            test_size = len(df)
            rising_methods = 0
            falling_methods = 0
            price = self.data.Close[-1]

            for i in range(test_size-7):
                first_prev_candle = df.iloc[i]
                second_prev_candle = df.iloc[i+1]
                third_prev_candle = df.iloc[i+2]
                prev_candle = df.iloc[i+3]
                testing_candle = df.iloc[i+4]
                testing_candle_2 = df.iloc[i+5]
                testing_candle_3 = df.iloc[i+6]
                final_candle = df.iloc[7]

                if is_bullish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle) and testing_candle.Close < prev_candle.Close and is_bearish_run_3(testing_candle, testing_candle_2, testing_candle_3):
                    if final_candle.Close > prev_candle.Close:
                        rising_methods += 1
                        # print('rising three methods')
                        price = self.data.Close[-1]
                        gain_amount = self.reward_percentage
                        risk_amount = self.risk_percentage
                        tp_level = price + self.reward_percentage
                        sl_level = price - self.risk_percentage

                        # dataframe.index = pd.to_datetime(dataframe.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                        # plt.close(fig)
                        # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')
                        # plt.close(fig)

                        if df.tail(1)['EMA_50'].values[0] > df.tail(1)['SMA_200'].values[0]:
                            # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='uptrend')
                            # thirty_eight_retracement = levels[2]
                            # sixty_one8_retracement = levels[4]
                            # if thirty_eight_retracement <= testing_candle_3.Close <= sixty_one8_retracement:
                            self.buy(tp=tp_level, sl=sl_level)

                elif is_bearish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle) and testing_candle.Close > prev_candle.Close and is_bullish_run_3(testing_candle, testing_candle_2, testing_candle_3):
                    if final_candle.Close < prev_candle.Close:
                        falling_methods += 1
                        # print('falling three methods')
                        price = self.data.Close[-1]
                        gain_amount = self.reward_percentage * self.equity
                        risk_amount = self.risk_percentage * self.equity
                        tp_level = price - self.reward_percentage
                        sl_level = price + self.risk_percentage

                        # dataframe.index = pd.to_datetime(dataframe.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                        # plt.close(fig)
                        # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')
                        # plt.close(fig)

                        if df.tail(1)['EMA_50'].values[0] < df.tail(1)['SMA_200'].values[0]:
                            # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='downtrend')
                            # thirty_eight_retracement = levels[2]
                            # sixty_one8_retracement = levels[4]
                            # if thirty_eight_retracement <= testing_candle_3.Close <= sixty_one8_retracement:
                            self.sell(tp=tp_level, sl=sl_level)
        
        def analyze_candlesticks(self, df):
            if not self.position and engulfing:
                self.bullish_engulfing(df=df)
                self.bearish_engulfing(df=df)
            if not self.position and three_white_soldiers:
                self.three_white_soldier(df=df)
            if not self.position and methods:
                self.methods(df=df)
            if not self.position and morning_star:
                self.morning_star(df=df)
        # self.support_and_resistance(df=df)
            if not self.position and pinbar:
                self.bullish_pinbar(df=df)
                # if not self.position:
                self.bearish_pinbar(df=df)
        # if not self.position:
        #   self.shooting_star(df=df)
            if not self.position and doji_star:
                self.doji_star(df=df)
        # if not self.position:
        # if not self.position:
        #   self.matching(df=df)


        def next(self):
            # super().next()
            # Creating a Pandas DataFrame
            # print(self.data)
            df = pd.DataFrame({'Open': self.data.Open, 'High': self.data.High, 'Low': self.data.Low, 'Close': self.data.Close, 'SMA_200': self.data.SMA_200, 'EMA_50': self.data.EMA_50})
            # df.dropna(inplace=True)
            df = df.fillna(0)
            new_day = self.data.index[-1].day
            mod = new_day % 5
            # print(df)
            # if mod == 0 and self.position:
            #   self.position.close()

            # if self.current_day < new_day and self.position:
            #     self.position.close()
            # self.current_day = new_day
            if not self.position:
                try:
                    self.analyze_candlesticks(df=df)
                except Exception as e:
                    print(f'Error occured here: {e}')
                    pass

    
    if dataframe == '5Min':
        df_to_use = './XAUUSD5M.csv'
    elif dataframe == '15Min':
        df_to_use = './XAUUSD15M.csv'
    elif dataframe == '30Min':
        df_to_use = './XAUUSD30M.csv'
    elif dataframe == '1H':
        df_to_use = './XAUUSD1H.csv'
    elif dataframe == '4H':
        df_to_use = './XAUUSD4H.csv'
    elif dataframe == '1D':
        df_to_use = './XAUUSD1D.csv'
    

    if backtest_period == '0-25':
        start = 0
        end = 0.25
    elif backtest_period == '25-50':
        start = 0.25
        end = 0.5
    elif backtest_period == '50-75':
        start = 0.5
        end = 0.75
    elif backtest_period == '75-100':
        start = 0.75
        end = 1

    
                
    df_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), df_to_use)
    df = pd.read_csv(df_path).drop_duplicates()
    df.index = pd.to_datetime(df['Time'].values)
    del df['Time']
    df["SMA_200"] = ta.sma(df["Close"], length=200)
    df["EMA_50"] = ta.ema(df["Close"], length=50)
    length = int(len(df) * start)
    second_length = int(len(df) * end)
    bt = Backtest(df[length:second_length], Strat, exclusive_orders=False, cash=10000)
    output = bt.run()
    
    # Convert the relevant output fields to a dictionary
    result_dict = {
        "Start": str(output['Start']),
        "End": str(output['End']),
        "Duration": str(output['Duration']),
        "Exposure Time [%]": output['Exposure Time [%]'],
        "Equity Final [$]": output['Equity Final [$]'],
        "Equity Peak [$]": output['Equity Peak [$]'],
        "Return [%]": output['Return [%]'],
        "Buy & Hold Return [%]": output['Buy & Hold Return [%]'],
        "Return (Ann.) [%]": output['Return (Ann.) [%]'],
        "Volatility (Ann.) [%]": output['Volatility (Ann.) [%]'],
        "Sharpe Ratio": output['Sharpe Ratio'],
        "Sortino Ratio": output['Sortino Ratio'],
        "Calmar Ratio": output['Calmar Ratio'],
        "Max. Drawdown [%]": output['Max. Drawdown [%]'],
        "Avg. Drawdown [%]": output['Avg. Drawdown [%]'],
        "Max. Drawdown Duration": str(output['Max. Drawdown Duration']),
        "Avg. Drawdown Duration": str(output['Avg. Drawdown Duration']),
        "# Trades": output['# Trades'],
        "Win Rate [%]": output['Win Rate [%]'],
        "Best Trade [%]": output['Best Trade [%]'],
        "Worst Trade [%]": output['Worst Trade [%]'],
        "Avg. Trade [%]": output['Avg. Trade [%]'],
        "Max. Trade Duration": str(output['Max. Trade Duration']),
        "Avg. Trade Duration": str(output['Avg. Trade Duration']),
        "Profit Factor": output['Profit Factor'],
        "Expectancy [%]": output['Expectancy [%]'],
        "SQN": output['SQN'],
    }
    return result_dict


@csrf_exempt
def candlesticks_bot(request, dataframe, backtest_period):
    async def inner_candlesticks():
        try:
            data = json.loads(request.body)

             # Access the boolean values
            engulfing = data.get('engulfing', False)
            pinbar = data.get('pinbar', False)
            morningStar = data.get('morningStar', False)
            threeWhiteSoldiers = data.get('threeWhiteSoldiers', False)
            dojiStar = data.get('dojiStar', False)
            methods = data.get('methods', False)

            result = await handle_api_request_candlesticks(engulfing=engulfing, 
            pinbar=pinbar, morning_star=morningStar, three_white_soldiers=threeWhiteSoldiers, 
            doji_star=dojiStar, methods=methods, dataframe=dataframe, backtest_period=backtest_period)
            return JsonResponse({'Output': result})
        except Exception as e:
            return JsonResponse({'Error': str(e)})

    # Run the asynchronous code using the event loop
    loop = asyncio.get_event_loop()
    return loop.run_until_complete(inner_candlesticks())


def check_moving_averages_for_buy(df, range, ma1_type, ma1, ma2_type, ma2):
    first_ma = f'{ma1_type}_{ma1}'
    second_ma = f'{ma2_type}_{ma2}'
    past_10_rows = df[[second_ma, first_ma]].tail(range)
    past_10_rows['Converge'] = past_10_rows[second_ma] < past_10_rows[first_ma]
    past = past_10_rows.tail(1)['Converge'].values[0]
    second_last_row = past_10_rows['Converge'].iloc[-2]
    print(past_10_rows)
    if past == False and second_last_row == True:
                # print('True')
        return True
    else:
                # print('False')
        return False


def check_moving_averages_for_sell(df, range, ma1_type, ma1, ma2_type, ma2):
    first_ma = f'{ma1_type}_{ma1}'
    second_ma = f'{ma2_type}_{ma2}'
    past_10_rows = df[[second_ma, first_ma]].tail(range)
    past_10_rows['Diverge'] = past_10_rows[second_ma] > past_10_rows[first_ma]
    past = past_10_rows.tail(1)['Diverge'].values[0]
    second_last_row = past_10_rows['Diverge'].iloc[-2]
    # print(past)
    if past == False and second_last_row == True:
        # print('True')
        return True
    else:
        # print('False')
        return False
    

@csrf_exempt
def moving_average(df, ma1_type, ma1, ma2_type, ma2):
        range = 2
        # Higher MA
        first_ma = f'{ma1_type}_{ma1}'
        second_ma = f'{ma2_type}_{ma2}'
        if ma1_type == 'SMA':
            df[first_ma] = ta.sma(df['Close'], length=int(ma1))
        else:
            df[first_ma] = ta.ema(df['Close'], length=int(ma1))
        # Lower MA
        if ma2_type == 'SMA':
            df[second_ma] = ta.sma(df['Close'], length=int(ma2))
        else:
            df[second_ma] = ta.ema(df['Close'], length=int(ma2))
        # 1 represents 'BUY'
        # -1 represents 'SELL'
        # 0 represents 'DO NOTHING'

        # already_sell = None
        # already_buy = None
        # try:
        #     already_sell = mt.positions_get()[0]._asdict()['type'] == 1
        # except:
        #     pass
        
        # try:
        #     already_buy = mt.positions_get()[0]._asdict()['type'] == 0
        # except:
        #     pass

        if df.tail(1)[second_ma].values[0] > df.tail(1)[first_ma].values[0]:
        
            if check_moving_averages_for_buy(df=df, range=range, ma1_type=ma1_type, ma1=ma1, ma2_type=ma2_type, ma2=ma2):
                # if already_sell:
                # close_order(ticker, lot_size, buy_order_type, buy_price) # NB
                # time.sleep(1)
                return 1
            return 0
               
        elif df.tail(1)[second_ma].values[0] < df.tail(1)[first_ma].values[0]:
            # print('2')
            # if open_positions is not None:
            #     print('7.0')
            #     close_order(ticker, lot_size, sell_order_type,  sell_price)
            #     print('7')
            # position = 'sell'
            if check_moving_averages_for_sell(df=df, range=range, ma1_type=ma1_type, ma1=ma1, ma2_type=ma2_type, ma2=ma2):
                # if already_buy:
                    # close_order(ticker, lot_size, sell_order_type, sell_price)
                    # time.sleep(1)
                # create_order(ticker, lot_size, sell_order_type, sell_price, sell_sl, sell_tp)
                return -1
            return 0
        else:
            return 0


@csrf_exempt
def api_call(request, asset): 
    # return JsonResponse({"message": "API Call Works!"})  
    # timeframe = timeframe.lower()
    test_variable = MovingAverageBot.objects.all()[0]
    ma1_type = str(test_variable.ma1_type)
    ma2_type = str(test_variable.ma2_type)
    ma1 = str(test_variable.ma1)
    ma2 = str(test_variable.ma2)
    try:
        end_date = (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")

        # Calculate the date 30 days ago from the current day
        start_date = (datetime.now() - timedelta(days=10)).strftime("%Y-%m-%d")

        # Download data using the calculated dates
        forex_asset = f"{asset}=X"
        data = yf.download(forex_asset, start=start_date, end=end_date, interval="15m")

        moving_average_output = moving_average(df=data, ma1_type=ma1_type, ma1=ma1, ma2_type=ma2_type, ma2=ma2)
            
        output = f'{moving_average_output}'
        # output = f"{data}"
        return JsonResponse({'message': output})

    except Exception as e:
        return JsonResponse({'message': f'Error: {e}'})


@csrf_exempt
def api_test(request):
    return JsonResponse({"message": "Hello World"})


@csrf_exempt
def new_test(request):
    return JsonResponse({"message": "Heyo!"})


@csrf_exempt
def download_mq4_file(request, bot):
    # Replace with the actual path to your .mq4 file
    if bot == 'trading-model':
        location = './trading-model.ex5'
    elif bot == 'risk-bot':
        location = './Risk-Bot-V2.ex5'
    file_location = os.path.join(os.path.dirname(os.path.realpath(__file__)), location)

    try:
        with open(file_location, 'rb') as f:
            file_data = f.read()

        # Create an HTTP response with the file content
        response = HttpResponse(file_data, content_type='application/octet-stream')
        response['Content-Disposition'] = 'attachment; filename="risk-bot.ex5"'
        return response
    except FileNotFoundError:
        # Handle file not exist case
        return HttpResponseNotFound('<h1>File not found</h1>')


@csrf_exempt
def encode_image(image_path):
    try:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    except Exception as e:
        return {"error": f"Error in encoding image function: {e}"}


@csrf_exempt
def analyse_image(image_data):
    try:
        # OpenAI API Key
        api_key = os.environ.get('OPENAI_API_KEY', '')

        # Getting the base64 string
        base64_image = base64.b64encode(image_data).decode('utf-8')

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }

        payload = {
            "model": "gpt-4o-mini",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "Please give a technical analysis of this image (of a trading chart)."
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            "max_tokens": 1000
        }


        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload)
        
        json_data = response.json()
        final_response = json_data['choices'][0]['message']['content']
        return final_response

    except Exception as e:
        print(f"Error occurred in analyse image function: {e}")
        return {"error": f"Error occurred in analyse image function: {e}"}


@csrf_exempt
def process_image(request):
    if request.method == 'POST':
        try:
            # Get the image data from the request
            data = json.loads(request.body)
            image_base64 = data.get('imageBase64', '')

            # Decode the base64 image data
            image_data = base64.b64decode(image_base64.encode('utf-8'))

            # Perform GPT-4 Vision processing
            analysed_image = analyse_image(image_data)

            return JsonResponse({"status": "success", "result": analysed_image})
        except Exception as e:
            print(f'error: {e}')
            return JsonResponse({"status": "error", "error": str(e)})
    
    print("Invalid request method")
    return JsonResponse({"status": "error", "error": "Invalid request method"})
# {status: 'error', error: 'embedded null byte'}


@csrf_exempt
def chosen_models(request, user_email, magic_number):
    try:
        if request.method == 'POST':
            # Decode the bytes to a string
            data_str = request.body.decode('utf-8')
            data = json.loads(data_str)

            today = timezone.localtime(timezone.now()).date()

            # Find the dictionary in the list
            dict_in_list = next((item for item in data if isinstance(item, dict)), None)

            new_model = Bot(username=user_email, magic_number=magic_number, parameters=data, time_saved=today)
            new_model.save()
            return JsonResponse({"message": f"{dict_in_list} with params: {user_email} and {magic_number}"})
        else:
            return JsonResponse({"message": "invalid request method"})
    except Exception as e:
        return JsonResponse({"Error": f"{e}"})



def check_json_in_list(lst):
    
    for item in lst:
        try:
            json.JSONDecoder().decode(item)
            return item
        except json.JSONDecodeError:
            pass
    return None        


@csrf_exempt
def run_bot(request, user_email, magic_number, asset):
    model_data = Bot.objects.filter(username=user_email, magic_number=magic_number).first()
    # model_parameters = list(model_data.parameters)
    model_parameters = ast.literal_eval(model_data.parameters)

    end_date = (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")

    # Calculate the date 30 days ago from the current day
    start_date = (datetime.now() - timedelta(days=10)).strftime("%Y-%m-%d")

    # Download data using the calculated dates
    forex_asset = f"{asset}=X"
    data = yf.download(forex_asset, start=start_date, end=end_date, interval="15m")

    output = trading_bot(df=data, params=model_parameters)

    # output = f'{data}'
    # Find the dictionary in the list
    dict_in_list = next((item for item in model_parameters if isinstance(item, dict)), None)

    return JsonResponse({"message": f"{output}"})


def trading_bot(df, params):

    trader_params = params

    # Find the dictionary in the list
    # dict_in_list = next((item for item in data if isinstance(item, dict)), None)
    
    dict_in_list = next((item for item in trader_params if isinstance(item, dict)), None)

    bbands_length = ''
    bbands_std = ''

    ma1_type = ''
    ma2_type = ''
    ma1 = ''
    ma2 = ''

    rsi_period = ''
    rsi_overbought = ''
    rsi_oversold = ''

    df['EMA_50'] = ta.ema(df['Close'], length=50)
    df['SMA_200'] = ta.sma(df['Close'], length=200)

    if 'BBands' in trader_params:
        bbands_length = dict_in_list['bbandsLength']
        bbands_std = dict_in_list['bbandsStd']

    elif 'Moving Averages' in trader_params:
        ma1_type = dict_in_list['ma1Type']
        ma1 = dict_in_list['ma1']
        ma2_type = dict_in_list['ma2Type']
        ma2 = dict_in_list['ma2']
    
    elif 'Relative Strength Index (RSI)' in trader_params:
        rsi_period = dict_in_list['rsiPeriod']
        rsi_overbought = dict_in_list['rsiOverbought']
        rsi_oversold = dict_in_list['rsiOversold']
    

    def bullish_engulfing(df):
        df_test = df.tail(7)
        df_test = df_test.drop_duplicates()
        test_size = len(df_test)
        num_engulfing = 0

        for i in range(test_size-1):
            first_candle = df_test.iloc[i-1]
            second_candle = df_test.iloc[i-2]
            third_candle  = df_test.iloc[i-3]
            fourth_candle = df_test.iloc[i-4]
            try:
                fifth_candle = df_test.iloc[i-5]
            except:
                print(df)
                print('')
                print(df_test)
            second_test = first_candle.Close > second_candle.Open

            if is_bearish_candle(second_candle) and is_bullish_candle(first_candle) and second_test == True and is_bearish_run(fifth_candle, fourth_candle, third_candle, second_candle):
                num_engulfing += 1

                # print('Bullish Engulfing')
                if df['EMA_50'].iloc[-1] > df['SMA_200'].iloc[-1]:

                    # Set the style of the plot
                    df.index = pd.to_datetime(df.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(df.tail(candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                    # plt.close(fig)
                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')
                    # if self.position:
                    #     self.position.close()

                    # if process_image(self.path) == 2:
                    #     price = self.data.Close[-1]
                    #     gain_amount = self.reward_percentage * self.equity
                    #     risk_amount = self.risk_percentage * self.equity
                    #     tp_level = price + self.reward_percentage
                    #     sl_level = price - self.risk_percentage
                    #     levels = get_fibonacci_levels(df=df.tail(75), trend='uptrend')
                    #     thirty_eight_retracement = levels[2]
                    #     sixty_one8_retracement = levels[4]
                    #     # if thirty_eight_retracement <= price <= sixty_one8_retracement:
                    #     # self.position.close()
                    #     self.buy(tp=tp_level, sl=sl_level)
                    # create_order(ticker, lot_size, buy_order_type, buy_price, buy_sl, buy_tp)
                    return 1
            # break


    def bearish_engulfing(df):
        df_test = df.tail(7)
        df_test = df_test.drop_duplicates()
        test_size = len(df_test)
        num_engulfing = 0

        for i in range(test_size-1):
            first_candle = df_test.iloc[i-1]
            second_candle = df_test.iloc[i-2]
            third_candle  = df_test.iloc[i-3]
            fourth_candle = df_test.iloc[i-4]
            fifth_candle = df_test.iloc[i-5]
            # first_test = first_candle.Open < second_candle.Close
            second_test = first_candle.Close < second_candle.Open

            if is_bullish_candle(second_candle) and is_bearish_candle(first_candle) and second_test == True and is_bullish_run(fifth_candle, fourth_candle, third_candle, second_candle):
                num_engulfing += 1
                # print('Bearish Engulfing')
                # price = self.data.Close[-1]

                if df['EMA_50'].iloc[-1] < df['SMA_200'].iloc[-1]:
                    df.index = pd.to_datetime(df.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(df.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                    # plt.close(fig)
                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')

                    # if self.position:
                    #     self.position.close()
                        # pass
                    # if process_image(self.path) == 0:
                    #     gain_amount = self.reward_percentage * self.equity
                    #     risk_amount = self.risk_percentage * self.equity
                    #     tp_level = price - self.reward_percentage
                    #     sl_level = price + self.risk_percentage
                    #     levels = get_fibonacci_levels(df=df.tail(75), trend='downtrend')
                    #     thirty_eight_retracement = levels[2]
                    #     sixty_one8_retracement = levels[4]
                    #     # if thirty_eight_retracement <= price <= sixty_one8_retracement:
                    #     # self.position.close()
                    #     self.sell(tp=tp_level, sl=sl_level)
                    # create_order(ticker, lot_size, sell_order_type, sell_price, sell_sl, sell_tp)
                    return -1
            # break


    def shooting_star(df):
        # print('')
        dataframe = df.drop_duplicates()
        df = df.tail(5)
        df = df.drop_duplicates()
        test_size = len(df)
        num_shooting_stars = 0
        bullish_shooting_stars = 0

        for i in range((test_size-1)-3):
            first_prev_candle = df.iloc[i]
            second_prev_candle = df.iloc[i+1]
            third_prev_candle = df.iloc[i+2]
            prev_candle = df.iloc[i+3]
            testing_candle = df.iloc[i+4]
            if is_bullish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                test= abs(testing_candle.High - testing_candle.Close)
                if 2 < test < 2.1:
                    num_shooting_stars += 1

                    dataframe.index = pd.to_datetime(dataframe.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(dataframe.tail(75), type='candle', volume=True, returnfig=True, style=style)
                    # plt.close(fig)
                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')

                    # print('bearish shooting star')
                    if df['EMA_50'].iloc[-1] < df['SMA_200'].iloc[-1]:
                        return -1

                        # create_order(ticker, lot_size, sell_order_type, sell_price, sell_sl, sell_tp)

            elif is_bearish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):

                test = abs(testing_candle.High - testing_candle.Close)
                if test > 2 and test < 2.1:
                    bullish_shooting_stars += 1
                    # print('bullish shooting star')
                    dataframe.index = pd.to_datetime(dataframe.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                    # plt.close(fig)
                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')

                    if df['EMA_50'].iloc[-1] > df['SMA_200'].iloc[-1]:
                        return 1
                        # create_order(ticker, lot_size, buy_order_type, buy_price, buy_sl, buy_tp)


    def three_white_soldiers(df):
        # print('')
        dataframe = df.drop_duplicates()
        df = df.drop_duplicates()
        df = df.tail(6)
        test_size = len(df)
        three_white_soldiers = 0
        three_black_crows = 0

        for i in range(test_size-5):
            first_prev_candle = df.iloc[i]
            second_prev_candle = df.iloc[i+1]
            third_prev_candle = df.iloc[i+2]
            prev_candle = df.iloc[i+3]
            testing_candle = df.iloc[i+4]
            testing_candle_2 = df.iloc[i+5]

            if is_bearish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                if testing_candle_2.Close > testing_candle.Close and testing_candle.Close > prev_candle.Close:
                    three_white_soldiers += 1
                    # print('bullish three white soldiers')
                    if df['EMA_50'].iloc[-1] > df['SMA_200'].iloc[-1]:
                        dataframe.index = pd.to_datetime(dataframe.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                        # plt.close(fig)
                        # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')
                        # plt.close(fig)
                        # if self.position:
                        #   self.position.close()
                        # if process_image(self.path) == 2:
                        #     price = self.data.Close[-1]
                        #     gain_amount = self.reward_percentage
                        #     risk_amount = self.risk_percentage
                        #     tp_level = price + self.reward_percentage
                        #     sl_level = price - self.risk_percentage
                        #     levels = get_fibonacci_levels(df=dataframe.tail(75), trend='uptrend')
                        #     thirty_eight_retracement = levels[2]
                        #     sixty_one8_retracement = levels[4]
                            # if thirty_eight_retracement <= prev_candle.Close <= sixty_one8_retracement:
                            # self.position.close()
                        # create_order(ticker, lot_size, buy_order_type, buy_price, buy_sl, buy_tp)
                        return 1
            
                            
            elif is_bullish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                if testing_candle_2.Close < testing_candle.Close and testing_candle.Close < prev_candle.Close:
                    three_black_crows += 1
                    # print('bearish three black crows')
                    if df['EMA_50'].iloc[-1] < df['SMA_200'].iloc[-1]:
                        df.index = pd.to_datetime(df.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(df.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                        # plt.close(fig)
                        # # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')
                        # plt.close(fig)
                        # if self.position:
                        #   self.position.close()
                        # if process_image(self.path) == 0:
                        #     price = self.data.Close[-1]
                        #     gain_amount = self.reward_percentage * self.equity
                        #     risk_amount = self.risk_percentage * self.equity
                        #     tp_level = price - self.reward_percentage
                        #     sl_level = price + self.risk_percentage
                        #     levels = get_fibonacci_levels(df=dataframe.tail(75), trend='downtrend')
                        #     thirty_eight_retracement = levels[2]
                        #     sixty_one8_retracement = levels[4]
                            # if thirty_eight_retracement <= prev_candle.Close <= sixty_one8_retracement:
                            # self.position.close()
                        # create_order(ticker, lot_size, sell_order_type, sell_price, sell_sl, sell_tp)
                        return -1
                

    def doji_star(df):
        # print('')
        df = df.drop_duplicates()
        dataframe = df
        df = df.tail(5)
        test_size = len(df)
        bullish_doji = 0
        bearish_doji = 0

        for i in range(test_size-4):
            first_prev_candle = df.iloc[i]
            second_prev_candle = df.iloc[i+1]
            third_prev_candle = df.iloc[i+2]
            prev_candle = df.iloc[i+3]
            testing_candle = df.iloc[i+4]
            # price = self.data.Close[-1]

            if is_bullish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                test1 = testing_candle.High - testing_candle.Close
                test2 = testing_candle.Close - testing_candle.Low
                if test1 == test2:
                    bullish_doji += 1
                    # print('bullish doji star')
                    # price = self.data.Close[-1]
                    # gain_amount = self.reward_percentage
                    # risk_amount = self.risk_percentage
                    # tp_level = price - gain_amount
                    # sl_level = price + risk_amount

                    dataframe.index = pd.to_datetime(dataframe.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                    # plt.close(fig)
                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')

                    if df['EMA_50'].iloc[-1] < df['SMA_200'].iloc[-1]:
                        return -1
                        # create_order(ticker, lot_size, sell_order_type, sell_price, sell_sl, sell_tp)
            elif is_bearish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                test1 = testing_candle.Open - testing_candle.Close
                test2 = testing_candle.Close - testing_candle.Low
                if test1 == test2:
                    bearish_doji += 1

                    dataframe.index = pd.to_datetime(dataframe.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                    # plt.close(fig)
                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')

                    if df['EMA_50'].iloc[-1] > df['SMA_200'].iloc[-1]:
                        return 1
                        # create_order(ticker, lot_size, buy_order_type, buy_price, buy_sl, buy_tp)


    def bullish_pinbar(df):
        dataframe = df.drop_duplicates()
        df = df.tail(1)
        df = df.drop_duplicates()
        test_size = len(df)
        num_pin_bars = 0
        # price = self.data.Close[-1]

        for i in range(test_size-1):
            candle = df.iloc[i]
            is_pin_bar = (candle.Close - candle.Low) > 0.05
            if is_pin_bar:
                num_pin_bars += 1
                # print('Bullish Pin Bar')

                dataframe.index = pd.to_datetime(dataframe.index)
                # style = mpf.make_mpf_style(base_mpf_style='classic')

                # Create the figure object without plotting
                # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                # plt.close(fig)
                # Save the figure to a file
                # fig.savefig('candlestick_chart.png')

                if df['EMA_50'].iloc[-1] > df['SMA_200'].iloc[-1]:
                    return 1
                    # create_order(ticker, lot_size, buy_order_type, buy_price, buy_sl, buy_tp)


    def bearish_pinbar(df):
        dataframe = df.drop_duplicates()
        df = df.tail(1)
        df = df.drop_duplicates()
        test_size = len(df)
        num_pin_bars = 0

        for i in range(test_size-1):
            candle = df.iloc[i]
            is_pin_bar = abs(candle.Close - candle.High) <  0.05
            if is_pin_bar:
                num_pin_bars += 1
                # print('Bearish Pin Bar')

                dataframe.index = pd.to_datetime(dataframe.index)
                # style = mpf.make_mpf_style(base_mpf_style='classic')

                # Create the figure object without plotting
                # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                # plt.close(fig)
                # Save the figure to a file
                # fig.savefig('candlestick_chart.png')
                if df['EMA_50'].iloc[-1] < df['SMA_200'].iloc[-1]:
                    return -1
                    # create_order(ticker, lot_size, sell_order_type, sell_price, sell_sl, sell_tp)


    def morning_star(df):
        # print('')
        dataframe = df.drop_duplicates()
        df = df.drop_duplicates()
        df = df.tail(6)
        test_size = len(df)
        morning_stars = 0
        evening_stars = 0
        # price = self.data.Close[-1]

        for i in range(test_size-5):
            first_prev_candle = df.iloc[i]
            second_prev_candle = df.iloc[i+1]
            third_prev_candle = df.iloc[i+2]
            prev_candle = df.iloc[i+3]
            testing_candle = df.iloc[i+4]
            testing_candle_2 = df.iloc[i+5]

            if is_bearish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                test = testing_candle.Open - testing_candle.Close
                if testing_candle_2.Close > testing_candle.Close and 0 < test < 2:
                    morning_stars += 1
                    # print('bullish morning star')
                    

                    dataframe.index = pd.to_datetime(dataframe.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)

                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')
                    # plt.close(fig)
                    if df['EMA_50'].iloc[-1] > df['SMA_200'].iloc[-1]:
                        # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='uptrend')
                        # thirty_eight_retracement = levels[2]
                        # sixty_one8_retracement = levels[4]
                        # if thirty_eight_retracement <= testing_candle.Close <= sixty_one8_retracement:
                        # create_order(ticker, lot_size, buy_order_type, buy_price, buy_sl, buy_tp)
                        return 1

            elif is_bullish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                test = testing_candle.Open - testing_candle.Close
                if testing_candle_2.Close < testing_candle.Close and 0 < test < 2 and testing_candle.Close < prev_candle.Close:
                    evening_stars += 1
                    # print('bearish morning star')

                    dataframe.index = pd.to_datetime(dataframe.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)

                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')
                    # plt.close(fig)
                    if df['EMA_50'].iloc[-1] < df['SMA_200'].iloc[-1]:
                        # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='downtrend')
                        # thirty_eight_retracement = levels[2]
                        # sixty_one8_retracement = levels[4]
                        # if thirty_eight_retracement <= testing_candle.Close <= sixty_one8_retracement:
                        # create_order(ticker, lot_size, sell_order_type, sell_price, sell_sl, sell_tp)
                        return -1


    def matching(df):
        # print('')
        df = df.drop_duplicates()
        dataframe = df
        df = df.tail(5)
        test_size = len(df)
        matching_lows = 0
        matching_highs = 0

        for i in range(test_size-4):
            first_prev_candle = df.iloc[i]
            second_prev_candle = df.iloc[i+1]
            third_prev_candle = df.iloc[i+2]
            prev_candle = df.iloc[i+3]
            testing_candle = df.iloc[i+4]

            if is_bearish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                if prev_candle.Low == testing_candle.Low and prev_candle.Close == testing_candle.Close:
                    matching_lows += 1
                    # print('matching low')

                    dataframe.index = pd.to_datetime(dataframe.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                    # plt.close(fig)
                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')

                    if df['EMA_50'].iloc[-1] > df['SMA_200'].iloc[-1]:
                        # create_order(ticker, lot_size, buy_order_type, buy_price, buy_sl, buy_tp)
                        return 1

            elif is_bullish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                if prev_candle.High == testing_candle.High and prev_candle.High == testing_candle.High:
                    matching_highs += 1
                    # print('matching high')

                    dataframe.index = pd.to_datetime(dataframe.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                    # plt.close(fig)
                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')

                    if df['EMA_50'].iloc[-1] < df['SMA_200'].iloc[-1]:
                        return -1
                        # create_order(ticker, lot_size, sell_order_type, sell_price, sell_sl, sell_tp)


    def methods(df):
        dataframe = df.drop_duplicates()
        df = df.drop_duplicates()
        df = df.tail(8)
        test_size = len(df)
        rising_methods = 0
        falling_methods = 0
        # price = self.data.Close[-1]

        for i in range(test_size-7):
            first_prev_candle = df.iloc[i]
            second_prev_candle = df.iloc[i+1]
            third_prev_candle = df.iloc[i+2]
            prev_candle = df.iloc[i+3]
            testing_candle = df.iloc[i+4]
            testing_candle_2 = df.iloc[i+5]
            testing_candle_3 = df.iloc[i+6]
            final_candle = df.iloc[7]

            if is_bullish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle) and testing_candle.close < prev_candle.close and is_bearish_run_3(testing_candle, testing_candle_2, testing_candle_3):
                if final_candle.Close > prev_candle.Close:
                    rising_methods += 1
                    # print('rising three methods')

                    dataframe.index = pd.to_datetime(dataframe.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                    # plt.close(fig)
                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')
                    # plt.close(fig)

                    if df['EMA_50'].iloc[-1] > df['SMA_200'].iloc[-1]:
                        # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='uptrend')
                        # thirty_eight_retracement = levels[2]
                        # sixty_one8_retracement = levels[4]
                        # if thirty_eight_retracement <= testing_candle_3.Close <= sixty_one8_retracement:
                        # create_order(ticker, lot_size, buy_order_type, buy_price, buy_sl, buy_tp)
                        return 1

            elif is_bearish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle) and testing_candle.close > prev_candle.close and is_bullish_run_3(testing_candle, testing_candle_2, testing_candle_3):
                if final_candle.Close < prev_candle.Close:
                    falling_methods += 1
                    # print('falling three methods')

                    dataframe.index = pd.to_datetime(dataframe.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                    # plt.close(fig)
                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')
                    # plt.close(fig)

                    if df['EMA_50'].iloc[-1] < df['SMA_200'].iloc[-1]:
                        # if thirty_eight_retracement <= testing_candle_3.Close <= sixty_one8_retracement:
                        # create_order(ticker, lot_size, sell_order_type, sell_price, sell_sl, sell_tp)
                        return -1


    def check_moving_averages_for_buy(df, range):
        past_10_rows = df[['SMA_149', 'SMA_202']].tail(range)
        past_10_rows['Converge'] = past_10_rows['SMA_149'] < past_10_rows['SMA_202']
        past = past_10_rows.tail(1)['Converge'].values[0]
        second_last_row = past_10_rows['Converge'].iloc[-2]
        print(past_10_rows)
        if past == False and second_last_row == True:
                # print('True')
            return True
        else:
                # print('False')
            return False


    def check_moving_averages_for_sell(df, range):
        past_10_rows = df[['SMA_149', 'SMA_202']].tail(range)
        past_10_rows['Diverge'] = past_10_rows['SMA_149'] > past_10_rows['SMA_202']
        past = past_10_rows.tail(1)['Diverge'].values[0]
        second_last_row = past_10_rows['Diverge'].iloc[-2]
        # print(past)
        print(past_10_rows)
        if past == False and second_last_row == True:
            # print('True')
            return True
        else:
                # print('False')
            return False


    def moving_average(df):
        # print(f'already_sell is {already_sell}')
        # print(f'Open Positions are {open_positions}')
        # position = ''
        # fema = df['EMA_50'].iloc[-1]
        # tsma = df['SMA_200'].iloc[-1]
        # print(f'df ema 50 is {fema}')
        # print(f'df sma 200 is {tsma}')
        if df.tail(1)['MA_Lower'].values[0] > df.tail(1)['MA_Upper'].values[0]:
            # print('1')
            # if open_positions is not None:
            #     close_order(ticker, lot_size, buy_order_type,  buy_price)
            #     print('4')
            if check_moving_averages_for_buy(df=df, range=range):
                # create_order(ticker, lot_size, buy_order_type, buy_price, buy_sl, buy_tp)
                return 1
        
        elif df.tail(1)['MA_Lower'].values[0] < df.tail(1)['MA_Upper'].values[0]:
            # print('2')
            # if open_positions is not None:
            #     print('7.0')
            #     close_order(ticker, lot_size, sell_order_type,  sell_price)
            #     print('7')
            if check_moving_averages_for_sell(df=df, range=range):
                
                # create_order(ticker, lot_size, sell_order_type, sell_price, sell_sl, sell_tp)
                return -1


    def bbands(df):
        upper_band = f'BBU_{bbands_length}_{float(bbands_std)}'
        lower_band = f'BBL_{bbands_length}_{float(bbands_std)}'
        try:
            if df['Close'].iloc[-1] >= df[upper_band].iloc[-1]:
                # create_order(ticker, lot_size, buy_order_type, buy_price, buy_sl, buy_tp)
                return 1
            
            elif df['Close'].iloc[-1] <= df[lower_band].iloc[-1]:
                # create_order(ticker, lot_size, sell_order_type, sell_price, sell_sl, sell_tp)
                return -1
        except: 
            pass


    def rsi(df):
        if df['RSI'].iloc[-1] > rsi_overbought:
            # create_order(ticker, lot_size, sell_order_type, sell_price, sell_sl, sell_tp)
            return -1
        
        elif df['RSI'].iloc[-1] < rsi_oversold:
            # create_order(ticker, lot_size, buy_order_type, buy_price, buy_sl, buy_tp)
            return 1


    def momentum(df):
        if df['MOM'].iloc[-1] > 80:
            # create_order(ticker, lot_size, sell_order_type, sell_price, sell_sl, sell_tp)
            return -1
        elif df['MOM'].iloc[-1] < 20:
            # create_order(ticker, lot_size, buy_order_type, buy_price, buy_sl, buy_tp)
            return 1

    if ma1_type == 'SMA':
        df['MA_Upper'] = ta.sma(df['Close'], length=ma1)
    elif ma1_type == 'EMA':
        df['MA_Upper'] = ta.ema(df['Close'], length=ma1)
    
    if ma2_type == 'SMA':
        df['MA_Lower'] = ta.sma(df['Close'], length=ma2)
    elif ma2_type == 'EMA':
        df['MA_Lower'] = ta.ema(df['Close'], length=ma2)
    # df['MA_Lower'] = ta.sma(df['Close'], length=ma2)
    # print(df)
    current_close = df['Close']
    current_close = ta.bbands(close=df['Close'], length=bbands_length, std=bbands_std, append=True)
    try:
        upper_band = f'BBU_{bbands_length}_{float(bbands_std)}'
        lower_band = f'BBL_{bbands_length}_{float(bbands_std)}'
        df[upper_band] = current_close[upper_band]
        df[lower_band] = current_close[lower_band]
    except: 
        pass

    df['RSI'] = ta.rsi(df['Close'], length = rsi_period)

    df['MOM'] = ta.mom(df['Close'])

    buy = 0
    sell = 0

    if 'Engulfing' in trader_params:
        bullish_engulfing_output = bullish_engulfing(df=df)
        bearish_engulfing_output = bearish_engulfing(df=df)
        if bullish_engulfing_output == 1:
            buy += 1
            # return bullish_engulfing_output
        elif bullish_engulfing_output == -1:
            sell += 1
        if bearish_engulfing_output == 1:
            buy += 1
            # return bearish_engulfing_output
        elif bearish_engulfing_output == -1:
            sell += 1
    elif 'Three White Soldiers' in trader_params:
        three_white_soldiers_output = three_white_soldiers(df=df)
        if three_white_soldiers_output == 1:
            buy += 1
            # return three_white_soldiers_output
        elif three_white_soldiers_output == -1:
            sell += 1
    elif 'Doji Star' in trader_params:
        doji_star_output = doji_star(df=df)
        if doji_star_output == 1:
            buy += 1
            # return doji_star_output
        elif doji_star_output == -1:
            sell += 1
    elif 'Pin Bar' in trader_params:
        bullish_pinbar_output = bullish_pinbar(df=df)
        bearish_pinbar_output = bearish_pinbar(df=df)
        if bullish_pinbar_output == 1:
            buy += 1
            # return bullish_pinbar_output
        elif bullish_pinbar_output == -1:
            sell += 1
        if bearish_pinbar_output == 1:
            buy += 1
        elif bearish_pinbar_output == -1:
            sell += 1
            # return bearish_pinbar_output
    elif 'Morning Star' in trader_params:
        morning_star_output = morning_star(df=df)
        if morning_star_output == 1:
            buy += 1
            # return morning_star_output
        elif morning_star_output == -1:
            sell += 1
    elif 'Matching' in trader_params:
        matching_output = matching(df=df)
        if matching_output == 1:
            buy += 1
            # return matching_output
        elif matching_output == -1:
            sell += 1
    elif 'Methods' in trader_params:
        methods_output = methods(df=df)
        if methods_output == 1:
            buy += 1
        elif methods_output == -1:
            sell += 1
            # return methods_output
    elif 'Moving Averages' in trader_params:
        ma_output = moving_average(df=df)
        if ma_output == 1:
            buy += 1
        elif ma_output == -1:
            sell += 1
            # return ma_output
    elif 'BBands' in trader_params:
        bbands_output = bbands(df=df)
        if bbands_output == 1:
            buy += 1
        elif bbands_output == -1:
            sell += 1
            # return bbands_output
    elif 'Relative Strength Index (RSI)' in trader_params:
        rsi_output = rsi(df=df)
        if rsi_output == 1:
            buy += 1
            # return rsi_output
        elif rsi_output == -1:
            sell += 1
    elif 'Momentum Trading Bot' in trader_params:
        momentum_output = momentum(df=df)
        if momentum_output == 1:
            buy += 1
        elif momentum_output == -1:
            sell += 1

    temp_dict = {}

    temp_dict['buy'] = buy
    temp_dict['sell'] = sell

    if buy > sell:
        return 1
    elif sell > buy:
        return -1
    else:
        return 0
    


@csrf_exempt
async def handle_api_request_backtest(dataframe, backtest_period, parameters):

    class backtestAgent(Strategy):
        equity = 100000
        risk_percentage = 20
        reward_percentage = 50
        # current_price = 0
        reward_ratio = 15
        position_size = 0.01
        current_position = ''


        def init(self):
            price = self.data.Close
            self.ma1 = self.I(SMA, price, 10)
            self.ma2 = self.I(SMA, price, 20)

            
        def bullish_engulfing(self, df):
            df_test = df.tail(6)
            df_test = df_test.drop_duplicates()
            test_size = len(df)
            num_engulfing = 0

            for i in range(test_size-1):
                first_candle = df_test.iloc[i-1]
                second_candle = df_test.iloc[i-2]
                third_candle  = df_test.iloc[i-3]
                fourth_candle = df_test.iloc[i-4]
                fifth_candle = df_test.iloc[i-5]
                second_test = first_candle.Close > second_candle.Open

                if is_bearish_candle(second_candle) and is_bullish_candle(first_candle) and second_test == True and is_bearish_run(fifth_candle, fourth_candle, third_candle, second_candle):
                    # num_engulfing += 1
                    # print('Bullish Engulfing')
                    if df.tail(1)['EMA_50'].values[0] > df.tail(1)['SMA_200'].values[0]:
                        # # Set the style of the plot
                        # df.index = pd.to_datetime(df.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')
                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(df.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                        # plt.close(fig)
                        # # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')
                        # # if self.position:
                        # #     self.position.close()

                        # if process_image(self.path) == 2:
                        price = self.data.Close[-1]
                        gain_amount = self.reward_percentage
                        risk_amount = self.risk_percentage
                        tp_level = price + self.reward_percentage
                        sl_level = price - self.risk_percentage

                        # levels = get_fibonacci_levels(df=df.tail(75), trend='uptrend')
                        # thirty_eight_retracement = levels[2]
                        # sixty_one8_retracement = levels[4]
                        # if thirty_eight_retracement <= price <= sixty_one8_retracement:
                            # self.position.close()
                        if self.position:
                            self.position.close()
                        self.buy(tp=tp_level, sl=sl_level)
                break


        def bearish_engulfing(self, df):
            df_test = df.tail(6)
            df_test = df_test.drop_duplicates()
            test_size = len(df)
            num_engulfing = 0

            for i in range(test_size-1):
                first_candle = df_test.iloc[i-1]
                second_candle = df_test.iloc[i-2]
                third_candle  = df_test.iloc[i-3]
                fourth_candle = df_test.iloc[i-4]
                fifth_candle = df_test.iloc[i-5]
                # first_test = first_candle.Open < second_candle.Close
                second_test = first_candle.Close < second_candle.Open

                if is_bullish_candle(second_candle) and is_bearish_candle(first_candle) and second_test == True and is_bullish_run(fifth_candle, fourth_candle, third_candle, second_candle):
                    # num_engulfing += 1
                    # print('Bearish Engulfing')
                    price = self.data.Close[-1]

                    if df.tail(1)['EMA_50'].values[0] < df.tail(1)['SMA_200'].values[0]:
                        # df.index = pd.to_datetime(df.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # # Create the figure object without plotting
                        # fig, axes = mpf.plot(df.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                        # plt.close(fig)
                        # # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')

                        # if self.position:
                        #     self.position.close()
                            # pass
                        # if process_image(self.path) == 0:
                        gain_amount = self.reward_percentage
                        risk_amount = self.risk_percentage
                        tp_level = price - self.reward_percentage
                        sl_level = price + self.risk_percentage
                        # levels = get_fibonacci_levels(df=df.tail(75), trend='downtrend')
                        # thirty_eight_retracement = levels[2]
                        # sixty_one8_retracement = levels[4]
                            # if thirty_eight_retracement <= price <= sixty_one8_retracement:
                            # self.position.close()
                        if self.position:
                            self.position.close()
                        self.sell(tp=tp_level, sl=sl_level)
                # Break statement might not be neccessary here. Think it might be safe to remove.
                break


        def bullish_pinbar(self, df):
            dataframe = df.drop_duplicates()
            df = df.tail(1)
            df = df.drop_duplicates()
            test_size = len(df)
            num_pin_bars = 0
            price = self.data.Close[-1]

            for i in range(test_size-1):
                candle = df.iloc[i]
                is_pin_bar = (candle.Close - candle.Low) > 5
                if is_pin_bar:
                    num_pin_bars += 1
                    # print('Bullish Pin Bar')
                    price = self.data.Close[-1]
                    gain_amount = self.reward_percentage
                    risk_amount = self.risk_percentage
                    tp_level = price + gain_amount
                    sl_level = price - risk_amount

                    # dataframe.index = pd.to_datetime(dataframe.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                    # plt.close(fig)
                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')

                    if df.tail(1)['EMA_50'].values[0] > df.tail(1)['SMA_200'].values[0]:
                        self.buy(tp=tp_level, sl=sl_level)


        def bearish_pinbar(self, df):
            dataframe = df.drop_duplicates()
            df = df.tail(1)
            df = df.drop_duplicates()
            test_size = len(df)
            num_pin_bars = 0

            for i in range(test_size-1):
                candle = df.iloc[i]
                is_pin_bar = abs(candle.Close - candle.High) <  5
                if is_pin_bar:
                    num_pin_bars += 1
                    # print('Bearish Pin Bar')
                    price = self.data.Close[-1]
                    gain_amount = self.reward_percentage
                    risk_amount = self.risk_percentage
                    tp_level = price - gain_amount
                    sl_level = price + risk_amount

                    # dataframe.index = pd.to_datetime(dataframe.index)
                    # style = mpf.make_mpf_style(base_mpf_style='classic')

                    # Create the figure object without plotting
                    # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                    # plt.close(fig)
                    # Save the figure to a file
                    # fig.savefig('candlestick_chart.png')

                    if df.tail(1)['EMA_50'].values[0] < df.tail(1)['SMA_200'].values[0]:
                        self.sell(tp=tp_level, sl=sl_level)
        

        def doji_star(self, df):
            # print('')
            df = df.drop_duplicates()
            dataframe = df
            df = df.tail(5)
            test_size = len(df)
            bullish_doji = 0
            bearish_doji = 0

            for i in range(test_size-4):
                first_prev_candle = df.iloc[i]
                second_prev_candle = df.iloc[i+1]
                third_prev_candle = df.iloc[i+2]
                prev_candle = df.iloc[i+3]
                testing_candle = df.iloc[i+4]
                price = self.data.Close[-1]

                if is_bullish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                    test1 = testing_candle.High - testing_candle.Close
                    test2 = testing_candle.Close - testing_candle.Low
                    if test1 == test2:
                        bullish_doji += 1
                        # print('bullish doji star')
                        price = self.data.Close[-1]
                        gain_amount = self.reward_percentage
                        risk_amount = self.risk_percentage
                        tp_level = price - gain_amount
                        sl_level = price + risk_amount

                        # dataframe.index = pd.to_datetime(dataframe.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                        # plt.close(fig)
                        # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')

                        if df.tail(1)['EMA_50'].values[0] < df.tail(1)['SMA_200'].values[0]:
                            self.sell(tp=tp_level, sl=sl_level)
                elif is_bearish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                    test1 = testing_candle.Open - testing_candle.Close
                    test2 = testing_candle.Close - testing_candle.Low
                    if test1 == test2:
                        bearish_doji += 1
                        price = self.data.Close[-1]
                        gain_amount = self.reward_percentage
                        risk_amount = self.risk_percentage
                        tp_level = price + gain_amount
                        sl_level = price - risk_amount

                        # dataframe.index = pd.to_datetime(dataframe.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                        # plt.close(fig)
                        # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')

                        if df.tail(1)['EMA_50'].values[0] > df.tail(1)['SMA_200'].values[0]:
                            self.buy(tp=tp_level, sl=sl_level)


        def three_white_soldiers(self, df):
            # print('')
            dataframe = df.drop_duplicates()
            df = df.drop_duplicates()
            df = df.tail(6)
            test_size = len(df)
            three_white_soldiers = 0
            three_black_crows = 0

            for i in range(test_size-5):
                first_prev_candle = df.iloc[i]
                second_prev_candle = df.iloc[i+1]
                third_prev_candle = df.iloc[i+2]
                prev_candle = df.iloc[i+3]
                testing_candle = df.iloc[i+4]
                testing_candle_2 = df.iloc[i+5]
                price = self.data.Close[-1]

                if is_bearish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                    if testing_candle_2.Close > testing_candle.Close and testing_candle.Close > prev_candle.Close:
                        three_white_soldiers += 1
                        # print('bullish three white soldiers')
                        if df.tail(1)['EMA_50'].values[0] > df.tail(1)['SMA_200'].values[0]:
                            # dataframe.index = pd.to_datetime(dataframe.index)
                            # style = mpf.make_mpf_style(base_mpf_style='classic')

                            # Create the figure object without plotting
                            # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                            # plt.close(fig)
                            # Save the figure to a file
                            # fig.savefig('candlestick_chart.png')
                            # plt.close(fig)
                            # if self.position:
                            #   self.position.close()
                            # if process_image(self.path) == 2:
                            price = self.data.Close[-1]
                            gain_amount = self.reward_percentage
                            risk_amount = self.risk_percentage
                            tp_level = price + self.reward_percentage
                            sl_level = price - self.risk_percentage
                            # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='uptrend')
                            # thirty_eight_retracement = levels[2]
                            # sixty_one8_retracement = levels[4]
                                # if thirty_eight_retracement <= prev_candle.Close <= sixty_one8_retracement:
                                # self.position.close()
                            self.buy(tp=tp_level, sl=sl_level)
                elif is_bullish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                    if testing_candle_2.Close < testing_candle.Close and testing_candle.Close < prev_candle.Close:
                        three_black_crows += 1
                        # print('bearish three black crows')
                        if df.tail(1)['EMA_50'].values[0] < df.tail(1)['SMA_200'].values[0]:
                            # df.index = pd.to_datetime(df.index)
                            # style = mpf.make_mpf_style(base_mpf_style='classic')

                            # Create the figure object without plotting
                            # fig, axes = mpf.plot(df.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                            # plt.close(fig)
                            # Save the figure to a file
                            # fig.savefig('candlestick_chart.png')
                            # plt.close(fig)
                            # if self.position:
                            #   self.position.close()
                            # if process_image(self.path) == 0:
                            price = self.data.Close[-1]
                            gain_amount = self.reward_percentage 
                            risk_amount = self.risk_percentage
                            tp_level = price - self.reward_percentage
                            sl_level = price + self.risk_percentage
                            # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='downtrend')
                            # thirty_eight_retracement = levels[2]
                            # sixty_one8_retracement = levels[4]
                                # if thirty_eight_retracement <= prev_candle.Close <= sixty_one8_retracement:
                                # self.position.close()
                            self.sell(tp=tp_level, sl=sl_level)


        def morning_star(self, df):
            # print('')
            dataframe = df.drop_duplicates()
            df = df.drop_duplicates()
            df = df.tail(6)
            test_size = len(df)
            morning_stars = 0
            evening_stars = 0
            price = self.data.Close[-1]

            for i in range(test_size-5):
                first_prev_candle = df.iloc[i]
                second_prev_candle = df.iloc[i+1]
                third_prev_candle = df.iloc[i+2]
                prev_candle = df.iloc[i+3]
                testing_candle = df.iloc[i+4]
                testing_candle_2 = df.iloc[i+5]

                if is_bearish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                    test = testing_candle.Open - testing_candle.Close
                    if testing_candle_2.Close > testing_candle.Close and 0 < test < 2:
                        morning_stars += 1
                        # print('bullish morning star')
                        price = self.data.Close[-1]
                        gain_amount = self.reward_percentage
                        risk_amount = self.risk_percentage
                        tp_level = price + gain_amount
                        sl_level = price - risk_amount

                        # dataframe.index = pd.to_datetime(dataframe.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)

                        # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')
                        # plt.close(fig)
                        if df.tail(1)['EMA_50'].values[0] > df.tail(1)['SMA_200'].values[0]:
                            # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='uptrend')
                            # thirty_eight_retracement = levels[2]
                            # sixty_one8_retracement = levels[4]
                            # if thirty_eight_retracement <= testing_candle.Close <= sixty_one8_retracement:
                            self.buy(tp=tp_level, sl=sl_level)

                elif is_bullish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle):
                    test = testing_candle.Open - testing_candle.Close
                    if testing_candle_2.Close < testing_candle.Close and 0 < test < 2 and testing_candle.Close < prev_candle.Close:
                        evening_stars += 1
                        # print('bearish morning star')
                        price = self.data.Close[-1]
                        gain_amount = self.reward_percentage
                        risk_amount = self.risk_percentage
                        tp_level = price - gain_amount
                        sl_level = price + risk_amount

                        # dataframe.index = pd.to_datetime(dataframe.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)

                        # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')
                        # plt.close(fig)
                        if df.tail(1)['EMA_50'].values[0] < df.tail(1)['SMA_200'].values[0]:
                            # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='downtrend')
                            # thirty_eight_retracement = levels[2]
                            # sixty_one8_retracement = levels[4]
                            # if thirty_eight_retracement <= testing_candle.Close <= sixty_one8_retracement:
                            self.sell(tp=tp_level, sl=sl_level)

            

        def methods(self, df):
            dataframe = df.drop_duplicates()
            df = df.drop_duplicates()
            df = df.tail(9)
            test_size = len(df)
            rising_methods = 0
            falling_methods = 0
            price = self.data.Close[-1]

            for i in range(test_size-7):
                first_prev_candle = df.iloc[i]
                second_prev_candle = df.iloc[i+1]
                third_prev_candle = df.iloc[i+2]
                prev_candle = df.iloc[i+3]
                testing_candle = df.iloc[i+4]
                testing_candle_2 = df.iloc[i+5]
                testing_candle_3 = df.iloc[i+6]
                final_candle = df.iloc[7]

                if is_bullish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle) and testing_candle.Close < prev_candle.Close and is_bearish_run_3(testing_candle, testing_candle_2, testing_candle_3):
                    if final_candle.Close > prev_candle.Close:
                        rising_methods += 1
                        # print('rising three methods')
                        price = self.data.Close[-1]
                        gain_amount = self.reward_percentage
                        risk_amount = self.risk_percentage
                        tp_level = price + self.reward_percentage
                        sl_level = price - self.risk_percentage

                        # dataframe.index = pd.to_datetime(dataframe.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                        # plt.close(fig)
                        # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')
                        # plt.close(fig)

                        if df.tail(1)['EMA_50'].values[0] > df.tail(1)['SMA_200'].values[0]:
                            # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='uptrend')
                            # thirty_eight_retracement = levels[2]
                            # sixty_one8_retracement = levels[4]
                            # if thirty_eight_retracement <= testing_candle_3.Close <= sixty_one8_retracement:
                            self.buy(tp=tp_level, sl=sl_level)

                elif is_bearish_run(first_prev_candle, second_prev_candle, third_prev_candle, prev_candle) and testing_candle.Close > prev_candle.Close and is_bullish_run_3(testing_candle, testing_candle_2, testing_candle_3):
                    if final_candle.Close < prev_candle.Close:
                        falling_methods += 1
                        # print('falling three methods')
                        price = self.data.Close[-1]
                        gain_amount = self.reward_percentage * self.equity
                        risk_amount = self.risk_percentage * self.equity
                        tp_level = price - self.reward_percentage
                        sl_level = price + self.risk_percentage

                        # dataframe.index = pd.to_datetime(dataframe.index)
                        # style = mpf.make_mpf_style(base_mpf_style='classic')

                        # Create the figure object without plotting
                        # fig, axes = mpf.plot(dataframe.tail(self.candlestick_backtrack), type='candle', volume=True, returnfig=True, style=style)
                        # plt.close(fig)
                        # Save the figure to a file
                        # fig.savefig('candlestick_chart.png')
                        # plt.close(fig)

                        if df.tail(1)['EMA_50'].values[0] < df.tail(1)['SMA_200'].values[0]:
                            # levels = get_fibonacci_levels(df=dataframe.tail(75), trend='downtrend')
                            # thirty_eight_retracement = levels[2]
                            # sixty_one8_retracement = levels[4]
                            # if thirty_eight_retracement <= testing_candle_3.Close <= sixty_one8_retracement:
                            self.sell(tp=tp_level, sl=sl_level)
        

        def momentum(self, df):
            if df.tail(1)['MOM'].values[0] > 80:
                price = self.data.Close[-1]
                gain_amount = self.reward_percentage
                risk_amount = self.risk_percentage
                tp_level = price - gain_amount
                sl_level = price + risk_amount

                # if self.current_position != 'sell':
                if self.position:
                    self.position.close()
                self.sell()
                    # self.current_position = 'sell'
                    # self.sell()

            elif df.tail(1)['MOM'].values[0] < 20:
                price = self.data.Close[-1]
                gain_amount = self.reward_percentage
                risk_amount = self.risk_percentage
                tp_level = price + gain_amount
                sl_level = price - risk_amount
                
                # if self.current_position != 'buy':
                if self.position:
                    self.position.close()
                self.buy()
                # self.current_position = 'buy'
                    # self.buy()
        

        def all_bots(self, df):
            if 'Momentum Trading Bot' in parameters:
                self.momentum(df=df)
            if 'Engulfing' in parameters:
                self.bullish_engulfing(df=df)
                self.bearish_engulfing(df=df)  
            if 'Pin Bar' in parameters:
                self.bullish_pinbar(df=df)
                self.bearish_pinbar(df=df)
            if 'Three White Soldiers' in parameters:
                self.three_white_soldiers(df=df)
            if 'Doji Star' in parameters:
                self.doji_star(df=df)
            if 'Morning Star' in parameters:
                self.morning_star(df=df)
            if 'Methods' in parameters:
                self.methods(df=df)
            

        def next(self):
            df = pd.DataFrame({'Open': self.data.Open, 'High': self.data.High, 'Low': self.data.Low, 'Close': self.data.Close, 'Volume': self.data.Volume})
            df['MOM'] = ta.mom(df['Close'])
            df['EMA_50'] = ta.ema(df['Close'], length=50)
            df['SMA_200'] = ta.sma(df['Close'], length=200)
            # if not self.position:
            try:
                self.all_bots(df=df)
                # print('Running Backtesting Algorithm...')
            except Exception as e:
                print(f'Exception is {e}')
                pass

    return_plot = False
    

    if dataframe == '5Min':
        df_to_use = './XAUUSD5M.csv'
    elif dataframe == '15Min':
        df_to_use = './XAUUSD15M.csv'
    elif dataframe == '30Min':
        df_to_use = './XAUUSD30M.csv'
    elif dataframe == '1H':
        df_to_use = './XAUUSD1H.csv'
    elif dataframe == '4H':
        df_to_use = './XAUUSD4H.csv'
        return_plot = True
    elif dataframe == '1D':
        df_to_use = './XAUUSD1D.csv'
        return_plot = True
    

    if backtest_period == '0-25':
        start = 0
        end = 0.25
    elif backtest_period == '25-50':
        start = 0.25
        end = 0.5
    elif backtest_period == '50-75':
        start = 0.5
        end = 0.75
    elif backtest_period == '75-100':
        start = 0.75
        end = 1

    df_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), df_to_use)
    df = pd.read_csv(df_path).drop_duplicates()
    df.index = pd.to_datetime(df['Time'].values)
    del df['Time']
    # test_length = int(len(df) * 0.25)
    length = int(len(df) * start)
    second_length = int(len(df) * end)
    bt = Backtest(df[length:second_length], backtestAgent, exclusive_orders=False, cash=10000)
    output = bt.run()
    
    if return_plot:

        p = bt.plot()
        
        item = json_item(p, "myplot")
        # print(item)
        
        plot_json = json.dumps(item)
    else:
        plot_json = {}

    # Convert the plot to HTML
    # html = file_html(plot, CDN, "backtesting plot")

    
    # plot_json = json_item(plot, "myplot")
    # print(item)
    
    # Convert the relevant output fields to a dictionary
    result_dict = {
        "Start": str(output['Start']),
        "End": str(output['End']),
        "Duration": str(output['Duration']),
        "Exposure Time [%]": output['Exposure Time [%]'],
        "Equity Final [$]": output['Equity Final [$]'],
        "Equity Peak [$]": output['Equity Peak [$]'],
        "Return [%]": output['Return [%]'],
        "Buy & Hold Return [%]": output['Buy & Hold Return [%]'],
        "Return (Ann.) [%]": output['Return (Ann.) [%]'],
        "Volatility (Ann.) [%]": output['Volatility (Ann.) [%]'],
        "Sharpe Ratio": output['Sharpe Ratio'],
        "Sortino Ratio": output['Sortino Ratio'],
        "Calmar Ratio": output['Calmar Ratio'],
        "Max. Drawdown [%]": output['Max. Drawdown [%]'],
        "Avg. Drawdown [%]": output['Avg. Drawdown [%]'],
        "Max. Drawdown Duration": str(output['Max. Drawdown Duration']),
        "Avg. Drawdown Duration": str(output['Avg. Drawdown Duration']),
        "# Trades": output['# Trades'],
        "Win Rate [%]": output['Win Rate [%]'],
        "Best Trade [%]": output['Best Trade [%]'],
        "Worst Trade [%]": output['Worst Trade [%]'],
        "Avg. Trade [%]": output['Avg. Trade [%]'],
        "Max. Trade Duration": str(output['Max. Trade Duration']),
        "Avg. Trade Duration": str(output['Avg. Trade Duration']),
        "Profit Factor": output['Profit Factor'],
        "Expectancy [%]": output['Expectancy [%]'],
        "SQN": output['SQN'],
    }
    return result_dict, plot_json


@csrf_exempt
def run_backtest(request, dataframe, backtest_period):
    dummy_param1 = None
    dummy_param2 = None
    try:
        if request.method == 'POST':
            # Decode the bytes to a string
            data_str = request.body.decode('utf-8')
            data = json.loads(data_str)
            model_parameters = data
            dummy_param1 = data
            async def inner_backtest():
                result = await handle_api_request_backtest(dataframe, backtest_period, model_parameters)
                return JsonResponse({'Output': result})

            # Run the asynchronous code using the event loop
            loop = asyncio.get_event_loop()
            return loop.run_until_complete(inner_backtest())
    except Exception as e:
        return JsonResponse({"Error Occured Here": f'{e}'})


@csrf_exempt
def interest_rates(request):
    api_ninjas_key = 'fhw7p7lWporgmk7eGGdpiQ==ce3O6xofIN88xuH2'
    api_url = 'https://api.api-ninjas.com/v1/interestrate'
    result = ''
    response = requests.get(api_url, headers={'X-Api-Key': api_ninjas_key, 'central_bank_only': 'true'})
    if response.status_code == requests.codes.ok:
        result = response.text
    else:
        result = response.status_code, response.text
    return JsonResponse({'Interest Rates': result})



def is_bearish_candle(candle):
    # Function to check if a candle is a bearish candle
    if candle.Open > candle.Close:
        return True
    return False


def is_bullish_candle(candle):
    # Function to check if a candle is a bullish candle
    if candle.Open < candle.Close:
        return True
    else:
        return False
    
# candle_type = is_bullish_candle(temp_dataset.head(1))
# print(f'Bullish Candle Type is: {candle_type}\n')


def is_bullish_engulfing(data):

    # In this function we take the final 3 candles. We then check if the first two 
    # candles (excluding the latest 3rd one) satisfies the candle that it is a bullish 
    # engulfing.
    candle1 = data.iloc[-3]
    candle2 = data.iloc[-2]

    if is_bearish_candle(candle1) and is_bullish_candle(candle2):
        if candle2.Close > candle1.Open and candle2.Open < candle1.Close:
            return True
        return False
    return False


def is_bearish_engulfing(data):

    # In this function we take the final 3 candles. We then check if the first two 
    # candles (excluding the latest 3rd one) satisfies the candle that it is a bearish
    # engulfing.
    candle1 = data.iloc[-3]
    candle2 = data.iloc[-2]

    if is_bullish_candle(candle1) and is_bearish_candle(candle2):
        if candle2.Close < candle1.Open and candle2.Open > candle1.Close:
            return True
        return False
    return False


def is_morning_star(data):

    # In this function we take the final 3 candles. We then check if the first two 
    # candles (excluding the latest 3rd one) satisfies the candle that it is a morning
    # star.
    candle1 = data.iloc[-4]
    candle2 = data.iloc[-3]
    candle3 = data.iloc[-2]

    if is_bearish_candle(candle1) and is_bullish_candle(candle3):
        if candle3.Close < candle1.Open and candle2.Close < candle3.Close:
            return True
        return False
    return False


def is_evening_star(data):

    # In this function we take the final 3 candles. We then check if the first two 
    # candles (excluding the latest 3rd one) satisfies the candle that it is an evening
    # star.
    candle1 = data.iloc[-4]
    candle2 = data.iloc[-3]
    candle3 = data.iloc[-2]

    if is_bullish_candle(candle1) and is_bearish_candle(candle3):
        if candle3.Close > candle1.Open and candle2.Close > candle3.Close:
            return True
        return False
    return False


def is_three_white_soldiers(data):
     # In this function we take the final 3 candles. We then check if the first two 
    # candles (excluding the latest 3rd one) satisfies the candle that it is a three
    # white soldiers.
    candle1 = data.iloc[-4]
    candle2 = data.iloc[-3]
    candle3 = data.iloc[-2]

    if is_bullish_candle(candle3) and is_bullish_candle(candle2) and is_bullish_candle(candle1):
        if candle3.Close > candle2.Close and candle2.Close > candle1.Close:
            return True
        return False
    return False


def is_three_black_crows(data):
     # In this function we take the final 3 candles. We then check if the first two 
    # candles (excluding the latest 3rd one) satisfies the candle that it is a three
    # white soldiers.
    candle1 = data.iloc[-4]
    candle2 = data.iloc[-3]
    candle3 = data.iloc[-2]

    if is_bearish_candle(candle3) and is_bearish_candle(candle2) and is_bearish_candle(candle1):
        if candle3.Close < candle2.Close and candle2.Close < candle1.Close:
            return True
        return False
    return False


def is_morning_doji_star(data):
    # In this function we take the final 3 candles. We then check if the first two 
    # candles (excluding the latest 3rd one) satisfies the candle that it is a morning
    # doji star.
    candle1 = data.iloc[-4]
    candle2 = data.iloc[-3]
    candle3 = data.iloc[-2]

    if is_bearish_candle(candle1) and is_bullish_candle(candle3) and candle3.Close < candle1.Open:
        if candle2.Open == candle2.Close:
            return True
        return False
    return False


def is_evening_doji_star(data):
    # In this function we take the final 3 candles. We then check if the first two 
    # candles (excluding the latest 3rd one) satisfies the candle that it is a morning
    # doji star.
    candle1 = data.iloc[-4]
    candle2 = data.iloc[-3]
    candle3 = data.iloc[-2]

    if is_bullish_candle(candle1) and is_bearish_candle(candle3) and candle3.Close > candle1.Open:
        if candle2.Open == candle2.Close:
            return True
        return False
    return False


def is_rising_three_methods(data):

    candle1 = data.iloc[-6]
    candle2 = data.iloc[-5]
    candle3 = data.iloc[-4]
    candle4 = data.iloc[-3]
    candle5 = data.iloc[-2]

    if is_bullish_candle(candle1) and is_bearish_candle(candle2) \
    and is_bearish_candle(candle3) and is_bearish_candle(candle4) and is_bullish_candle(candle5):
        if candle2.Close > candle3.Close and candle3.Close > candle4.Close and \
            candle5.Close > candle1.Close:
            return True
        return False
    return False


def is_falling_three_methods(data):

    candle1 = data.iloc[-6]
    candle2 = data.iloc[-5]
    candle3 = data.iloc[-4]
    candle4 = data.iloc[-3]
    candle5 = data.iloc[-2]

    if is_bearish_candle(candle1) and is_bullish_candle(candle2) \
    and is_bullish_candle(candle3) and is_bullish_candle(candle4) and is_bearish_candle(candle5):
        if candle2.Close < candle3.Close and candle3.Close < candle4.Close and \
            candle5.Close < candle1.Close:
            return True
        return False
    return False


def is_hammer(data):

    candle1 = data.iloc[-6]
    candle2 = data.iloc[-5]
    candle3 = data.iloc[-4]
    candle4 = data.iloc[-3]
    candle5 = data.iloc[-2]

    if is_bearish_candle(candle1) and is_bearish_candle(candle2) and is_bullish_candle(candle3) \
    and is_bullish_candle(candle4) and is_bullish_candle(candle5):
        if abs(candle3.Close - candle3.Open) < abs(candle3.Open - candle3.Low): 
            return True
        return False
    return False


def is_hanging_man(data):

    candle1 = data.iloc[-6]
    candle2 = data.iloc[-5]
    candle3 = data.iloc[-4]
    candle4 = data.iloc[-3]
    candle5 = data.iloc[-2]

    if is_bullish_candle(candle1) and is_bullish_candle(candle2) and is_bearish_candle(candle3) \
    and is_bearish_candle(candle4) and is_bearish_candle(candle5):
        if abs(candle3.Open - candle3.Close) < abs(candle3.Close - candle3.Low): 
            return True
        return False
    return False


def is_inverted_hammer(data):

    candle1 = data.iloc[-6]
    candle2 = data.iloc[-5]
    candle3 = data.iloc[-4]
    candle4 = data.iloc[-3]
    candle5 = data.iloc[-2]

    if is_bearish_candle(candle1) and is_bearish_candle(candle2) and is_bullish_candle(candle3) \
    and is_bullish_candle(candle4) and is_bullish_candle(candle5):
        if abs(candle3.Close - candle3.Open) < abs(candle3.High - candle3.Close): 
            return True
        return False
    
    elif is_bearish_candle(candle1) and is_bearish_candle(candle2) and is_bearish_candle(candle3) \
    and is_bullish_candle(candle4) and is_bullish_candle(candle5):
        if abs(candle3.Close - candle3.Open) < abs(candle3.High - candle3.Open): 
            return True
        return False
    
    return False


def is_shooting_star(data):

    candle1 = data.iloc[-6]
    candle2 = data.iloc[-5]
    candle3 = data.iloc[-4]
    candle4 = data.iloc[-3]
    candle5 = data.iloc[-2]

    if is_bullish_candle(candle1) and is_bullish_candle(candle2) and is_bullish_candle(candle3) \
    and is_bearish_candle(candle4) and is_bearish_candle(candle5):
        if abs(candle3.Close - candle3.Open) < abs(candle3.High - candle3.Close): 
            return True
        return False

    elif is_bullish_candle(candle1) and is_bullish_candle(candle2) and is_bearish_candle(candle3) \
    and is_bearish_candle(candle4) and is_bearish_candle(candle5):
        if abs(candle3.Close - candle3.Open) < abs(candle3.High - candle3.Open): 
            return True
        return False
    
    return False


def is_bullish_kicker(data):

    candle1 = data.iloc[-5]
    candle2 = data.iloc[-4]
    candle3 = data.iloc[-3]
    candle4 = data.iloc[-2]

    if is_bearish_candle(candle1) and is_bearish_candle(candle2) and is_bearish_candle(candle3) \
    and is_bullish_candle(candle4):
        if candle4.Close > candle3.Open:
            return True
        return False
    
    return False


def is_bearish_kicker(data):

    candle1 = data.iloc[-5]
    candle2 = data.iloc[-4]
    candle3 = data.iloc[-3]
    candle4 = data.iloc[-2]

    if is_bullish_candle(candle1) and is_bullish_candle(candle2) and is_bullish_candle(candle3) \
    and is_bearish_candle(candle4):
        if candle4.Close < candle3.Open:
            return True
        return False    
    return False


def is_bullish_harami(data):

    candle1 = data.iloc[-3]
    candle2 = data.iloc[-2]

    if is_bearish_candle(candle1) and is_bullish_candle(candle2):
        if candle2.Close < candle1.Open and candle2.Open > candle1.Close:
            return True
        return False
    return False


def is_bearish_harami(data):

    candle1 = data.iloc[-3]
    candle2 = data.iloc[-2]

    if is_bullish_candle(candle1) and is_bearish_candle(candle2):
        if candle2.Close > candle1.Open and candle2.Open < candle1.Close:
            return True
        return False
    return False


def is_bullish_three_line_strike(data):

    candle1 = data.iloc[-5]
    candle2 = data.iloc[-4]
    candle3 = data.iloc[-3]
    candle4 = data.iloc[-2]

    if is_bearish_candle(candle1) and is_bearish_candle(candle2) and is_bearish_candle(candle3) \
    and is_bullish_candle(candle4):
        if candle2.Close < candle1.Close and candle3.Close < candle2.Close and \
        candle4.Close > candle1.Open:
            return True
        return False

    return False


def is_bearish_three_line_strike(data):

    candle1 = data.iloc[-5]
    candle2 = data.iloc[-4]
    candle3 = data.iloc[-3]
    candle4 = data.iloc[-2]

    if is_bullish_candle(candle1) and is_bullish_candle(candle2) and is_bullish_candle(candle3) \
    and is_bearish_candle(candle4):
        if candle2.Close > candle1.Close and candle3.Close > candle2.Close and \
        candle4.Close < candle1.Open:
            return True
        return False

    return False


def moving_average(type, number, data):
    latest_moving_average_value = 0
    if type == 'SMA':
        latest_moving_average_value = ta.sma(data['Close'], length=number)
    elif type == 'EMA':
        latest_moving_average_value = ta.ema(data['Close'], length=number)
    return latest_moving_average_value.iloc[-1]


def bbands(condition, band, data):
    band = band.upper()
    condition = condition.upper()
    bbands = ta.bbands(data['Close'], length=20, std=2)  # By default, it calculates with a length of 20 and std of 2
    upper_band = bbands['BBU_20_2.0'].iloc[-1]
    middle_band = bbands['BBM_20_2.0'].iloc[-1]
    lower_band = bbands['BBL_20_2.0'].iloc[-1]
    current_price = data.Close.iloc[-1]

    if condition == 'LT' and band == 'LOWER':
        if current_price < lower_band:
            return True
        return False
    elif condition == 'LT' and band == 'MIDDLE':
        if current_price < middle_band:
            return True
        return False
    elif condition == 'LT' and band == 'UPPER':
        if current_price < upper_band:
            return True
        return False
    
    elif condition == 'GT' and band == 'LOWER':
        if current_price > lower_band:
            return True
        return False
    elif condition == 'GT' and band == 'MIDDLE':
        if current_price > middle_band:
            return True
        return False
    elif condition == 'GT' and band == 'UPPER':
        if current_price > upper_band:
            return True
        return False
    return False


def momentum(comparison, threshold, data):
    comparison = comparison.upper()
    current_momentum = ta.mom(data['Close']).iloc[-1]
    if comparison == 'ABOVE':
        if current_momentum > threshold:
            return True
        return False
    elif comparison == 'BELOW':
        if current_momentum < threshold:
            return True
        return False
    return False


def rsi(comparison, rsi_level, data):
    comparison = comparison.upper()
    current_rsi = ta.rsi(data['Close']).iloc[-1]
    if comparison == 'ABOVE':
        if current_rsi > rsi_level:
            return True
        return False
    elif comparison == 'BELOW':
        if current_rsi < rsi_level:
            return True
        return False
    return False


# Function to split Dataset By Year 
def split_df(df, start_year, end_year):
    # Convert the index to datetime if it's not already
    df.index = pd.to_datetime(df.index)
    
    # Filter rows based on the specified start and end years
    new_df = df[(df.index.year >= start_year) & (df.index.year <= end_year)]

    return new_df


async def genesys_backest(generated_code, start_year, end_year, chosen_dataset, initial_capital):

    class GenesysBacktest(Strategy):
        def init(self):
            price = self.data.Close
            self.current_equity = 0
            self.true_init_equity = init_capital

        def set_take_profit(self, number, type_of_setting):
            current_equity = self.equity
            # print(f'Current Equity: {current_equity}\n')
            type_of_setting = type_of_setting.upper()
            number = float(number)
            
            # print(f'self.init_equity: {self.init_equity} vs current equity: {current_equity} with diff: {((current_equity - self.init_equity) / self.true_init_equity) * 100}')
            if type_of_setting == 'PERCENTAGE':
                percentage = ((current_equity - self.current_equity) / self.true_init_equity) * 100
                if percentage >= number:
                    self.position.close()
            elif type_of_setting == 'NUMBER':
                difference = current_equity - self.current_equity
                if difference >= number:
                    self.position.close()
        

        def set_stop_loss(self, number, type_of_setting):
            type_of_setting = type_of_setting.upper()
            number = -(float(number))
            current_equity = self.equity
            if type_of_setting == 'PERCENTAGE':
                percentage = ((current_equity - self.current_equity) / self.true_init_equity) * 100
                if percentage <= number:
                    self.position.close()
            elif type_of_setting == 'NUMBER':
                difference = current_equity - self.current_equity
                if difference <= number:
                    self.position.close()
                    

          
        def next(self):
            dataset = pd.DataFrame({'Open': self.data.Open, 'High': self.data.High, 'Low': self.data.Low, 'Close': self.data.Close, 'Volume': self.data.Volume})
            try:
                exec(generated_code)    
            except Exception as e:
                print(f'Exception: {e}')
    try:
        # Query the model asynchronously using sync_to_async
        queryset = await sync_to_async(SaveDataset.objects.all().first)()
        dataset_to_use = f'./{chosen_dataset}'
        df_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), dataset_to_use)
        df = pd.read_csv(df_path).drop_duplicates()
        df.index = pd.to_datetime(df['Time'].values)
        del df['Time']
        
        # split_queryset = await sync_to_async(SplitDataset.objects.get)()

        start_year = int(start_year)
        end_year = int(end_year)
        new_df = split_df(df, start_year, end_year)
        # print(df)
        
        # init_capital_queryset = await sync_to_async(SetInitCapital.objects.get)()
        # init_capital = float(init_capital_queryset.initial_capital)
        init_capital = initial_capital

        bt = Backtest(new_df, GenesysBacktest,
                exclusive_orders=True, cash=init_capital)

        output = bt.run()

        # return_plot = False

        # if len(new_df) > 5000:
        #     return_plot = True
        # else:
        #     return_plot = False
        try:
            p = bt.plot()
                
            item = json_item(p, "new_plot")
            # print(item)
                
            plot_json = json.dumps(item)
        except Exception as e:
            plot_json = {}
        
        # Convert the relevant output fields to a dictionary
        result_dict = {
            "Start": str(output['Start']),
            "End": str(output['End']),
            "Duration": str(output['Duration']),
            "Exposure Time [%]": str(output['Exposure Time [%]']),
            "Equity Final [$]": str(output['Equity Final [$]']),
            "Equity Peak [$]": str(output['Equity Peak [$]']),
            "Return [%]": str(output['Return [%]']),
            "Buy & Hold Return [%]": str(output['Buy & Hold Return [%]']),
            "Return (Ann.) [%]": str(output['Return (Ann.) [%]']),
            "Volatility (Ann.) [%]": str(output['Volatility (Ann.) [%]']),
            "Sharpe Ratio": str(output['Sharpe Ratio']),
            "Sortino Ratio": str(output['Sortino Ratio']),
            "Calmar Ratio": str(output['Calmar Ratio']),
            "Max. Drawdown [%]": str(output['Max. Drawdown [%]']),
            "Avg. Drawdown [%]": str(output['Avg. Drawdown [%]']),
            "Max. Drawdown Duration": str(output['Max. Drawdown Duration']),
            "Avg. Drawdown Duration": str(output['Avg. Drawdown Duration']),
            "# Trades": str(output['# Trades']),
            "Win Rate [%]": str(output['Win Rate [%]']),
            "Best Trade [%]": str(output['Best Trade [%]']),
            "Worst Trade [%]": str(output['Worst Trade [%]']),
            "Avg. Trade [%]": str(output['Avg. Trade [%]']),
            "Max. Trade Duration": str(output['Max. Trade Duration']),
            "Avg. Trade Duration": str(output['Avg. Trade Duration']),
            "Profit Factor": str(output['Profit Factor']),
            "Expectancy [%]": str(output['Expectancy [%]']),
            # "SQN": output['SQN'],
        }
        return result_dict, plot_json
    except Exception as e:
        return JsonResponse({'error': str(e)})


@csrf_exempt
def genesys(request):
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            generated_code = data.get('generatedCode', '')

            # Execute the generated code

            try:
                async def inner_genesys_backtest():
                    result = await genesys_backest(generated_code, start_year, end_year, chosen_dataset, initial_capital)
                    return JsonResponse({'message': result})

                # Run the asynchronous code using the event loop
                loop = asyncio.get_event_loop()
                return loop.run_until_complete(inner_genesys_backtest())
                    
                # return JsonResponse({'message': f'{result}'})
            except Exception as e:
                return JsonResponse({'error': f'Error executing code: {str(e)}'}, status=400)
        except json.JSONDecodeError:
            return JsonResponse({'error': 'Invalid JSON data'}, status=400)
    else:
        return JsonResponse({'message': 'api-call works!'})

async def run_genesys_backtests():
    """
    Periodically fetches untested backtest models, runs the backtest, and stores the results.
    """
    try:
        # Get all untested models
        untested_models = await sync_to_async(list)(BacktestModels.objects.filter(model_backtested=False))
        
        for model in untested_models:
            try:
                # Run the backtest
                result_dict, plot_json = await genesys_backest(
                    model.generated_code,
                    model.dataset_start,
                    model.dataset_end,
                    model.chosen_dataset,
                    model.initial_capital
                )
                
                # Parse dates
                from datetime import datetime
                start_date = datetime.strptime(result_dict.get("Start"), "%Y-%m-%d %H:%M:%S").date()
                end_date = datetime.strptime(result_dict.get("End"), "%Y-%m-%d %H:%M:%S").date()

                # Create the result object using sync_to_async
                await sync_to_async(BacktestResult.objects.create)(
                    backtest_model=model,
                    start=start_date,
                    end=end_date,
                    duration=result_dict.get("Duration"),
                    exposure_time=float(result_dict.get("Exposure Time [%]", 0)),
                    equity_final=float(result_dict.get("Equity Final [$]", 0)),
                    equity_peak=float(result_dict.get("Equity Peak [$]", 0)),
                    return_percent=float(result_dict.get("Return [%]", 0)),
                    buy_hold_return=float(result_dict.get("Buy & Hold Return [%]", 0)),
                    annual_return=float(result_dict.get("Return (Ann.) [%]", 0)),
                    volatility_annual=float(result_dict.get("Volatility (Ann.) [%]", 0)),
                    sharpe_ratio=float(result_dict.get("Sharpe Ratio", 0)),
                    sortino_ratio=float(result_dict.get("Sortino Ratio", 0)),
                    calmar_ratio=float(result_dict.get("Calmar Ratio", 0)),
                    max_drawdown=float(result_dict.get("Max. Drawdown [%]", 0)),
                    avg_drawdown=float(result_dict.get("Avg. Drawdown [%]", 0)),
                    max_drawdown_duration=result_dict.get("Max. Drawdown Duration"),
                    avg_drawdown_duration=result_dict.get("Avg. Drawdown Duration"),
                    num_trades=int(result_dict.get("# Trades", 0)),
                    win_rate=float(result_dict.get("Win Rate [%]", 0)),
                    best_trade=float(result_dict.get("Best Trade [%]", 0)),
                    worst_trade=float(result_dict.get("Worst Trade [%]", 0)),
                    avg_trade=float(result_dict.get("Avg. Trade [%]", 0)),
                    max_trade_duration=result_dict.get("Max. Trade Duration"),
                    avg_trade_duration=result_dict.get("Avg. Trade Duration"),
                    profit_factor=float(result_dict.get("Profit Factor", 0)),
                    expectancy=float(result_dict.get("Expectancy [%]", 0)),
                    plot_json=plot_json
                )

                # Update model status
                model.model_backtested = True
                await sync_to_async(model.save)()
                
                print(f"Successfully processed backtest for model {model.id}")
                
            except Exception as e:
                print(f"Error processing backtest for model {model.id}: {str(e)}")
                # Consider adding error tracking to your model
                # model.error_message = str(e)
                # await sync_to_async(model.save)()
    
    except Exception as e:
        print(f"Error in run_genesys_backtests: {str(e)}")

        # except Exception as e:
        #     print(f"Error processing backtest for {model}: {e}")
from asgiref.sync import async_to_sync  # Import async_to_sync to call async functions

@csrf_exempt
def trigger_backtest(request):
    try:
        # Create a new event loop for this request
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            # Run the async function in this new loop
            loop.run_until_complete(run_genesys_backtests())
            return JsonResponse({"status": "success", "message": "Backtest completed successfully."}, status=200)
        except Exception as e:
            return JsonResponse({"status": "error", "message": str(e)}, status=500)
        finally:
            # Always close the loop
            loop.close()
    except Exception as e:
        return JsonResponse({"status": "error", "message": f"Loop error: {str(e)}"}, status=500)


# import asyncio

# Replace your current scheduler code with this

def run_genesys_backtests_wrapper():
    """
    Wrapper function that sets up the event loop and runs the async function
    """
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        return loop.run_until_complete(run_genesys_backtests())
    finally:
        loop.close()

# Schedule the wrapper function instead of the async function directly
scheduler.add_job(
    run_genesys_backtests_wrapper,
    trigger=IntervalTrigger(minutes=50),
    id='run_genesys_backtests',
    name='Update genesys backtests every 50 minutes',
    replace_existing=True
)


@csrf_exempt
def fetch_backtested_results(request):
    try:
        # Get all backtest results ordered by creation time in descending order
        results = BacktestResult.objects.select_related('backtest_model').order_by('-created_at')
        
        # Group results by backtest model
        grouped_results = {}
        
        for result in results:
            model_id = result.backtest_model.id if result.backtest_model else 'unknown'
            
            if model_id not in grouped_results:
                # If this is the first result for this model, initialize the model info
                if result.backtest_model:
                    grouped_results[model_id] = {
                        'model_info': {
                            'id': result.backtest_model.id,
                            'dataset': result.backtest_model.chosen_dataset,
                            'start_date': result.backtest_model.dataset_start,
                            'end_date': result.backtest_model.dataset_end,
                            'initial_capital': result.backtest_model.initial_capital,
                            'code_snippet': result.backtest_model.generated_code,

                            # 'code_snippet': result.backtest_model.generated_code if len(result.backtest_model.generated_code) > 200 else result.backtest_model.generated_code,
                        },
                        'results': []
                    }
                else:
                    grouped_results[model_id] = {
                        'model_info': {
                            'id': 'unknown',
                            'dataset': 'Unknown',
                            'start_date': 'Unknown',
                            'end_date': 'Unknown',
                            'initial_capital': 0,
                            'code_snippet': 'No code available',
                        },
                        'results': []
                    }
            
            # Add this result to the appropriate group
            grouped_results[model_id]['results'].append({
                'id': result.id,
                'start': result.start,
                'end': result.end,
                'duration': result.duration,
                'exposure_time': result.exposure_time,
                'equity_final': result.equity_final,
                'equity_peak': result.equity_peak,
                'return_percent': result.return_percent,
                'buy_hold_return': result.buy_hold_return,
                'annual_return': result.annual_return,
                'volatility_annual': result.volatility_annual,
                'sharpe_ratio': result.sharpe_ratio,
                'sortino_ratio': result.sortino_ratio,
                'calmar_ratio': result.calmar_ratio,
                'max_drawdown': result.max_drawdown,
                'avg_drawdown': result.avg_drawdown,
                'max_drawdown_duration': result.max_drawdown_duration,
                'avg_drawdown_duration': result.avg_drawdown_duration,
                'num_trades': result.num_trades,
                'win_rate': result.win_rate,
                'best_trade': result.best_trade,
                'worst_trade': result.worst_trade,
                'avg_trade': result.avg_trade,
                'max_trade_duration': result.max_trade_duration,
                'avg_trade_duration': result.avg_trade_duration,
                'profit_factor': result.profit_factor,
                'expectancy': result.expectancy,
                'created_at': result.created_at,
                'has_plot': bool(result.plot_json),
                'plot_json': result.plot_json
            })
        
        # Convert to list for easier handling in React
        results_list = list(grouped_results.values())
        
        return JsonResponse({'status': 'success', 'data': results_list})
    
    except Exception as e:
        return JsonResponse({'status': 'error', 'message': str(e)}, status=500)




@csrf_exempt
def delete_backtest_model(request):
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            model_id = data.get('model_id')
            
            if not model_id:
                return JsonResponse({'status': 'error', 'message': 'Model ID is required'})
                
            model = BacktestModels.objects.get(id=model_id)
            # This will also delete all related BacktestResult objects due to CASCADE
            model.delete()
            
            return JsonResponse({'status': 'success', 'message': 'Model and associated results deleted'})
        except BacktestModels.DoesNotExist:
            return JsonResponse({'status': 'error', 'message': 'Model not found'})
        except Exception as e:
            return JsonResponse({'status': 'error', 'message': str(e)})
    
    return JsonResponse({'status': 'error', 'message': 'Only POST method is allowed'})

# @csrf_exempt
# async def test_async_backtest(request):
#     try:
#         # Run the asynchronous backtest function
#         await run_genesys_backtests()
#         return JsonResponse({'message': 'Backtest Successful!'})
#     except Exception as e:
#         return JsonResponse({'message': str(e)})


@csrf_exempt
def save_dataset(request, dataset):
    dataset_to_save = dataset
    try:
        try:
            # If there are no saved objects
            SaveDataset.objects.all().delete()
        except Exception as e:
            pass
        new_dataset = SaveDataset(dataset=dataset_to_save)
        new_dataset.save()

        query_test = SaveDataset.objects.all().first().dataset
        return JsonResponse({'saved-dataset': f'{query_test}'})

    except Exception as e:
        return JsonResponse({'error': f'Error Occured: {e}'})
    

@csrf_exempt
def split_dataset(request):

    try:
        SplitDataset.objects.all().delete()
    except Exception as e:
        pass

    if request.method == 'POST':
        # Parse the request data
        data = json.loads(request.body)
        start_year = data.get('start_year')
        end_year = data.get('end_year')

        # Perform any necessary validation
        # For example, check if start_year and end_year are valid integers

        # Save the start and end years to the database
        split_dataset = SplitDataset.objects.create(
            start_year=start_year,
            end_year=end_year
        )

        # Return a success response
        return JsonResponse({'message': 'Start and end years saved successfully.'})

    # If the request method is not POST, return an error response
    return JsonResponse({'error': 'Only POST requests are allowed.'}, status=405)


@csrf_exempt
def set_init_capital(request):

    try:
        SetInitCapital.objects.all().delete()
    except Exception as e:
        pass

    if request.method == 'POST':
        data = json.loads(request.body)
        initial_capital = data.get('initialCapital')

        # Save initial capital to the database
        SetInitCapital.objects.create(initial_capital=initial_capital)

        return JsonResponse({'message': 'Initial capital saved successfully'})
    else:
        return JsonResponse({'error': 'Method not allowed'}, status=405)


# def obtain_dataset(asset, interval, num_days):

#     # Calculate the date 30 days ago from the current day
#     start_date = (datetime.now() - timedelta(days=num_days)).strftime("%Y-%m-%d")

#     # Get latest candle
#     end_date = (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")

#     # Download data using the calculated dates
#     forex_asset = f"{asset}=X"
#     data = yf.download(forex_asset, start=start_date, end=end_date, interval=interval)

#     return data


def generate_trading_image(df):
    df.index = pd.to_datetime(df.index)
    style = mpf.make_mpf_style(base_mpf_style='classic')
    fig, axes = mpf.plot(df, type='line', volume=False, returnfig=True, style=style)
    plt.close(fig)
    output_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'trading_chart.png')
    fig.savefig(output_path)


@csrf_exempt
def test_cnn(request, asset, interval, num_days):
    dataset = obtain_dataset(asset=asset, interval=interval, num_days=num_days)
    # classification = image_classification(data=dataset)
    up = is_uptrend(data=dataset)
    down = is_downtrend(data=dataset)
    ranger = is_ranging_market(data=dataset)
    return JsonResponse({'classification': f'  Uptrend: {up}\nDowntrend: {down}\nRanging Market: {ranger}'})


# def image_classification(data):
#     generate_trading_image(df=data)
#     # print('Loading Model')
#     url = "https://us-central1-glowing-road-419608.cloudfunctions.net/function-1"
#     # path_to_image = '/candlestick_chart.png'
#     path_to_image = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'trading_chart.png')


#     with open(path_to_image, 'rb') as image_file:
#         # Read the image file and encode it as base64
#         image_data = base64.b64encode(image_file.read()).decode('utf-8')

#         headers = {'Content-Type': 'application/json'}
#         payload = {'data': image_data}

#         try:
#             response = requests.post(url, json=payload, headers=headers)
#             response.raise_for_status()  # Check for HTTP errors

#             # Extract the predictions from the response
#             response_data = response.json()
#             # predictions = response_data['predictions'][0]

#             # Find the index of the highest prediction
#             # highest_pred_index = predictions.index(max(predictions))

#             # Map the index to the corresponding class name
#             # predicted_class = class_names[highest_pred_index]

#             # print(f"Predictions: {predictions}")
            
#             # print(f"Highest prediction index: {highest_pred_index}")
#             # print(f"Predicted class: {predicted_class}")
#             # print(response_data)
#             return response_data['response']

#         except requests.exceptions.RequestException as e:
#             return JsonResponse({'Error in image classification function: ': f"{e}"})


# def is_uptrend(data):
#   image_class = image_classification(data=data)
# #   print(f'Uptrend Function: {image_class}')
#   if image_class == 'uptrend':
#     return True
#   else:
#     return False


# def is_downtrend(data):
#   image_class = image_classification(data=data)
# #   print(f'Downtrend Function: {image_class}')
#   if image_class == 'downtrend':
#     return True
#   else:
#     return False


# def is_ranging_market(data):
#   image_class = image_classification(data=data)
# #   print(f'Ranging Market Function: {image_class}')
#   if image_class == 'ranging market':
#     return True
#   else:
#     return False


def support_and_resistance(df):
    peaks_range = [2, 3]
    num_peaks = -999

    sample_df = df
    sample = sample_df['Close'].to_numpy().flatten()
    sample_original = sample.copy()

    maxima = argrelextrema(sample, np.greater)
    minima = argrelextrema(sample, np.less)

    extrema = np.concatenate((maxima, minima), axis=1)[0]
    extrema_prices = np.concatenate((sample[maxima], sample[minima]))
    interval = extrema_prices[0] / 10000

    bandwidth = interval

    while num_peaks < peaks_range[0] or num_peaks > peaks_range[1]:
        initial_price = extrema_prices[0]
        kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(extrema_prices.reshape(-1, 1))

        a, b = min(extrema_prices), max(extrema_prices)
        price_range = np.linspace(a, b, 1000).reshape(-1, 1)

        pdf = np.exp(kde.score_samples(price_range))
        peaks = find_peaks(pdf)[0]
        num_peaks = len(peaks)
        bandwidth += interval

        if bandwidth > 100 * interval:
            print('Failed to converge, stopping...')
            break

    new_price_range = price_range[peaks]
    new_price_range = np.delete(new_price_range, 1, axis=0)

    # Return support and resistance levels
    return new_price_range


def is_support_level(data):
    support_level = support_and_resistance(data)[0][0]
    latest_price = data.iloc[-1].Close
    return latest_price <= support_level


def is_resistance_level(data):
    resistance_level = support_and_resistance(data)[1][0]
    latest_price = data.iloc[-1].Close
    return latest_price >= resistance_level



import pandas as pd
import numpy as np

# ==================== DIP DETECTION ALGORITHMS ====================

def detect_rsi_dip(close, period=14, oversold=30):
    """
    Detect dips using RSI (Relative Strength Index).
    Returns True when RSI is oversold.
    """
    delta = np.diff(close, prepend=close[0])
    gain = np.where(delta > 0, delta, 0)
    loss = np.where(delta < 0, -delta, 0)
    
    avg_gain = np.convolve(gain, np.ones(period)/period, mode='same')
    avg_loss = np.convolve(loss, np.ones(period)/period, mode='same')
    
    rs = np.divide(avg_gain, avg_loss, out=np.zeros_like(avg_gain), where=avg_loss!=0)
    rsi = 100 - (100 / (1 + rs))
    
    return rsi, rsi < oversold


def detect_bollinger_dip(close, period=20, std_dev=2):
    """
    Detect dips using Bollinger Bands.
    Returns True when price touches or goes below lower band.
    """
    sma = np.convolve(close, np.ones(period)/period, mode='same')
    
    # Calculate rolling standard deviation
    rolling_std = np.array([
        np.std(close[max(0, i-period):i+1]) if i >= period-1 else np.std(close[:i+1])
        for i in range(len(close))
    ])
    
    lower_band = sma - (std_dev * rolling_std)
    upper_band = sma + (std_dev * rolling_std)
    
    return lower_band, upper_band, close <= lower_band


def detect_price_momentum_dip(close, short_period=10, long_period=30, threshold=-0.02):
    """
    Detect dips using price momentum.
    Returns True when short-term momentum drops significantly below long-term.
    """
    short_ma = np.convolve(close, np.ones(short_period)/short_period, mode='same')
    long_ma = np.convolve(close, np.ones(long_period)/long_period, mode='same')
    
    momentum_diff = (short_ma - long_ma) / long_ma
    
    return momentum_diff, momentum_diff < threshold


def detect_percentage_pullback(close, lookback=20, pullback_pct=0.03):
    """
    Detect dips based on percentage pullback from recent high.
    Returns True when price has pulled back by specified percentage.
    """
    rolling_max = np.array([
        np.max(close[max(0, i-lookback):i+1])
        for i in range(len(close))
    ])
    
    pullback = (rolling_max - close) / rolling_max
    
    return rolling_max, pullback >= pullback_pct


# ==================== MAIN BUY_HOLD FUNCTION ====================

def buy_hold(dataset, 
             dip_threshold=2,
             rsi_period=14, 
             rsi_oversold=35,
             bb_period=20, 
             bb_std=2,
             momentum_short=10, 
             momentum_long=30,
             pullback_period=20, 
             pullback_pct=0.025):
    """
    Determines if buy conditions are met based on dip detection algorithms.
    
    Parameters:
    -----------
    dataset : pd.DataFrame
        DataFrame with OHLCV data (must have 'Close' column)
    dip_threshold : int
        Minimum number of dip conditions to trigger buy (default: 2)
    rsi_period : int
        RSI calculation period (default: 14)
    rsi_oversold : float
        RSI oversold threshold (default: 35)
    bb_period : int
        Bollinger Bands period (default: 20)
    bb_std : float
        Bollinger Bands standard deviation multiplier (default: 2)
    momentum_short : int
        Short-term momentum period (default: 10)
    momentum_long : int
        Long-term momentum period (default: 30)
    pullback_period : int
        Period for pullback calculation (default: 20)
    pullback_pct : float
        Minimum pullback percentage to trigger signal (default: 0.025)
    
    Returns:
    --------
    bool : True if buy conditions are met, False otherwise
    
    Usage in Genesys:
    -----------------
    if buy_hold(dataset):
        self.buy(size=0.95)
        self.current_equity = self.equity
    """
    
    # Get current index (last bar)
    try:
            
        current_idx = len(dataset) - 1
        
        # Need enough data for indicators
        min_periods = max(rsi_period, bb_period, momentum_long, pullback_period)
        if current_idx < min_periods:
            return False  # Not enough data yet
        
        # Extract close prices as numpy array
        close = dataset['Close'].values
        
        # ==================== CALCULATE INDICATORS ====================
        
        # 1. RSI Dip Detection
        rsi, rsi_dip = detect_rsi_dip(close, rsi_period, rsi_oversold)
        rsi_signal = rsi_dip[-1]
        
        # 2. Bollinger Bands Dip Detection
        lower_bb, upper_bb, bb_dip = detect_bollinger_dip(close, bb_period, bb_std)
        bb_signal = bb_dip[-1]
        
        # 3. Price Momentum Dip Detection
        momentum, momentum_dip = detect_price_momentum_dip(close, momentum_short, momentum_long)
        momentum_signal = momentum_dip[-1]
        
        # 4. Percentage Pullback Detection
        recent_high, pullback_dip = detect_percentage_pullback(close, pullback_period, pullback_pct)
        pullback_signal = pullback_dip[-1]
        
        # ==================== CALCULATE DIP SCORE ====================
        
        dip_score = (
            int(rsi_signal) + 
            int(bb_signal) + 
            int(momentum_signal) + 
            int(pullback_signal)
        )
        
        # Return True if dip threshold is met
        return dip_score >= dip_threshold
    
    except Exception as e:
        print(f'Exception occured in buy_hold system: {e}')
        return False # no trade


# ==================== REGIME DETECTION ====================

def detect_market_regime(close, high, low, atr_period=14, adx_period=14, 
                         trending_threshold=25, ranging_threshold=20):
    """
    Detect market regime using ADX (Average Directional Index) and ATR.
    
    Returns:
    --------
    regime : array
        0 = Ranging, 1 = Trending Up, -1 = Trending Down
    adx : array
        ADX values
    """
    # Calculate True Range
    tr1 = high - low
    tr2 = np.abs(high - np.roll(close, 1))
    tr3 = np.abs(low - np.roll(close, 1))
    tr = np.maximum(tr1, np.maximum(tr2, tr3))
    tr[0] = tr1[0]  # Fix first value
    
    # Calculate ATR
    atr = np.convolve(tr, np.ones(atr_period)/atr_period, mode='same')
    
    # Calculate Directional Movement
    high_diff = np.diff(high, prepend=high[0])
    low_diff = -np.diff(low, prepend=low[0])
    
    plus_dm = np.where((high_diff > low_diff) & (high_diff > 0), high_diff, 0)
    minus_dm = np.where((low_diff > high_diff) & (low_diff > 0), low_diff, 0)
    
    # Smooth the directional movements
    plus_dm_smooth = np.convolve(plus_dm, np.ones(adx_period)/adx_period, mode='same')
    minus_dm_smooth = np.convolve(minus_dm, np.ones(adx_period)/adx_period, mode='same')
    
    # Calculate Directional Indicators
    plus_di = 100 * plus_dm_smooth / (atr + 1e-10)
    minus_di = 100 * minus_dm_smooth / (atr + 1e-10)
    
    # Calculate DX and ADX
    dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di + 1e-10)
    adx = np.convolve(dx, np.ones(adx_period)/adx_period, mode='same')
    
    # Determine regime
    regime = np.zeros(len(close))
    regime[adx >= trending_threshold] = np.where(
        plus_di[adx >= trending_threshold] > minus_di[adx >= trending_threshold], 
        1, -1
    )
    regime[adx < ranging_threshold] = 0
    
    # For values between thresholds, keep previous regime (hysteresis)
    mask = (adx >= ranging_threshold) & (adx < trending_threshold)
    for i in range(1, len(regime)):
        if mask[i]:
            regime[i] = regime[i-1]
    
    return regime, adx, plus_di, minus_di

# ==================== RALLY DETECTION ALGORITHMS ====================

def detect_rsi_rally(close, period=14, overbought=70):
    """
    Detect rallies using RSI (Relative Strength Index).
    Returns True when RSI is overbought.
    """
    delta = np.diff(close, prepend=close[0])
    gain = np.where(delta > 0, delta, 0)
    loss = np.where(delta < 0, -delta, 0)
    
    avg_gain = np.convolve(gain, np.ones(period)/period, mode='same')
    avg_loss = np.convolve(loss, np.ones(period)/period, mode='same')
    
    rs = np.divide(avg_gain, avg_loss, out=np.zeros_like(avg_gain), where=avg_loss!=0)
    rsi = 100 - (100 / (1 + rs))
    
    return rsi, rsi > overbought


def detect_bollinger_rally(close, period=20, std_dev=2):
    """
    Detect rallies using Bollinger Bands.
    Returns True when price touches or goes above upper band.
    """
    sma = np.convolve(close, np.ones(period)/period, mode='same')
    
    # Calculate rolling standard deviation
    rolling_std = np.array([
        np.std(close[max(0, i-period):i+1]) if i >= period-1 else np.std(close[:i+1])
        for i in range(len(close))
    ])
    
    lower_band = sma - (std_dev * rolling_std)
    upper_band = sma + (std_dev * rolling_std)
    
    return lower_band, upper_band, close >= upper_band


def detect_price_momentum_rally(close, short_period=10, long_period=30, threshold=0.02):
    """
    Detect rallies using price momentum.
    Returns True when short-term momentum rises significantly above long-term.
    """
    short_ma = np.convolve(close, np.ones(short_period)/short_period, mode='same')
    long_ma = np.convolve(close, np.ones(long_period)/long_period, mode='same')
    
    momentum_diff = (short_ma - long_ma) / long_ma
    
    return momentum_diff, momentum_diff > threshold


def detect_percentage_pump(close, lookback=20, pump_pct=0.03):
    """
    Detect rallies based on percentage rise from recent low.
    Returns True when price has pumped by specified percentage.
    """
    rolling_min = np.array([
        np.min(close[max(0, i-lookback):i+1])
        for i in range(len(close))
    ])
    
    pump = (close - rolling_min) / rolling_min
    
    return rolling_min, pump >= pump_pct


# ==================== MAIN SELL_HOLD FUNCTION ====================

def sell_hold(dataset, 
              rally_threshold=2,
              rsi_period=14, 
              rsi_overbought=65,
              bb_period=20, 
              bb_std=2,
              momentum_short=10, 
              momentum_long=30,
              pump_period=20, 
              pump_pct=0.025):
    """
    Determines if sell conditions are met based on rally detection algorithms.
    
    Parameters:
    -----------
    dataset : pd.DataFrame
        DataFrame with OHLCV data (must have 'Close' column)
    rally_threshold : int
        Minimum number of rally conditions to trigger sell (default: 2)
    rsi_period : int
        RSI calculation period (default: 14)
    rsi_overbought : float
        RSI overbought threshold (default: 65)
    bb_period : int
        Bollinger Bands period (default: 20)
    bb_std : float
        Bollinger Bands standard deviation multiplier (default: 2)
    momentum_short : int
        Short-term momentum period (default: 10)
    momentum_long : int
        Long-term momentum period (default: 30)
    pump_period : int
        Period for pump calculation (default: 20)
    pump_pct : float
        Minimum pump percentage to trigger signal (default: 0.025)
    
    Returns:
    --------
    bool : True if sell conditions are met, False otherwise
    
    Usage in Genesys:
    -----------------
    if sell_hold(dataset):
        self.sell(size=0.95)
        self.current_equity = self.equity
    """
    
    # Get current index (last bar)
    try:
            
        current_idx = len(dataset) - 1
        
        # Need enough data for indicators
        min_periods = max(rsi_period, bb_period, momentum_long, pump_period)
        if current_idx < min_periods:
            return False  # Not enough data yet
        
        # Extract close prices as numpy array
        close = dataset['Close'].values
        
        # ==================== CALCULATE INDICATORS ====================
        
        # 1. RSI Rally Detection
        rsi, rsi_rally = detect_rsi_rally(close, rsi_period, rsi_overbought)
        rsi_signal = rsi_rally[-1]
        
        # 2. Bollinger Bands Rally Detection
        lower_bb, upper_bb, bb_rally = detect_bollinger_rally(close, bb_period, bb_std)
        bb_signal = bb_rally[-1]
        
        # 3. Price Momentum Rally Detection
        momentum, momentum_rally = detect_price_momentum_rally(close, momentum_short, momentum_long)
        momentum_signal = momentum_rally[-1]
        
        # 4. Percentage Pump Detection
        recent_low, pump_rally = detect_percentage_pump(close, pump_period, pump_pct)
        pump_signal = pump_rally[-1]
        
        # ==================== CALCULATE RALLY SCORE ====================
        
        rally_score = (
            int(rsi_signal) + 
            int(bb_signal) + 
            int(momentum_signal) + 
            int(pump_signal)
        )
        
        # Return True if rally threshold is met
        return rally_score >= rally_threshold
    
    except Exception as e:
        print(f'Exception occured in sell_hold system: {e}')
        return False  # no trade


# ==================== MAIN BUY_HOLD_REGIME FUNCTION ====================

def buy_hold_regime(dataset,
                    atr_period=14,
                    adx_period=14,
                    trending_threshold=25,
                    ranging_threshold=20,
                    trending_dip_threshold=2,
                    ranging_dip_threshold=3,
                    downtrend_dip_threshold=4,
                    rsi_period=14,
                    rsi_oversold=35,
                    bb_period=20,
                    bb_std=2,
                    momentum_short=10,
                    momentum_long=30,
                    pullback_period=20,
                    pullback_pct=0.025):
    """
    Determines if buy conditions are met based on market regime and dip detection.
    
    Adapts strategy based on detected market regime:
    - Trending Up (regime=1): Buys dips aggressively
    - Ranging (regime=0): More selective with dips
    - Trending Down (regime=-1): Only extreme dips
    
    Parameters:
    -----------
    dataset : pd.DataFrame
        DataFrame with OHLCV data (must have 'Open', 'High', 'Low', 'Close', 'Volume')
    atr_period : int
        ATR calculation period (default: 14)
    adx_period : int
        ADX calculation period (default: 14)
    trending_threshold : float
        ADX threshold for trending market (default: 25)
    ranging_threshold : float
        ADX threshold for ranging market (default: 20)
    trending_dip_threshold : int
        Min dip signals for trending market (default: 2)
    ranging_dip_threshold : int
        Min dip signals for ranging market (default: 3)
    downtrend_dip_threshold : int
        Min dip signals for downtrend (default: 4)
    rsi_period : int
        RSI calculation period (default: 14)
    rsi_oversold : float
        RSI oversold threshold (default: 35)
    bb_period : int
        Bollinger Bands period (default: 20)
    bb_std : float
        Bollinger Bands standard deviation (default: 2)
    momentum_short : int
        Short-term momentum period (default: 10)
    momentum_long : int
        Long-term momentum period (default: 30)
    pullback_period : int
        Pullback calculation period (default: 20)
    pullback_pct : float
        Minimum pullback percentage (default: 0.025)
    
    Returns:
    --------
    bool : True if buy conditions are met, False otherwise
    
    Usage in Genesys:
    -----------------
    if buy_hold_regime(dataset):
        self.buy(size=0.95)
        self.current_equity = self.equity
    """
    try:
            
        # Get current index
        current_idx = len(dataset) - 1
        
        # Need enough data for indicators
        min_periods = max(rsi_period, bb_period, momentum_long, pullback_period, adx_period, atr_period)
        if current_idx < min_periods:
            return False  # Not enough data yet
        
        # Extract data
        close = dataset['Close'].values
        high = dataset['High'].values
        low = dataset['Low'].values
        
        # ==================== DETECT MARKET REGIME ====================
        
        regime, adx, plus_di, minus_di = detect_market_regime(
            close, high, low,
            atr_period, adx_period,
            trending_threshold, ranging_threshold
        )
        current_regime = regime[-1]
        
        # ==================== CALCULATE DIP INDICATORS ====================
        
        # 1. RSI Dip Detection
        rsi, rsi_dip = detect_rsi_dip(close, rsi_period, rsi_oversold)
        rsi_signal = rsi_dip[-1]
        
        # 2. Bollinger Bands Dip Detection
        lower_bb, upper_bb, bb_dip = detect_bollinger_dip(close, bb_period, bb_std)
        bb_signal = bb_dip[-1]
        
        # 3. Price Momentum Dip Detection
        momentum, momentum_dip = detect_price_momentum_dip(close, momentum_short, momentum_long)
        momentum_signal = momentum_dip[-1]
        
        # 4. Percentage Pullback Detection
        recent_high, pullback_dip = detect_percentage_pullback(close, pullback_period, pullback_pct)
        pullback_signal = pullback_dip[-1]
        
        # Calculate dip score
        dip_score = (
            int(rsi_signal) + 
            int(bb_signal) + 
            int(momentum_signal) + 
            int(pullback_signal)
        )
        
        # ==================== REGIME-BASED DECISION ====================
        
        should_buy = False
        position_size = 0.0
        take_profit_pct = 10.0
        stop_loss_pct = 5.0
        
        if current_regime == 1:  # TRENDING UPWARD
            if dip_score >= trending_dip_threshold:
                should_buy = True
                position_size = 0.95  # Aggressive
                take_profit_pct = 12.0
                stop_loss_pct = 4.0
        
        elif current_regime == 0:  # RANGING
            if dip_score >= ranging_dip_threshold:
                should_buy = True
                position_size = 0.60  # Conservative
                take_profit_pct = 6.0
                stop_loss_pct = 3.0
        
        elif current_regime == -1:  # TRENDING DOWNWARD
            if dip_score >= downtrend_dip_threshold:  # Only extreme dips
                should_buy = True
                position_size = 0.30  # Very small
                take_profit_pct = 6.0
                stop_loss_pct = 3.0
        
        return should_buy
    except Exception as e:
        print(f'Exception occured in buy_hold_regime system: {e}')
        return False # no trade




def is_asian_range_buy(asset):
    try:
        # Specify your local time zone
        local_timezone = pytz.timezone('Africa/Johannesburg')

        # Get the current time in the specified time zone
        current_time_local = datetime.now(local_timezone)

        # Extract the current hour
        current_hour_local = current_time_local.hour

        # Ensure that the algorithm runs after London Open in South African Time
        if current_hour_local >= 9:
            dataset = obtain_dataset(asset=asset, interval='1d', num_days=31)
            uptrend = is_uptrend(data=dataset)

            if uptrend:
                df = obtain_dataset(asset=asset, interval='1m', num_days=1)
                df.index = pd.to_datetime(df.index)
                today = datetime.now(local_timezone).strftime("%Y-%m-%d")
                df_today = df[df.index.strftime("%Y-%m-%d") == today]
                asian_range = df_today.between_time('01:00', '05:00')
                ranging_market = is_ranging_market(data=asian_range)

                if ranging_market:
                    levels = support_and_resistance(asian_range)
                    support_level = levels[0][0]
                    resistance_level = levels[1][0]
                    last_close = df.iloc[-1].Close

                    if uptrend and last_close >= resistance_level:
                        return True

            return False # no trade

        return False # no trade

    except Exception as e:
        print(f'Exception occured in asian_range_buy: {e} with asset: {asset}')
        return False # no trade


def is_asian_range_sell(asset):
    try:
        # Specify your local time zone
        local_timezone = pytz.timezone('Africa/Johannesburg')

        # Get the current time in the specified time zone
        current_time_local = datetime.now(local_timezone)

        # Extract the current hour
        current_hour_local = current_time_local.hour

        # Ensure that the algorithm runs after London Open in South African Time
        if current_hour_local >= 9:
            dataset = obtain_dataset(asset=asset, interval='1d', num_days=31)
            downtrend = is_downtrend(data=dataset)

            if downtrend:
                df = obtain_dataset(asset=asset, interval='1m', num_days=1)
                df.index = pd.to_datetime(df.index)
                today = datetime.now(local_timezone).strftime("%Y-%m-%d")
                df_today = df[df.index.strftime("%Y-%m-%d") == today]
                asian_range = df_today.between_time('01:00', '05:00')
                ranging_market = is_ranging_market(data=asian_range)

                if ranging_market:
                    levels = support_and_resistance(asian_range)
                    support_level = levels[0][0]
                    resistance_level = levels[1][0]
                    last_close = df.iloc[-1].Close
                    
                    if downtrend and last_close <= support_level:
                        return True


            return False # no trade

        return False # no trade

    except Exception as e:
        print(f'Exception occured in asian_range_sell: {e} with asset: {asset}')
        return False # no trade


def is_fibonacci_level(data, trend, level):
    # Ensure trend is either 'uptrend' or 'downtrend'
    try:
        trend = trend.lower()
        if trend not in ['uptrend', 'downtrend']:
            raise ValueError("Trend must be 'uptrend' or 'downtrend'")
        
        # Get the lowest close price and highest high price
        Low = data['Close'].min()
        High = data['High'].max()
        latest_price = data.iloc[-1]['Close']

        # Calculate the difference
        Diff = High - Low

        # Calculate Fibonacci levels based on the trend
        if trend == 'downtrend':
            Fib100 = High
            Fib618 = High - (Diff * 0.618)
            Fib50 = High - (Diff * 0.5)
            Fib382 = High - (Diff * 0.382)
            Fib236 = High - (Diff * 0.236)
            Fib0 = Low
        else:  # 'uptrend'
            Fib100 = Low
            Fib618 = Low + (Diff * 0.618)
            Fib50 = Low + (Diff * 0.5)
            Fib382 = Low + (Diff * 0.382)
            Fib236 = Low + (Diff * 0.236)
            Fib0 = High

        # Check if the latest price is below the specified Fibonacci level
        if trend == 'downtrend':
            if level == 0 and latest_price >= Fib0:
                return True
            elif level == 23.6 and latest_price >= Fib236:
                return True
            elif level == 38.2 and latest_price >= Fib382:
                return True
            elif level == 50 and latest_price >= Fib50:
                return True
            elif level == 61.8 and latest_price >= Fib618:
                return True
            elif level == 100 and latest_price >= Fib100:
                return True
        elif trend == 'uptrend':
            if level == 0 and latest_price <= Fib0:
                return True
            elif level == 23.6 and latest_price <= Fib236:
                return True
            elif level == 38.2 and latest_price <= Fib382:
                return True
            elif level == 50 and latest_price <= Fib50:
                return True
            elif level == 61.8 and latest_price <= Fib618:
                return True
            elif level == 100 and latest_price <= Fib100:
                return True
        
        return False
    except Exception as e:
        return JsonResponse({"error": f"Error occured in Fibonacci function: {e}"})


def is_ote_buy(asset):
    try:
        dataset = obtain_dataset(asset=asset, interval='1d', num_days=213)
        ranging_market = is_ranging_market(data=dataset)
        if ranging_market:
            latest_price = dataset.iloc[-1].Close
            support_level = support_and_resistance(dataset)[0][0]
            resistance_level = support_and_resistance(dataset)[1][0]
            if latest_price >= support_level and latest_price <= resistance_level:
                prev_4_days_data = obtain_dataset(asset=asset, interval='1h', num_days=4)
                uptrend = is_uptrend(data=prev_4_days_data)
                if uptrend:
                    prev_2_days_data = obtain_dataset(asset=asset, interval='1h', num_days=2)
                    if is_fibonacci_level(data=prev_2_days_data, trend='uptrend', level=50):
                        return True
    except Exception as e:
        print(f'Error occured in ote_buy_function: {e}')
        return JsonResponse({'message': f'Error occured in ote_buy_function: {e}'})


def is_ote_sell(asset):
    try:
        dataset = obtain_dataset(asset=asset, interval='1d', num_days=213)
        ranging_market = is_ranging_market(data=dataset)
        if ranging_market:
            latest_price = dataset.iloc[-1].Close
            support_level = support_and_resistance(dataset)[0][0]
            resistance_level = support_and_resistance(dataset)[1][0]
            if latest_price >= support_level and latest_price <= resistance_level:
                prev_4_days_data = obtain_dataset(asset=asset, interval='1h', num_days=4)
                downtrend = is_downtrend(data=prev_4_days_data)
                if downtrend:
                    prev_2_days_data = obtain_dataset(asset=asset, interval='1h', num_days=2)
                    if is_fibonacci_level(data=prev_2_days_data, trend='downtrend', level=50):
                        return True
    except Exception as e:
        print(f'Error occured in ote_sell_function: {e}')
        return JsonResponse({'message': f'Error occured in ote_sell_function: {e}'})


def is_bearish_orderblock(asset, tolerance=0.005, timeframe='1d'):
    try:
        dataset = obtain_dataset(asset=asset, interval=timeframe, num_days=214)
        ranging_market = is_ranging_market(data=dataset)
        
        if not ranging_market:
            return False
        
        levels = support_and_resistance(dataset)
        support_level = levels[0][0]
        resistance_level = levels[1][0]
        
        last_close = dataset.iloc[-1].Close
        
        if last_close <= support_level:
            return False
        
        # Loop through the dataset in reverse order to find the most recent candle where the resistance level is between the low and high
        for index, row in dataset[::-1].iterrows():
            if row['Low'] <= resistance_level <= row['High']:
                orderblock_open = row['High']
                orderblock_close = row['Low']
                
                # Check if the last_close is within the tolerance range of the orderblock_close
                if abs(last_close - orderblock_close) <= tolerance * orderblock_close and last_close < resistance_level:
                    return True    
        return False
    except Exception as e:
        print(f'Error occured in bearish orderblock function: {e}')
        return JsonResponse({'message': f'Error occured in bearish orderblock function: {e}'})


def is_bullish_orderblock(asset, tolerance=0.005, timeframe='1d'):
    try:
        dataset = obtain_dataset(asset=asset, interval=timeframe, num_days=214)
        ranging_market = is_ranging_market(data=dataset)
        
        if not ranging_market:
            return False
        
        levels = support_and_resistance(dataset)
        support_level = levels[0][0]
        resistance_level = levels[1][0]
        
        last_close = dataset.iloc[-1].Close
        
        if last_close >= resistance_level:
            return False
        
        # Loop through the dataset in reverse order to find the most recent candle where the resistance level is between the low and high
        for index, row in dataset[::-1].iterrows():
            if row['Low'] <= support_level <= row['High']:
                orderblock_open = row['High']
                orderblock_close = row['Low']
                
                # Check if the last_close is within the tolerance range of the orderblock_close
                if abs(last_close - orderblock_close) <= tolerance * orderblock_close and last_close > support_level:
                    return True
        return False
    except Exception as e:
        print(f'Error occured in bullish orderblock function: {e}')
        return JsonResponse({'message': f'Error occured in bullish orderblock function: {e}'})


def is_bullish_weekly_profile(asset):
    try:
        dataframe = obtain_dataset(asset=asset, interval='1wk', num_days=1)
        weekly_opening_price = dataframe.iloc[0]['Open']
        current_price = dataframe.iloc[-1]['Close']
        if current_price > weekly_opening_price:
            return True
        else:
            return False
    except Exception as e:
        print(f'Error occured in weekly bullish profile function: {e}')
        return JsonResponse({'message': f'Error occured in weekly bullish function: {e}'})


def is_bearish_weekly_profile(asset):
    try:
        dataframe = obtain_dataset(asset=asset, interval='1wk', num_days=1)
        weekly_opening_price = dataframe.iloc[0]['Open']
        current_price = dataframe.iloc[-1]['Close']
        if current_price < weekly_opening_price:
            return True
        else:
            return False
    except Exception as e:
        print(f'Error occured in weekly bearish profile function: {e}')
        return JsonResponse({'message': f'Error occured in weekly bearish function: {e}'})


@csrf_exempt
def genesys_live(request, identifier, num_positions, asset, interval, order_ticket, bot_id):
    
    return_statement = None
    percentage_test = 0

    asset = asset
    # print(f'Identifier: {identifier}\nInitial Equity: {initial_equity}\nTrade Equity: {trade_equity}\nNum Positions: {num_positions}\n')
    
    # def set_take_profit(number, type_of_setting):
    #     # current_equity = equity
    #     # print(f'Current Equity: {current_equity}\n')
    #     type_of_setting = type_of_setting.upper()
    #     number = float(number)
    #     nonlocal percentage_test, return_statement
            
    #     if type_of_setting == 'PERCENTAGE':
    #         percentage = ((current_equity - trade_equity) / initial_equity) * 100

    #         # variable = f'Identifier: {identifier}\nInitial Equity: {initial_equity}\nTrade Equity: {trade_equity}\nNum Positions: {num_positions}\nPercentage is: {percentage}\n'
    #         # percentage_test = variable
    #         if percentage >= number:
    #             return_statement = "close_position"
    #     elif type_of_setting == 'NUMBER':
    #         difference = current_equity - trade_equity
    #         if difference >= number:
    #             return_statement = "close_position"
        
    # def set_stop_loss(number, type_of_setting):
    #     nonlocal return_statement, percentage_test
    #     type_of_setting = type_of_setting.upper()
    #     number = -(float(number))
    #     # Get 'equity' here from the 'GenesysLive' model.
    #     if type_of_setting == 'PERCENTAGE':
    #         percentage = ((current_equity - trade_equity) / initial_equity) * 100
    #         percentage_test = percentage
    #         if percentage <= number:
    #             return_statement = "close_position"
    #     elif type_of_setting == 'NUMBER':
    #         difference = current_equity - trade_equity
    #         if difference <= number:
    #             return_statement = "close_position"
                    
    
    model_query = GenesysLive.objects.filter(model_id=identifier)

    if len(model_query) == 0:
        return JsonResponse({"message": f"Model has no such identifier"})
    
    # Check if there is any trade with the given model_id and order_ticket
    model_exists = uniqueBot.objects.filter(bot_id=bot_id).exists()

    # Return the appropriate response based on whether the model exists or not
    if model_exists:
        return JsonResponse({"message": "Model already has an ongoing position"})


    today = datetime.now().date()  # Get the current date
    model_traded = tradeModel.objects.filter(asset=asset, date_taken__date=today, model_id=identifier, timeframe=interval).exists()  # Filter by date only
    if model_traded:
        return JsonResponse({"message": f"Model Has Already Taken a trade for today"})


    if interval == '1d' or interval == '1wk':
        number_of_days = 213
    elif interval == '15m' or interval == '5m':
        number_of_days = 3
    else:
        number_of_days = 14

    dataset = obtain_dataset(asset=asset, interval=interval, num_days=number_of_days)
    
    model_code = model_query[0].model_code

    # test_model_id = 5505503

    # Inside genesys_live function
    try:
        # Execute model_code within a namespace dictionary
         # Initialize the namespace dictionary with functions
        namespace = {
            # 'set_take_profit': set_take_profit,
            # 'set_stop_loss': set_stop_loss,
            'interval': interval,
            'num_positions': num_positions,
            'is_support_level': is_support_level,
            'is_resistance_level': is_resistance_level,
            'dataset': dataset,
            'is_uptrend': is_uptrend,
            'is_downtrend': is_downtrend,
            'is_ranging_market': is_ranging_market,
            'is_bullish_candle': is_bullish_candle,
            'is_bearish_candle': is_bearish_candle,
            'is_bullish_engulfing': is_bullish_engulfing,
            'is_bearish_engulfing': is_bearish_engulfing,
            'is_morning_star': is_morning_star,
            'is_evening_star': is_evening_star,
            'is_three_white_soldiers': is_three_white_soldiers,
            'is_three_black_crows': is_three_black_crows,
            'is_morning_doji_star': is_morning_doji_star,
            'is_evening_doji_star': is_evening_doji_star,
            'is_rising_three_methods': is_rising_three_methods,
            'is_falling_three_methods': is_falling_three_methods,
            'is_hammer': is_hammer,
            'is_hanging_man': is_hanging_man,
            'is_inverted_hammer': is_inverted_hammer,
            'is_shooting_star': is_shooting_star,
            'is_bullish_kicker': is_bullish_kicker,
            'is_bearish_kicker': is_bearish_kicker,
            'is_bullish_harami': is_bullish_harami,
            'is_bearish_harami': is_bearish_harami,
            'is_bullish_three_line_strike': is_bullish_three_line_strike,
            'is_bearish_three_line_strike': is_bearish_three_line_strike,
            'moving_average': moving_average,
            'bbands': bbands,
            'momentum': momentum,
            'rsi': rsi,
            'is_asian_range_buy': is_asian_range_buy,
            'is_asian_range_sell': is_asian_range_sell,
            'asset': asset,
            'is_fibonacci_level': is_fibonacci_level,
            'is_ote_buy': is_ote_buy,
            'is_ote_sell': is_ote_sell,
            'is_bullish_orderblock': is_bullish_orderblock,
            'is_bearish_orderblock': is_bearish_orderblock,
            'is_bullish_weekly_profile': is_bullish_weekly_profile,
            'is_bearish_weekly_profile': is_bearish_weekly_profile,
            'buy_hold': buy_hold,
            'buy_hold_regime': buy_hold_regime,
            'is_bullish_bias': is_bullish_bias,
            'is_bearish_bias': is_bearish_bias,
            'is_high_volume': is_high_volume,
            'is_low_volume': is_low_volume,
            'is_stable_market': is_stable_market,
            'is_choppy_market': is_choppy_market,
            'is_volatile_market': is_volatile_market,
            'new_york_session': new_york_session,
            'london_session': london_session,
            'asian_session': asian_session,
            'snow_alpha_buy': snow_alpha_buy,
            'snow_alpha_short': snow_alpha_short,
            'ice_beta_buy': ice_beta_buy,
            'ice_beta_short': ice_beta_short,
            'frost_gamma_buy': frost_gamma_buy,
            'frost_gamma_short': frost_gamma_short,
            'glacier_x_buy': glacier_x_buy,
            'glacier_x_short': glacier_x_short,
            'avalanche_z_buy': avalanche_z_buy,
            'avalanche_z_short': avalanche_z_short,
            'polar_prime_buy': polar_prime_buy,
            'polar_prime_short': polar_prime_short,
            'blizzard_omega_buy': blizzard_omega_buy,
            'blizzard_omega_short': blizzard_omega_short,
            'tundra_sigma_buy': tundra_sigma_buy,
            'tundra_sigma_short': tundra_sigma_short,
            'arctic_delta_buy': arctic_delta_buy,
            'arctic_delta_short': arctic_delta_short,
            'permafrost_theta_buy': permafrost_theta_buy,
            'permafrost_theta_short': permafrost_theta_short,
        }
    
        exec(model_code, namespace)
        
        # Retrieve return_statement from the namespace
        if return_statement == None:
            return_statement = namespace.get('return_statement', None)
    except Exception as e:
        # Log and handle exceptions
        print(f"Error executing model code: {e} with interval: {interval}\n with dataset:\n {dataset} with length of {len(dataset)}")
        return JsonResponse({"error": f"{e} with interval: {interval}"})

    # Check return_statement and handle accordingly
    if return_statement:
        return JsonResponse({"message": return_statement})
    else:
        return JsonResponse({"message": f"No message to send order from backend"})


@csrf_exempt
def delete_unique_bot(request, bot_id):
    try:
        bot = uniqueBot.objects.filter(bot_id=bot_id)
        bot.delete()
        return JsonResponse({"message": f"Bot deleted Successfully!"})
    except Exception as e:
        return JsonResponse({"message": f"Error occured in Delete Bot Function: {e}"})


@csrf_exempt
def clear_stuff(request):
    uniqueBot.objects.all().delete()
    # tradeModel.objects.all().delete()
    # Trade.objects.all().delete()
    return JsonResponse({"message": f"All models deleted!"})


@csrf_exempt
def save_genesys_model(request):
    
    if request.method == 'POST':
        model_id = ''
        model_code = ''
        true_initial_equity = ''
        try:
            data = json.loads(request.body)
            model_id = int(data.get('model_id'))
            model_code = data.get('model_code')
            true_initial_equity = float(data.get('true_initial_equity'))
            
            # Save the data to your model
            new_model = GenesysLive(
                model_id=model_id,
                model_code=model_code,
                true_initial_equity=true_initial_equity
            )
            new_model.save()
            
            return JsonResponse({'message': f'Model saved successfully\n model_id: {model_id}, true_initial_equity: {true_initial_equity}'})
        except Exception as e:
            return JsonResponse({'message': f'{e}\nmodel_id: {model_id}, true_initial_equity: {true_initial_equity}'})
    else:
        return JsonResponse({'error': 'Invalid request method'}, status=405)


@csrf_exempt
def test_date(request, asset):
    today = datetime.now().date()  # Get the current date
    model_traded = tradeModel.objects.filter(asset=asset, date_taken__date=today).exists()  # Filter by date only
    if model_traded:
        return JsonResponse({"message": f"Model Has Already Taken a trade for today: {model_traded}"})
    else:
        return JsonResponse({"message": f"Model Has Taken No trade for the day: {model_traded}"})



@csrf_exempt
def save_new_trade_model(request, model_id, initial_equity, order_ticket, asset, volume, type_of_trade, timeframe, bot_id):
    try:
        model_query = GenesysLive.objects.filter(model_id=model_id)
        if len(model_query) == 0:
            return JsonResponse({"message": f"Model has no such identifier"})
        model_code = model_query[0].model_code
        today = datetime.now()  # Get the current date and time
        new_trade_model = tradeModel(model_id=model_id, model_code=model_code, initial_equity=initial_equity, order_ticket=order_ticket, type_of_trade=type_of_trade, volume=volume, asset=asset, profit=-1.0, timeframe=timeframe, date_taken=today)
        new_trade_model.save()
        unique_bot = uniqueBot(model_id=model_id, order_ticket=order_ticket, asset=asset, bot_id=bot_id)
        unique_bot.save()
        return JsonResponse({'message': f'Saved New Model Successfully!'})
    except Exception as e:
        return JsonResponse({'message': f'Error Occured in Save New Trade Model Function: {e}'})


@csrf_exempt
def update_trade_model(request, model_id, order_ticket, profit):
    
    try:
        profit = float(profit)
        model_query = tradeModel.objects.filter(model_id=model_id, order_ticket=order_ticket)
        if len(model_query) == 0:
            return JsonResponse({"message": f"Model has no such identifier"})
        # Update Profit if Match is Found
        model_query.update(profit=profit)
        return JsonResponse({'message': 'Model Updated Successfully!'})
    except Exception as e:
        return JsonResponse({'message': f'Exception Occured In Update Trade Model Function: {e}'})


@csrf_exempt
def get_model_performance(request):
    if request.method == 'GET':
        models = tradeModel.objects.values(
            'model_id', 
            'model_code', 
            'initial_equity', 
            'order_ticket', 
            'asset', 
            'profit', 
            'volume', 
            'type_of_trade', 
            'timeframe', 
            'date_taken'
        )

        # Remove duplicates by using a dictionary to track unique order_tickets
        unique_models = {}
        for model in models:
            if model['order_ticket'] not in unique_models:
                unique_models[model['order_ticket']] = model
        
        data = list(unique_models.values())
        
        return JsonResponse(data, safe=False)
    else:
        return JsonResponse({'error': 'Invalid HTTP method'}, status=405)


@csrf_exempt
def get_user_assets(request, email='butterrobot83@gmail.com'):
    try:
        # Fetch unique asset names from tradeModel
        user_assets = tradeModel.objects.values_list('asset', flat=True).distinct()
        
        # Convert QuerySet to a list
        unique_assets = list(user_assets)

        return JsonResponse({'message': unique_assets})
    except Exception as e:
        print(f'Error occurred in get_user_assets: {e}')
        return JsonResponse({'error': f'Error occurred in get_user_assets: {e}'})



# @csrf_exempt
# def fetch_asset_data(request, asset):
#     try:
#         asset = asset.upper()
#         email = 'butterrobot83@gmail.com'
#         trade_data = Trade.objects.filter(email=email, asset=asset)

#         # Initialize variables
#         profit_list = []
#         win_count = 0
#         loss_count = 0
#         overall_return = 0

#         # Iterate over trade data to calculate the required values
#         for trade in trade_data:
#             amount = trade.amount  # Assuming 'amount' is the profit/loss attribute in Trade model
#             profit_list.append(amount)
#             overall_return += amount
#             if amount >= 0:
#                 win_count += 1
#             elif amount < 0:
#                 loss_count += 1

#         total_trades = len(trade_data)
#         win_rate = (win_count / total_trades) * 100 if total_trades > 0 else 0
#         loss_rate = 100 - win_rate

#         return JsonResponse({
#             'profit_list': profit_list,
#             'win_rate': win_rate,
#             'loss_rate': loss_rate,
#             'overall_return': overall_return
#         })

#     except Exception as e:
#         print(f'Error occurred in fetch_asset_data: {e}')
#         return JsonResponse({'error': f'Error occurred in fetch_asset_data: {e}'})


@csrf_exempt
def fetch_asset_data(request, asset):
    try:
        asset = asset.upper()
        email = 'butterrobot83@gmail.com'
        trade_data = Trade.objects.filter(email=email, asset=asset)

        # Initialize variables for overall stats
        profit_list = []
        win_count = 0
        loss_count = 0
        overall_return = 0

        # Initialize dictionaries to store strategy-based metrics
        strategy_profit_list = {}
        strategy_win_count = {}
        strategy_loss_count = {}
        strategy_overall_return = {}

        # Iterate over trade data to calculate the required values
        for trade in trade_data:
            amount = trade.amount  # Assuming 'amount' is the profit/loss attribute in Trade model
            profit_list.append(amount)
            overall_return += amount

            if amount >= 0:
                win_count += 1
            else:
                loss_count += 1

            strategy = trade.strategy
            if strategy not in strategy_profit_list:
                strategy_profit_list[strategy] = []
                strategy_win_count[strategy] = 0
                strategy_loss_count[strategy] = 0
                strategy_overall_return[strategy] = 0

            strategy_profit_list[strategy].append(amount)
            strategy_overall_return[strategy] += amount

            if amount >= 0:
                strategy_win_count[strategy] += 1
            else:
                strategy_loss_count[strategy] += 1

        # Calculate overall win rate and loss rate
        total_trades = len(trade_data)
        win_rate = (win_count / total_trades) * 100 if total_trades > 0 else 0
        loss_rate = 100 - win_rate

        # Calculate win rate, loss rate, and overall return for each strategy
        strategy_metrics = {}
        for strategy in strategy_profit_list:
            total_trades_strategy = len(strategy_profit_list[strategy])
            strategy_win_rate = (strategy_win_count[strategy] / total_trades_strategy) * 100 if total_trades_strategy > 0 else 0
            strategy_loss_rate = 100 - strategy_win_rate

            strategy_metrics[strategy] = {
                'profit_list': strategy_profit_list[strategy],
                'win_rate': strategy_win_rate,
                'loss_rate': strategy_loss_rate,
                'overall_return': strategy_overall_return[strategy]
            }

        return JsonResponse({
            'overall': {
                'profit_list': profit_list,
                'win_rate': win_rate,
                'loss_rate': loss_rate,
                'overall_return': overall_return
            },
            'strategy_metrics': strategy_metrics
        })

    except Exception as e:
        print(f'Error occurred in fetch_asset_data: {e}')
        return JsonResponse({'error': f'Error occurred in fetch_asset_data: {e}'})


@csrf_exempt
def fetch_asset_data_from_models(request, asset):
    try:
        asset = asset.upper()
        model_data = tradeModel.objects.filter(asset=asset)
        
        # Get distinct model IDs for the given asset
        model_ids = model_data.values_list('model_id', flat=True).distinct()
        response_data = []

        for model_id in model_ids:
            model_trades = model_data.filter(model_id=model_id)
            profits = [trade.profit for trade in model_trades]
            equity_curve = calculate_equity_curve(profits)
            win_rate, loss_rate, overall_return = calculate_performance_metrics(profits)
            
            response_data.append({
                'model_id': model_id,
                'equity_curve': equity_curve,
                'win_rate': win_rate,
                'loss_rate': loss_rate,
                'overall_return': overall_return,
            })

        return JsonResponse({'data': response_data})

    except Exception as e:
        print(f'Error occurred in fetch_asset_data_from_models: {e}')
        return JsonResponse({'error': f'Error occurred in fetch_asset_data_from_models: {e}'})


def calculate_equity_curve(profits):
    initial_equity = 10000
    equity = initial_equity
    equity_curve = [equity]
    for profit in profits:
        equity += profit
        equity_curve.append(equity)
    return equity_curve


def calculate_performance_metrics(profits):
    total_trades = len(profits)
    winning_trades = len([profit for profit in profits if profit > 0])
    losing_trades = len([profit for profit in profits if profit < 0])
    
    win_rate = (winning_trades / total_trades) * 100 if total_trades > 0 else 0
    loss_rate = (losing_trades / total_trades) * 100 if total_trades > 0 else 0
    overall_return = sum(profits)
    
    return win_rate, loss_rate, overall_return


@csrf_exempt
def get_asset_summary(request, asset):
    try:
        asset = asset.upper()
        asset_news_summary = dailyBrief.objects.filter(asset=asset)[0].summary
        return JsonResponse({'message': f'{asset_news_summary}'})
    except Exception as e:
        print(f'Error occurred in get_asset_daily_brief_data: {e}')
        return JsonResponse({'error': f'Error occurred in get_asset_daily_brief_data: {e}'})


@csrf_exempt
def generate_cot_data(request):
    try:
        # Get requested assets from POST data if provided
        if request.method == 'POST':
            requested_assets = json.loads(request.body).get('assets', [])
        else:
            # Default assets for GET requests
            requested_assets = [
                'USD INDEX - ICE FUTURES U.S.',
                'EURO FX - CHICAGO MERCANTILE EXCHANGE',
                'BRITISH POUND - CHICAGO MERCANTILE EXCHANGE',
                'CANADIAN DOLLAR - CHICAGO MERCANTILE EXCHANGE',
                'SWISS FRANC - CHICAGO MERCANTILE EXCHANGE',
                'JAPANESE YEN - CHICAGO MERCANTILE EXCHANGE',
                'NZ DOLLAR - CHICAGO MERCANTILE EXCHANGE',
                'AUSTRALIAN DOLLAR - CHICAGO MERCANTILE EXCHANGE',
                'GOLD - COMMODITY EXCHANGE INC.',
                'UST BOND - CHICAGO BOARD OF TRADE',
                'UST 10Y NOTE - CHICAGO BOARD OF TRADE',
                'UST 5Y NOTE - CHICAGO BOARD OF TRADE',
                'NASDAQ MINI - CHICAGO MERCANTILE EXCHANGE',
                'E-MINI S&P 500 -',
                'DOW JONES U.S. REAL ESTATE IDX - CHICAGO BOARD OF TRADE'
            ]

        # Get the current year and previous year
        current_year = pd.Timestamp.now().year
        previous_year = current_year - 1

        # Create list to store DataFrames
        df_list = []

        # Fetch data for previous and current year
        for year in range(previous_year, current_year + 1):
            single_year = cot.cot_year(year, cot_report_type='legacy_futopt')
            df_list.append(single_year)

        # Concatenate all DataFrames
        df = pd.concat(df_list, ignore_index=True)

        # Convert dates to datetime
        df['As of Date in Form YYYY-MM-DD'] = pd.to_datetime(df['As of Date in Form YYYY-MM-DD'])

        # Filter for current year data
        unfiltered_currency_df = df[df['As of Date in Form YYYY-MM-DD'].dt.year == current_year]

        # Filter for requested assets
        unfiltered_currency_df = unfiltered_currency_df[
            unfiltered_currency_df['Market and Exchange Names'].isin(requested_assets)
        ]

        # Remove specific exclusions (e.g., MICRO GOLD)
        unfiltered_currency_df = unfiltered_currency_df[
            unfiltered_currency_df['Market and Exchange Names'] != 'MICRO GOLD - COMMODITY EXCHANGE INC.'
        ]

        # Fill missing values and ensure numeric columns
        numeric_columns = [
            'Noncommercial Positions-Long (All)',
            'Noncommercial Positions-Short (All)',
            'Commercial Positions-Long (All)',
            'Commercial Positions-Short (All)'
        ]
        
        unfiltered_currency_df[numeric_columns] = unfiltered_currency_df[numeric_columns].fillna(0).astype(float)

        # Calculate net positions for unfiltered data
        unfiltered_currency_df['Net Noncommercial Positions'] = (
            unfiltered_currency_df['Noncommercial Positions-Long (All)'] - 
            unfiltered_currency_df['Noncommercial Positions-Short (All)']
        )
        unfiltered_currency_df['Net Commercial Positions'] = (
            unfiltered_currency_df['Commercial Positions-Long (All)'] - 
            unfiltered_currency_df['Commercial Positions-Short (All)']
        )

        # Get the rows with maximum open interest for each market
        idx = unfiltered_currency_df.groupby('Market and Exchange Names')['Open Interest (All)'].idxmax()
        currency_df = unfiltered_currency_df.loc[idx]

        # Calculate total positions
        currency_df['Total Noncommercial Positions'] = (
            currency_df['Noncommercial Positions-Long (All)'] + 
            currency_df['Noncommercial Positions-Short (All)']
        )
        currency_df['Total Commercial Positions'] = (
            currency_df['Commercial Positions-Long (All)'] + 
            currency_df['Commercial Positions-Short (All)']
        )
        currency_df['Total Positions'] = (
            currency_df['Total Noncommercial Positions'] + 
            currency_df['Total Commercial Positions']
        )

        # Calculate percentages
        currency_df['Percentage Noncommercial Long'] = (
            currency_df['Noncommercial Positions-Long (All)'] / 
            currency_df['Total Noncommercial Positions']
        ) * 100
        currency_df['Percentage Noncommercial Short'] = (
            currency_df['Noncommercial Positions-Short (All)'] / 
            currency_df['Total Noncommercial Positions']
        ) * 100
        currency_df['Percentage Commercial Long'] = (
            currency_df['Commercial Positions-Long (All)'] / 
            currency_df['Total Commercial Positions']
        ) * 100
        currency_df['Percentage Commercial Short'] = (
            currency_df['Commercial Positions-Short (All)'] / 
            currency_df['Total Commercial Positions']
        ) * 100

        # Generate chart data instead of plots
        chart_data = generate_chart_data_for_cot(unfiltered_currency_df, requested_assets)

        # Prepare response data
        data = {}
        round_off_number = 2

        for asset in requested_assets:
            asset_df = currency_df[currency_df['Market and Exchange Names'] == asset]
            
            if not asset_df.empty:
                latest_data = asset_df.iloc[0]
                data[asset] = {
                    'Date': latest_data['As of Date in Form YYYY-MM-DD'].strftime('%Y-%m-%d'),
                    'Percentage Noncommercial Long': round(latest_data['Percentage Noncommercial Long'], round_off_number),
                    'Percentage Noncommercial Short': round(latest_data['Percentage Noncommercial Short'], round_off_number),
                    'Percentage Commercial Long': round(latest_data['Percentage Commercial Long'], round_off_number),
                    'Percentage Commercial Short': round(latest_data['Percentage Commercial Short'], round_off_number),
                    'Chart Data': chart_data.get(asset, {})
                }

        return JsonResponse(data)
    
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


def generate_chart_data_for_cot(df, requested_assets):
    """Generate chart data for frontend interactive charts"""
    chart_data = {}
    
    for asset in requested_assets:
        asset_data = df[df['Market and Exchange Names'] == asset].copy()
        
        if asset_data.empty:
            continue
            
        # Sort by date
        asset_data = asset_data.sort_values('As of Date in Form YYYY-MM-DD')
        
        # Calculate midpoint for commercial positions
        min_commercial = asset_data['Net Commercial Positions'].min()
        max_commercial = asset_data['Net Commercial Positions'].max()
        midpoint_commercial = (min_commercial + max_commercial) / 2
        
        # Prepare data for frontend
        dates = asset_data['As of Date in Form YYYY-MM-DD'].dt.strftime('%Y-%m-%d').tolist()
        net_noncommercial = asset_data['Net Noncommercial Positions'].round(2).tolist()
        net_commercial = asset_data['Net Commercial Positions'].round(2).tolist()
        open_interest = asset_data['Open Interest (All)'].round(2).tolist()
        
        chart_data[asset] = {
            'dates': dates,
            'netNoncommercial': net_noncommercial,
            'netCommercial': net_commercial,
            'openInterest': open_interest,
            'midpointCommercial': round(midpoint_commercial, 2),
            'minCommercial': round(min_commercial, 2),
            'maxCommercial': round(max_commercial, 2)
        }
    
    return chart_data

def plot_net_positions(df):
    # Get the unique currencies
    unique_currencies = df['Market and Exchange Names'].unique()
    plot_urls = {}

    # Set the plot style
    sns.set(style="whitegrid")

    for currency in unique_currencies:
        # Filter DataFrame for the current currency
        currency_data = df[df['Market and Exchange Names'] == currency]

        # Calculate min, max, and midpoint for Net Commercial Positions
        min_commercial = currency_data['Net Commercial Positions'].min()
        max_commercial = currency_data['Net Commercial Positions'].max()
        midpoint_commercial = (min_commercial + max_commercial) / 2

        # Create a figure with two subplots: one for net positions and one for Open Interest
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)

        # Plot net noncommercial and net commercial positions on the first subplot (ax1)
        ax1.plot(currency_data['As of Date in Form YYYY-MM-DD'], 
                 currency_data['Net Noncommercial Positions'], 
                 label='Net Noncommercial Positions', color='blue')
        ax1.plot(currency_data['As of Date in Form YYYY-MM-DD'], 
                 currency_data['Net Commercial Positions'], 
                 label='Net Commercial Positions', color='red')
        ax1.axhline(y=midpoint_commercial, color='green', linestyle='--', 
                    label=f'Midpoint of Net Commercial Positions ({midpoint_commercial:.2f})')

        # Customize the first subplot (Net Positions)
        ax1.set_ylabel('Net Positions')
        ax1.legend(loc='upper left')
        ax1.set_title(f'Net Positions and Open Interest for {currency}')
        ax1.grid(True)

        # Plot Open Interest on the second subplot (ax2)
        ax2.plot(currency_data['As of Date in Form YYYY-MM-DD'], 
                 currency_data['Open Interest (All)'], 
                 label='Open Interest', color='purple', linestyle='-')

        # Customize the second subplot (Open Interest)
        ax2.set_xlabel('Date')
        ax2.set_ylabel('Open Interest')
        ax2.legend(loc='upper left')
        ax2.grid(True)

        # Rotate the x-axis labels for better readability
        for label in ax2.get_xticklabels():
            label.set_rotation(45)
            label.set_horizontalalignment('right')

        # Adjust layout to prevent overlap
        plt.tight_layout()

        # Save the plot to a bytes buffer
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png')
        buffer.seek(0)
        plot_url = base64.b64encode(buffer.getvalue()).decode('utf-8')
        plot_urls[currency] = f'data:image/png;base64,{plot_url}'

        plt.close()

    return plot_urls


@csrf_exempt
def create_chill_data(request):
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            section = data.get('section')
            text = data.get('text')
            
            # Check if a Chill instance with the same section already exists
            if Chill.objects.filter(section=section).exists():
                return JsonResponse({'message': 'Chill Data with this section already exists!'}, status=400)
            
            # If not, save the new data
            chill_data = Chill(
                section=section,
                text=text
            )
            chill_data.save()
            return JsonResponse({'message': 'Chill Data Saved Successfully!'}, status=201)
        
        except Exception as e:
            print(f'Error in Chill Data Function: {e}')
            return JsonResponse({'message': f'Error in Chill Data Function: {e}'}, status=500)
    else:
        return JsonResponse({'message': 'Invalid request method.'}, status=405)


@csrf_exempt
def fetch_chill_sections(request):
    try:
        sections = Chill.objects.all().order_by('id')  # Sort by id to maintain order
        section_list = [{'section': section.section, 'text': section.text} for section in sections]
        return JsonResponse({'sections': section_list}, status=200)
    except Exception as e:
        return JsonResponse({'message': str(e)}, status=500)


@csrf_exempt
def fetch_chill_data(request):
    try:
        if request.method == 'GET':
            section = request.GET.get('section')
            if section:
                chill_data = Chill.objects.filter(section=section).first()  # Fetch the Chill data based on the section
                if chill_data:
                    return JsonResponse({'section': chill_data.section, 'text': chill_data.text})
                else:
                    return JsonResponse({'message': 'Section not found'}, status=404)
            else:
                return JsonResponse({'message': 'No section provided'}, status=400)
        else:
            return JsonResponse({'message': 'Invalid request method'}, status=405)
    except Exception as e:
        return JsonResponse({'message': f'Error fetching Chill data: {e}'})


@csrf_exempt
def edit_chill_data(request):
    try:
        if request.method == 'POST':
            body = json.loads(request.body)
            section = body.get('section')
            text = body.get('text')

            if not section or not text:
                return JsonResponse({'message': 'Invalid data'}, status=400)

            # Use update() to update the record
            updated_count = Chill.objects.filter(section=section).update(text=text)

            if updated_count == 0:
                return JsonResponse({'message': 'Section not found'}, status=404)

            return JsonResponse({'message': 'Section updated successfully'}, status=200)
        else:
            return JsonResponse({'message': 'Invalid request method'}, status=405)
    except Chill.DoesNotExist:
        return JsonResponse({'message': 'Section not found'}, status=404)
    except Exception as e:
        return JsonResponse({'message': str(e)}, status=500)

@csrf_exempt
def delete_chill_entry(request):
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            section = data.get('section')
            if section:
                Chill.objects.filter(section=section).delete()  # Deletes entry with the specified section name
                return JsonResponse({'message': 'Entry deleted successfully'}, status=200)
            return JsonResponse({'message': 'Section name not provided'}, status=400)
        except Chill.DoesNotExist:
            return JsonResponse({'message': 'Entry does not exist'}, status=404)
        except Exception as e:
            return JsonResponse({'message': str(e)}, status=500)
    return JsonResponse({'message': 'Invalid request method'}, status=405)


@csrf_exempt
def fetch_trading_images(request):
    try:
        # Define the base directory for images
        base_dir = os.path.join(os.path.dirname(__file__), 'image_folder')
        
        # Initialize an empty dictionary to hold folders and their images
        images_data = {}
        
        # Loop through each subfolder in the base directory
        for folder in os.listdir(base_dir):
            folder_path = os.path.join(base_dir, folder)
            if os.path.isdir(folder_path):
                encoded_images = []
                # Read and encode each image file in Base64
                for img_file in os.listdir(folder_path):
                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):
                        img_path = os.path.join(folder_path, img_file)
                        with open(img_path, "rb") as image_file:
                            # Encode the image and add it to the list
                            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
                            encoded_images.append({
                                "filename": img_file,
                                "data": f"data:image/{img_file.split('.')[-1]};base64,{encoded_string}"
                            })
                images_data[folder] = encoded_images
        
        return JsonResponse({"folders": images_data}, status=200)
    except Exception as e:
        return JsonResponse({"error": str(e)}, status=500)


@csrf_exempt
def alert_bot(request):
    if request.method == "GET":
        # Fetch all alerts
        alerts = AlertBot.objects.all()
        alerts_data = [
            {
                "id": alert.id,
                "asset": alert.asset,
                "price": alert.price,
                "condition": alert.condition,
                "checked": alert.checked
            }
            for alert in alerts
        ]
        return JsonResponse({"alerts": alerts_data}, status=200)

    elif request.method == "POST":
        try:
            data = json.loads(request.body)
            
            if not isinstance(data, list):
                return JsonResponse({"error": "Expected a list of alerts."}, status=400)
            
            responses = []
            for alert in data:
                asset = alert.get("asset")
                price = alert.get("price")
                condition = alert.get("condition")

                if not asset or not price or not condition:
                    responses.append({"asset": asset, "error": "Missing required fields."})
                    continue

                # Ensure uniqueness per asset
                existing_alert = AlertBot.objects.filter(asset=asset).first()

                if existing_alert:
                    existing_alert.price = price
                    existing_alert.condition = condition
                    existing_alert.checked = False
                    existing_alert.save()
                    responses.append({"asset": asset, "message": "Asset alert updated."})
                else:
                    AlertBot.objects.create(asset=asset, price=price, condition=condition, checked=False)
                    responses.append({"asset": asset, "message": "Asset alert created."})
            
            return JsonResponse({"results": responses}, status=200)
        
        except Exception as e:
            return JsonResponse({"error": str(e)}, status=500)

    elif request.method == "DELETE":
        try:
            alert_id = request.GET.get('id')
            if not alert_id:
                return JsonResponse({"error": "Alert ID is required."}, status=400)
            
            alert = AlertBot.objects.filter(id=alert_id).first()
            if not alert:
                return JsonResponse({"error": "Alert not found."}, status=404)
            
            alert.delete()
            return JsonResponse({"message": "Alert deleted successfully."}, status=200)
        
        except Exception as e:
            return JsonResponse({"error": str(e)}, status=500)
            
    else:
        return JsonResponse({"error": "Invalid request method."}, status=405)


def send_whatsapp_message(asset, message):
    """
    Sends a WhatsApp message using Twilio.
    """
    # Twilio setup
    ACCOUNT_SID = os.environ['TWILIO_SID']
    AUTH_TOKEN = os.environ['TWILIO_AUTH_TOKEN']
    TWILIO_CLIENT = Client(ACCOUNT_SID, AUTH_TOKEN)
    
    TWILIO_CLIENT.messages.create(
        body=message,
        from_='whatsapp:+14155238886',
        to='whatsapp:+27847316417'
    )
    print(f"{asset}: {message}")


def is_stock_index(asset):
    """
    Determines if an asset is a stock index based on the asset name.
    """
    stock_indices = [
        'S&P 500', 
        'NASDAQ', 
        'DOW JONES',
        'FTSE 100',
        'DAX',
        'CAC 40',
        'Nikkei 225',
        'Hang Seng'
    ]
    return asset in stock_indices


def is_individual_stock(asset):
    """
    Determines if an asset is an individual stock.
    """
    stocks = [
        'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 
        'META', 'NVDA', 'JPM', 'V', 'WMT'
    ]
    return asset in stocks


def is_crypto(asset):
    """
    Determines if an asset is a cryptocurrency.
    """
    cryptos = ['BTC-USD', 'ETH-USD', 'BNB-USD', 'XRP-USD', 
               'ADA-USD', 'DOGE-USD', 'SOL-USD', 'MATIC-USD']
    return asset in cryptos


def is_bond(asset):
    """
    Determines if an asset is a bond/treasury based on the asset name.
    """
    bonds = ['ZB1!', 'US10Y', 'US5Y']
    return asset in bonds


def is_commodity(asset):
    """
    Determines if an asset is a commodity (currently just Gold).
    """
    commodities = ['XAUUSD']
    return asset in commodities


def get_index_data(asset, timeframe, lookback_days):
    """
    Fetches historical data for a given index.

    Args:
        asset (str): The index to fetch data for.
        timeframe (str): The data interval (e.g., '1d', '1wk', '1mo').
        lookback_days (int): The number of days to look back from the current date.

    Returns:
        pandas.DataFrame: A DataFrame containing the historical data.
                          Returns None if the asset is invalid.
    """
    # Define tickers for each index
    tickers = {
        'S&P 500': '^GSPC',
        'NASDAQ': '^IXIC',
        'DOW JONES': '^DJI',
        'FTSE 100': '^FTSE',
        'DAX': '^GDAXI',
        'CAC 40': '^FCHI',
        'NIKKEI 225': '^N225',
        'HANG SENG': '^HSI'
    }

    # Get the ticker for the specified asset
    ticker = tickers.get(asset.upper())

    if ticker is None:
        print(f"Invalid asset: {asset}. Please choose from available indices.")
        return None

    # Calculate start date based on lookback period
    end_date = pd.to_datetime('today')
    start_date = end_date - pd.Timedelta(days=lookback_days)

    # Fetch data using yfinance
    try:
        data = yf.download(ticker, start=start_date, end=end_date, interval=timeframe)
        return data
    except Exception as e:
        print(f"Error fetching data for {asset}: {e}")
        return None


def get_stock_data(asset, timeframe, lookback_days):
    """
    Fetches historical data for individual stocks.

    Args:
        asset (str): The stock ticker (e.g., 'AAPL', 'MSFT').
        timeframe (str): The data interval (e.g., '1d', '1wk', '1mo').
        lookback_days (int): The number of days to look back from the current date.

    Returns:
        pandas.DataFrame: A DataFrame containing the historical data.
                          Returns None if the asset is invalid.
    """
    # Calculate start date based on lookback period
    end_date = pd.to_datetime('today')
    start_date = end_date - pd.Timedelta(days=lookback_days)

    # Fetch data using yfinance
    try:
        data = yf.download(asset, start=start_date, end=end_date, interval=timeframe)
        return data
    except Exception as e:
        print(f"Error fetching data for {asset}: {e}")
        return None


def get_crypto_data(asset, timeframe, lookback_days):
    """
    Fetches historical data for cryptocurrencies.

    Args:
        asset (str): The crypto ticker (e.g., 'BTC-USD', 'ETH-USD').
        timeframe (str): The data interval (e.g., '1d', '1wk', '1mo').
        lookback_days (int): The number of days to look back from the current date.

    Returns:
        pandas.DataFrame: A DataFrame containing the historical data.
                          Returns None if the asset is invalid.
    """
    # Calculate start date based on lookback period
    end_date = pd.to_datetime('today')
    start_date = end_date - pd.Timedelta(days=lookback_days)

    # Fetch data using yfinance
    try:
        data = yf.download(asset, start=start_date, end=end_date, interval=timeframe)
        return data
    except Exception as e:
        print(f"Error fetching data for {asset}: {e}")
        return None


def get_bond_data(asset, timeframe, lookback_days):
    """
    Fetches historical data for bonds and treasuries.

    Args:
        asset (str): The bond to fetch data for ('ZB1!', 'US10Y', or 'US5Y').
        timeframe (str): The data interval (e.g., '1d', '1wk', '1mo').
        lookback_days (int): The number of days to look back from the current date.

    Returns:
        pandas.DataFrame: A DataFrame containing the historical data.
                          Returns None if the asset is invalid.
    """
    # Define tickers for each bond
    tickers = {
        'ZB1!': 'ZB=F',       # 30-Year Treasury Bond Futures
        'US10Y': '^TNX',      # 10-Year Treasury Yield
        'US5Y': '^FVX'        # 5-Year Treasury Yield
    }

    # Get the ticker for the specified asset
    ticker = tickers.get(asset.upper())

    if ticker is None:
        print(f"Invalid asset: {asset}. Please choose from 'ZB1!', 'US10Y', or 'US5Y'.")
        return None

    # Calculate start date based on lookback period
    end_date = pd.to_datetime('today')
    start_date = end_date - pd.Timedelta(days=lookback_days)

    # Fetch data using yfinance
    try:
        data = yf.download(ticker, start=start_date, end=end_date, interval=timeframe)
        return data
    except Exception as e:
        print(f"Error fetching data for {asset}: {e}")
        return None


def get_commodity_data(asset, timeframe, lookback_days):
    """
    Fetches historical data for commodities (currently Gold).

    Args:
        asset (str): The commodity to fetch data for ('XAUUSD').
        timeframe (str): The data interval (e.g., '1d', '1wk', '1mo').
        lookback_days (int): The number of days to look back from the current date.

    Returns:
        pandas.DataFrame: A DataFrame containing the historical data.
                          Returns None if the asset is invalid.
    """
    # Define tickers for each commodity
    tickers = {
        'XAUUSD': 'GC=F'  # Gold Futures
    }

    # Get the ticker for the specified asset
    ticker = tickers.get(asset.upper())

    if ticker is None:
        print(f"Invalid asset: {asset}. Please choose from 'XAUUSD'.")
        return None

    # Calculate start date based on lookback period
    end_date = pd.to_datetime('today')
    start_date = end_date - pd.Timedelta(days=lookback_days)

    # Fetch data using yfinance
    try:
        data = yf.download(ticker, start=start_date, end=end_date, interval=timeframe)
        return data
    except Exception as e:
        print(f"Error fetching data for {asset}: {e}")
        return None


def get_latest_price(asset):
    """
    Gets the latest price for an asset, using appropriate data source.
    Returns the latest closing price as a float.
    """
    if is_stock_index(asset):
        # Use get_index_data for stock indices
        data = get_index_data(asset, timeframe="1d", lookback_days=1)
        if data is None or data.empty:
            return None
        return round(float(data["Close"].iloc[-1]), 2)
    
    elif is_individual_stock(asset):
        # Use get_stock_data for individual stocks
        data = get_stock_data(asset, timeframe="1d", lookback_days=1)
        if data is None or data.empty:
            return None
        return round(float(data["Close"].iloc[-1]), 2)
    
    elif is_crypto(asset):
        # Use get_crypto_data for cryptocurrencies
        data = get_crypto_data(asset, timeframe="1d", lookback_days=1)
        if data is None or data.empty:
            return None
        return round(float(data["Close"].iloc[-1]), 2)
    
    elif is_bond(asset):
        # Use get_bond_data for bonds/treasuries
        data = get_bond_data(asset, timeframe="1d", lookback_days=1)
        if data is None or data.empty:
            return None
        return round(float(data["Close"].iloc[-1]), 3)
    
    elif is_commodity(asset):
        # Use get_commodity_data for commodities (Gold)
        data = get_commodity_data(asset, timeframe="1d", lookback_days=1)
        if data is None or data.empty:
            return None
        return round(float(data["Close"].iloc[-1]), 2)
    
    else:
        # Use existing obtain_dataset function for Forex
        data = obtain_dataset(asset, interval="1m", num_days=1)
        if data.empty:
            return None
        return round(float(data["Close"].iloc[-1]), 5)


def manage_alerts():
    """
    Checks all alerts in the AlertBot model and sends notifications if conditions are met.
    Designed to be called periodically by an external scheduler.
    """
    alerts = AlertBot.objects.filter(checked=False)  # Fetch unchecked alerts
    
    for alert in alerts:
        try:
            asset = alert.asset
            target_price = alert.price
            condition = alert.condition  # e.g., ">" or "<"

            # Get the latest price using appropriate data source
            latest_price = get_latest_price(asset)

            if latest_price is None:
                print(f"No data available for {asset}. Skipping...")
                continue

            # Check the condition (compare scalar values only)
            condition_met = (
                (condition == ">" and latest_price > float(target_price)) or
                (condition == "<" and latest_price < float(target_price))
            )

            if condition_met:
                # Send notification
                message = (
                    f"Alert triggered for {asset}! Current price: {latest_price} "
                    f"{condition} {target_price}"
                )
                send_whatsapp_message(asset, message)

                # Mark the alert as checked
                alert.checked = True
                alert.save()

        except Exception as e:
            print(f"Error processing alert for {alert.asset}: {e}")


# Schedule the alert_bot function to run every 5 minutes
scheduler.add_job(
    manage_alerts,
    trigger=IntervalTrigger(minutes=5),
    id='manage_alerts_job',
    name='Check alerts every 5 minutes',
    replace_existing=True
)


@csrf_exempt
def create_finetuning_data(request):
    try:
        # Query all CHILL entries
        chill_data = Chill.objects.all()

        # Prepare dataset for JSONL format
        data_list = []
        for entry in chill_data:
            data_list.append({
                "messages": [
                    {"role": "system", "content": "TraderGPT is a trading assistant that provides advanced market analysis and trading strategies."},
                    {"role": "user", "content": entry.section},
                    {"role": "assistant", "content": entry.text}
                ]
            })

        # Define file path
        file_path = 'chill_data.jsonl'

        # Save as JSONL file
        with open(file_path, 'w') as jsonl_file:
            for item in data_list:
                jsonl_file.write(json.dumps(item) + '\n')

        # Serve the file as a download
        with open(file_path, 'rb') as jsonl_file:
            response = HttpResponse(jsonl_file.read(), content_type='application/jsonl')
            response['Content-Disposition'] = f'attachment; filename="{os.path.basename(file_path)}"'
            return response
    except Exception as e:
        return JsonResponse({'message': f"Error occurred: \n{e}"})

@csrf_exempt
def create_image_finetuning_data(request):
    try:
        # Define the base directory for images
        base_dir = os.path.join(os.path.dirname(__file__), 'image_folder')

        # Prepare dataset for JSONL format
        data_list = []
        for root, _, files in os.walk(base_dir):
            for img_file in files:
                if img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):
                    img_path = os.path.join(root, img_file)
                    with open(img_path, "rb") as image_file:
                        # Encode image in Base64
                        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
                        
                        # Create a single message for the image with proper formatting
                        image_content = {
                            "messages": [
                                {
                                    "role": "system",
                                    "content": "TraderGPT is a trading assistant that provides advanced market analysis and trading strategies."
                                },
                                {
                                    "role": "user",
                                    "content": [
                                        {
                                            "type": "text",
                                            "text": "What do you see in this image?"
                                        },
                                        {
                                            "type": "image_url",
                                            "image_url": {
                                                "url": f"data:image/{img_file.split('.')[-1]};base64,{encoded_string}"
                                            }
                                        }
                                    ]
                                },
                                {
                                    "role": "assistant",
                                    "content": "This is a trading chart. I can help analyze it."
                                }
                            ]
                        }
                        data_list.append(image_content)

        # Define file path
        file_path = 'image_data.jsonl'

        # Save as JSONL file
        with open(file_path, 'w') as jsonl_file:
            for item in data_list:
                jsonl_file.write(json.dumps(item) + '\n')

        # Serve the file as a download
        with open(file_path, 'rb') as jsonl_file:
            response = HttpResponse(jsonl_file.read(), content_type='application/jsonl')
            response['Content-Disposition'] = f'attachment; filename="{os.path.basename(file_path)}"'
            return response

    except Exception as e:
        return JsonResponse({'message': f"Error occurred: {str(e)}"})


# @csrf_exempt
# def create_combined_finetuning_data(request):
#     try:
#         # Prepare combined dataset
#         data_list = []

#         # Step 1: Add CHILL data
#         chill_data = Chill.objects.all()
#         for entry in chill_data:
#             data_list.append({
#                 "messages": [
#                     {
#                         "role": "system",
#                         "content": "TraderGPT is a trading assistant that provides advanced market analysis and trading strategies."
#                     },
#                     {
#                         "role": "user",
#                         "content": entry.section
#                     },
#                     {
#                         "role": "assistant",
#                         "content": entry.text
#                     }
#                 ]
#             })

#         # Step 2: Add Image data
#         base_dir = os.path.join(os.path.dirname(__file__), 'image_folder')
#         for root, _, files in os.walk(base_dir):
#             for img_file in files:
#                 if img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):
#                     img_path = os.path.join(root, img_file)
#                     with open(img_path, "rb") as image_file:
#                         # Encode image in Base64
#                         encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
                        
#                         # Create properly formatted image entry
#                         image_content = {
#                             "messages": [
#                                 {
#                                     "role": "system",
#                                     "content": "TraderGPT is a trading assistant that provides advanced market analysis and trading strategies."
#                                 },
#                                 {
#                                     "role": "user",
#                                     "content": [
#                                         {
#                                             "type": "text",
#                                             "text": "What do you see in this image?"
#                                         },
#                                         {
#                                             "type": "image_url",
#                                             "image_url": {
#                                                 "url": f"data:image/{img_file.split('.')[-1]};base64,{encoded_string}"
#                                             }
#                                         }
#                                     ]
#                                 },
#                                 {
#                                     "role": "assistant",
#                                     "content": "This is a trading chart. I can help analyze it."
#                                 }
#                             ]
#                         }
#                         data_list.append(image_content)

#         # Step 3: Save combined data as JSONL
#         file_path = 'combined_finetuning_data.jsonl'
#         with open(file_path, 'w') as jsonl_file:
#             for item in data_list:
#                 jsonl_file.write(json.dumps(item) + '\n')

#         # Step 4: Serve the file as a download
#         with open(file_path, 'rb') as jsonl_file:
#             response = HttpResponse(jsonl_file.read(), content_type='application/jsonl')
#             response['Content-Disposition'] = f'attachment; filename="{os.path.basename(file_path)}"'
#             return response

#     except Exception as e:
#         return JsonResponse({'message': f"Error occurred: {str(e)}"})
        

@csrf_exempt
def create_combined_finetuning_data(request):
    try:
        # Settings for chunking
        MAX_ENTRIES_PER_FILE = 1000  # Adjust this number based on your needs
        
        # Prepare combined dataset
        all_data = []
        
        # Step 1: Add CHILL data
        chill_data = Chill.objects.all()
        for entry in chill_data:
            all_data.append({
                "messages": [
                    {
                        "role": "system",
                        "content": "TraderGPT is a trading assistant that provides advanced market analysis and trading strategies."
                    },
                    {
                        "role": "user",
                        "content": entry.section
                    },
                    {
                        "role": "assistant",
                        "content": entry.text
                    }
                ]
            })

        # Step 2: Add Image data
        base_dir = os.path.join(os.path.dirname(__file__), 'image_folder')
        for root, _, files in os.walk(base_dir):
            for img_file in files:
                if img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):
                    img_path = os.path.join(root, img_file)
                    with open(img_path, "rb") as image_file:
                        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
                        image_content = {
                            "messages": [
                                {
                                    "role": "system",
                                    "content": "TraderGPT is a trading assistant that provides advanced market analysis and trading strategies."
                                },
                                {
                                    "role": "user",
                                    "content": [
                                        {
                                            "type": "text",
                                            "text": "What do you see in this image?"
                                        },
                                        {
                                            "type": "image_url",
                                            "image_url": {
                                                "url": f"data:image/{img_file.split('.')[-1]};base64,{encoded_string}"
                                            }
                                        }
                                    ]
                                },
                                {
                                    "role": "assistant",
                                    "content": "This is a trading chart. I can help analyze it."
                                }
                            ]
                        }
                        all_data.append(image_content)

        # Step 3: Create chunks and save as separate JSONL files
        chunk_files = []
        for i in range(0, len(all_data), MAX_ENTRIES_PER_FILE):
            chunk = all_data[i:i + MAX_ENTRIES_PER_FILE]
            file_path = f'finetuning_data_part_{i//MAX_ENTRIES_PER_FILE + 1}.jsonl'
            chunk_files.append(file_path)
            
            with open(file_path, 'w') as jsonl_file:
                for item in chunk:
                    jsonl_file.write(json.dumps(item) + '\n')

        # Step 4: Create ZIP file containing all chunks
        zip_file_path = 'chunked_finetuning_data.zip'
        with zipfile.ZipFile(zip_file_path, 'w') as zipf:
            for file_path in chunk_files:
                zipf.write(file_path)
                
        # Step 5: Serve the ZIP file
        with open(zip_file_path, 'rb') as zip_file:
            response = HttpResponse(zip_file.read(), content_type='application/zip')
            response['Content-Disposition'] = f'attachment; filename="{os.path.basename(zip_file_path)}"'

        # Clean up temporary files
        for file_path in chunk_files:
            os.remove(file_path)
        os.remove(zip_file_path)

        return response

    except Exception as e:
        return JsonResponse({'message': f"Error occurred: {str(e)}"})


# Fetch all accounts
@csrf_exempt
def get_accounts(request):
    accounts = Account.objects.all()
    accounts_data = [
        {
            "id": account.id,
            "name": account.account_name,
            "initial_capital": account.initial_capital,
            "main_assets": account.main_assets,  # Include main_assets in the response
        }
        for account in accounts
    ]
    return JsonResponse(accounts_data, safe=False)


@csrf_exempt
def create_account(request):
    if request.method == "POST":
        data = json.loads(request.body)
        account_name = data.get("name")
        initial_capital = data.get("initial_capital")
        main_assets = data.get("main_assets")  # Get main_assets from request

        if account_name and initial_capital is not None and main_assets:
            account = Account.objects.create(account_name=account_name, initial_capital=initial_capital, main_assets=main_assets)
            return JsonResponse({"message": "Account created successfully!", "id": account.id}, status=201)
        
        return JsonResponse({"error": "Invalid data"}, status=400)


# Delete an account
@csrf_exempt
def delete_account(request, account_id):
    if request.method == "DELETE":
        account = get_object_or_404(Account, id=account_id)
        account.delete()
        return JsonResponse({"message": "Account deleted successfully!"}, status=200)
    return JsonResponse({"error": "Invalid request method"}, status=405)


@csrf_exempt  # You may want to add CSRF handling or use Django's built-in token authentication
def update_account(request):
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            account_id = data.get('id')
            name = data.get('name')
            initial_capital = data.get('initial_capital')
            main_assets = data.get('main_assets')  # Get the updated main_assets

            # Find the account by ID
            account = Account.objects.get(id=account_id)
            account.account_name = name  # Update account name
            account.initial_capital = initial_capital  # Update initial capital

            # Update main_assets if it exists
            if main_assets:
                account.main_assets = main_assets  # Save the updated main assets

            account.save()

            return JsonResponse({
                'id': account.id,
                'name': account.account_name,
                'initial_capital': account.initial_capital,
                'main_assets': account.main_assets,  # Return updated main assets
            })

        except Account.DoesNotExist:
            return JsonResponse({'error': 'Account not found'}, status=404)

        except Exception as e:
            return JsonResponse({'error': str(e)}, status=500)

    return JsonResponse({'error': 'Invalid request method'}, status=400)


@csrf_exempt
def get_trading_analytics(request):
    account_name = request.GET.get('account_name')  # Get account_name from the query params
    print(f'Account name is: {account_name}')
    error_count = 0
    
    try:
        # Fetch the account data
        error_count += 1
        account = Account.objects.get(account_name=account_name)
        error_count += 1
        # Fetch related trades for this account
        trades = AccountTrades.objects.filter(account=account)
        error_count += 1

        
        # Prepare the data for response
        analytics_data = {
            'account_name': account.account_name,
            'main_assets': account.main_assets,
            'initial_capital': account.initial_capital,
            'trades': [{
                'asset': trade.asset,
                'order_type': trade.order_type,
                'amount': trade.amount,
                'outcome': trade.outcome,
                'strategy': trade.strategy,
                'day_of_week_entered': trade.day_of_week_entered,
                'day_of_week_closed': trade.day_of_week_closed,
                'trading_session_entered': trade.trading_session_entered,
                'trading_session_closed': trade.trading_session_closed,
            } for trade in trades]
        }
        print('Test 4')
        error_count += 1

        
        return JsonResponse(analytics_data, safe=False)
    
    except Account.DoesNotExist:
        return JsonResponse({'error': f'Account not found with account name: {account_name} with error count: {error_count}'}, status=404)


@csrf_exempt
def create_new_trade_data(request):
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            account_name = data.get('account_name')
            asset = data.get('asset')
            order_type = data.get('order_type')
            strategy = data.get('strategy')
            day_of_week_entered = data.get('day_of_week_entered')
            trading_session_entered = data.get('trading_session_entered')
            outcome = data.get('outcome')
            amount = data.get('amount')
            emotional_bias = data.get('emotional_bias', '')
            reflection = data.get('reflection', '')
            
            # Retrieve the account based on the account_name
            account = Account.objects.get(account_name=account_name)

            # Create the trade entry with the current timestamp
            trade = AccountTrades.objects.create(
                account=account,
                asset=asset,
                order_type=order_type,
                strategy=strategy,
                day_of_week_entered=day_of_week_entered,
                trading_session_entered=trading_session_entered,
                amount=amount,
                emotional_bias=emotional_bias,
                reflection=reflection,
                outcome=outcome,
                date_entered=now(),  # Save the current timestamp
            )

            return JsonResponse({'message': 'Trade recorded successfully!'}, status=200)

        except Account.DoesNotExist:
            return JsonResponse({'error': 'Account not found.'}, status=404)
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=400)

    return JsonResponse({'error': 'Invalid request method.'}, status=405)


@csrf_exempt
def fetch_trading_data(request):
    if request.method == 'GET':
        try:
            # Fetching all trade data (you can filter based on the request if needed)
            trades = AccountTrades.objects.all().values('account__account_name', 'asset', 'order_type', 'strategy', 'day_of_week_entered', 'day_of_week_closed', 'trading_session_entered', 'trading_session_closed', 'outcome', 'amount', 'emotional_bias', 'reflection')
            # Return the data as JSON
            return JsonResponse(list(trades), safe=False)
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=400)


@csrf_exempt
def fetch_account_data(request):
    if request.method == 'GET':
        try:
            # Get the account_name from the request
            account_name = request.GET.get('account_name')
            if account_name:
                # Fetch the account data based on account_name
                account = Account.objects.filter(account_name=account_name).first()
                if account:
                    # Serialize the account data
                    account_data = {
                        'account_name': account.account_name,
                        'main_assets': account.main_assets,
                        'initial_capital': account.initial_capital,
                        'trades': list(
                            account.trades.values(
                                'asset',
                                'order_type',
                                'strategy',
                                'day_of_week_entered',
                                'day_of_week_closed',
                                'trading_session_entered',
                                'trading_session_closed',
                                'outcome',
                                'amount',
                                'emotional_bias',
                                'reflection',
                                'date_entered',  # Add this field
                            )
                        )
                    }
                    return JsonResponse(account_data, safe=False)
                else:
                    return JsonResponse({'error': 'Account not found'}, status=404)
            else:
                return JsonResponse({'error': 'Account name is required'}, status=400)
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
def time_trading_analytics(request):
    if request.method == 'GET':
        account_name = request.GET.get('account_name')
        time_frame = request.GET.get('time_frame', 'month')  # month, week, day
        start_date = request.GET.get('start_date')
        
        # Get base queryset
        trades = AccountTrades.objects.filter(
            account__account_name=account_name,
            date_entered__gte=start_date
        )
        
        # Calculate basic metrics
        total_trades = trades.count()
        winning_trades = trades.filter(outcome='Profit').count()
        win_rate = (winning_trades / total_trades * 100) if total_trades > 0 else 0
        
        total_profit = trades.filter(outcome='Profit').aggregate(Sum('amount'))['amount__sum'] or 0
        total_loss = abs(trades.filter(outcome='Loss').aggregate(Sum('amount'))['amount__sum'] or 0)
        
        profit_factor = total_profit / total_loss if total_loss > 0 else float('inf')
        
        # Get performance by different dimensions
        performance_by_day = list(trades.values('day_of_week_entered').annotate(
            total=Sum('amount'),
            count=Count('id'),
            win_rate=Count(Case(When(outcome='Profit', then=1))) * 100.0 / Count('id')
        ))
        
        performance_by_session = list(trades.values('trading_session_entered').annotate(
            total=Sum('amount'),
            count=Count('id'),
            win_rate=Count(Case(When(outcome='Profit', then=1))) * 100.0 / Count('id')
        ))
        
        performance_by_asset = list(trades.values('asset').annotate(
            total=Sum('amount'),
            count=Count('id'),
            win_rate=Count(Case(When(outcome='Profit', then=1))) * 100.0 / Count('id')
        ))
        
        performance_by_strategy = list(trades.values('strategy').annotate(
            total=Sum('amount'),
            count=Count('id'),
            win_rate=Count(Case(When(outcome='Profit', then=1))) * 100.0 / Count('id')
        ))
        
        # Time series data based on timeframe
        if time_frame == 'month':
            time_series = trades.annotate(
                period=ExtractMonth('date_entered')
            )
        elif time_frame == 'week':
            time_series = trades.annotate(
                period=ExtractWeek('date_entered')
            )
        else:  # day
            time_series = trades.annotate(
                period=F('date_entered__date')
            )
        
        time_series = list(time_series.values('period').annotate(
            total=Sum('amount'),
            count=Count('id'),
            win_rate=Count(Case(When(outcome='Profit', then=1))) * 100.0 / Count('id')
        ).order_by('period'))
        
        response_data = {
            'summary': {
                'total_trades': total_trades,
                'win_rate': win_rate,
                'profit_factor': profit_factor,
                'total_profit': total_profit,
                'average_win': trades.filter(outcome='Profit').aggregate(Avg('amount'))['amount__avg'],
                'average_loss': trades.filter(outcome='Loss').aggregate(Avg('amount'))['amount__avg'],
            },
            'by_day': performance_by_day,
            'by_session': performance_by_session,
            'by_asset': performance_by_asset,
            'by_strategy': performance_by_strategy,
            'time_series': time_series,
        }
        
        return JsonResponse(response_data)
    
    return JsonResponse({'error': 'Method not allowed'}, status=405)


@csrf_exempt
def simulate_trading_performance(request):
    """
    Simulates future trading performance based on historical data.
    Uses Monte Carlo simulation with historical win rate, profit/loss distributions.
    """
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            account_name = data.get('account_name')
            simulation_period = data.get('simulation_period', 'month')  # week, 2weeks, 3weeks, month, 3months, 6months, 9months, year
            num_simulations = data.get('num_simulations', 1000)  # Number of Monte Carlo runs
            
            # Fetch account and trades
            account = Account.objects.get(account_name=account_name)
            trades = AccountTrades.objects.filter(account=account)
            
            if trades.count() < 10:
                return JsonResponse({
                    'error': 'Insufficient trade history. At least 10 trades required for simulation.'
                }, status=400)
            
            # Calculate historical statistics
            total_trades = trades.count()
            winning_trades = trades.filter(outcome='Win')
            losing_trades = trades.filter(outcome='Loss')
            breakeven_trades = trades.filter(outcome='Break-even')
            
            win_count = winning_trades.count()
            loss_count = losing_trades.count()
            breakeven_count = breakeven_trades.count()
            
            # Win rate and outcome probabilities
            win_rate = win_count / total_trades if total_trades > 0 else 0
            loss_rate = loss_count / total_trades if total_trades > 0 else 0
            breakeven_rate = breakeven_count / total_trades if total_trades > 0 else 0
            
            # Win/Loss amounts
            win_amounts = list(winning_trades.values_list('amount', flat=True))
            loss_amounts = list(losing_trades.values_list('amount', flat=True))
            
            avg_win = sum(win_amounts) / len(win_amounts) if win_amounts else 0
            avg_loss = sum(loss_amounts) / len(loss_amounts) if loss_amounts else 0
            
            # Standard deviations for realistic variation
            import statistics
            std_win = statistics.stdev(win_amounts) if len(win_amounts) > 1 else avg_win * 0.3
            std_loss = statistics.stdev(loss_amounts) if len(loss_amounts) > 1 else avg_loss * 0.3
            
            # Calculate average trades per week from historical data
            if trades.filter(date_entered__isnull=False).exists():
                earliest_trade = trades.filter(date_entered__isnull=False).earliest('date_entered').date_entered
                latest_trade = trades.filter(date_entered__isnull=False).latest('date_entered').date_entered
                weeks_of_trading = max(1, (latest_trade - earliest_trade).days / 7)
                avg_trades_per_week = total_trades / weeks_of_trading
            else:
                # Default assumption if no dates available
                avg_trades_per_week = 10
            
            # Determine number of weeks to simulate
            period_weeks = {
                'week': 1,
                '2weeks': 2,
                '3weeks': 3,
                'month': 4,
                '3months': 13,
                '6months': 26,
                '9months': 39,
                'year': 52
            }
            weeks = period_weeks.get(simulation_period, 4)
            
            # Expected number of trades in simulation period
            expected_trades = int(avg_trades_per_week * weeks)
            
            # Run Monte Carlo simulations
            import random
            simulation_results = []
            final_balances = []
            max_drawdowns = []
            recovery_times = []
            
            for sim in range(num_simulations):
                balance = account.initial_capital
                peak_balance = balance
                current_drawdown = 0
                max_drawdown = 0
                in_drawdown = False
                drawdown_start_trade = 0
                trades_in_drawdown = 0
                
                equity_curve = [balance]
                
                for trade_num in range(expected_trades):
                    # Randomly determine outcome based on historical probabilities
                    outcome_roll = random.random()
                    
                    if outcome_roll < win_rate:
                        # Win
                        trade_amount = random.gauss(avg_win, std_win)
                        trade_amount = max(0, trade_amount)  # Ensure non-negative
                        balance += trade_amount
                    elif outcome_roll < win_rate + loss_rate:
                        # Loss
                        trade_amount = random.gauss(avg_loss, std_loss)
                        trade_amount = max(0, trade_amount)  # Ensure non-negative
                        balance -= trade_amount
                    # else: breakeven (no change)
                    
                    equity_curve.append(balance)
                    
                    # Track drawdown
                    if balance > peak_balance:
                        peak_balance = balance
                        if in_drawdown:
                            # Recovery complete
                            recovery_times.append(trades_in_drawdown)
                            in_drawdown = False
                    
                    if balance < peak_balance:
                        current_drawdown = ((peak_balance - balance) / peak_balance) * 100
                        max_drawdown = max(max_drawdown, current_drawdown)
                        if not in_drawdown:
                            in_drawdown = True
                            drawdown_start_trade = trade_num
                            trades_in_drawdown = 0
                        trades_in_drawdown += 1
                
                final_balances.append(balance)
                max_drawdowns.append(max_drawdown)
                simulation_results.append({
                    'final_balance': balance,
                    'equity_curve': equity_curve,
                    'max_drawdown': max_drawdown,
                    'return_pct': ((balance - account.initial_capital) / account.initial_capital) * 100
                })
            
            # Statistical analysis of simulations
            final_balances.sort()
            max_drawdowns.sort()
            
            # Percentile calculations
            def percentile(data, p):
                index = int(len(data) * p)
                return data[min(index, len(data) - 1)]
            
            best_case = percentile(final_balances, 0.95)  # 95th percentile
            expected_case = percentile(final_balances, 0.50)  # Median
            worst_case = percentile(final_balances, 0.05)  # 5th percentile
            
            worst_drawdown = percentile(max_drawdowns, 0.95)
            expected_drawdown = percentile(max_drawdowns, 0.50)
            
            # Average recovery time
            avg_recovery_trades = sum(recovery_times) / len(recovery_times) if recovery_times else 0
            avg_recovery_weeks = avg_recovery_trades / avg_trades_per_week if avg_trades_per_week > 0 else 0
            
            # Probability of profit
            profitable_simulations = sum(1 for b in final_balances if b > account.initial_capital)
            probability_of_profit = (profitable_simulations / num_simulations) * 100
            
            # Expected value
            expected_balance = sum(final_balances) / len(final_balances)
            expected_return_pct = ((expected_balance - account.initial_capital) / account.initial_capital) * 100
            
            # Risk of ruin (losing 50% of capital)
            ruin_threshold = account.initial_capital * 0.5
            simulations_ruined = sum(1 for b in final_balances if b < ruin_threshold)
            risk_of_ruin = (simulations_ruined / num_simulations) * 100
            
            response_data = {
                'simulation_period': simulation_period,
                'weeks': weeks,
                'expected_trades': expected_trades,
                'initial_capital': account.initial_capital,
                'historical_stats': {
                    'total_trades': total_trades,
                    'win_rate': round(win_rate * 100, 2),
                    'loss_rate': round(loss_rate * 100, 2),
                    'breakeven_rate': round(breakeven_rate * 100, 2),
                    'avg_win': round(avg_win, 2),
                    'avg_loss': round(avg_loss, 2),
                    'avg_trades_per_week': round(avg_trades_per_week, 2)
                },
                'projections': {
                    'best_case': {
                        'balance': round(best_case, 2),
                        'return_pct': round(((best_case - account.initial_capital) / account.initial_capital) * 100, 2),
                        'profit': round(best_case - account.initial_capital, 2)
                    },
                    'expected_case': {
                        'balance': round(expected_case, 2),
                        'return_pct': round(((expected_case - account.initial_capital) / account.initial_capital) * 100, 2),
                        'profit': round(expected_case - account.initial_capital, 2)
                    },
                    'worst_case': {
                        'balance': round(worst_case, 2),
                        'return_pct': round(((worst_case - account.initial_capital) / account.initial_capital) * 100, 2),
                        'profit': round(worst_case - account.initial_capital, 2)
                    }
                },
                'risk_metrics': {
                    'worst_drawdown_pct': round(worst_drawdown, 2),
                    'expected_drawdown_pct': round(expected_drawdown, 2),
                    'avg_recovery_weeks': round(avg_recovery_weeks, 2),
                    'probability_of_profit': round(probability_of_profit, 2),
                    'risk_of_ruin': round(risk_of_ruin, 2)
                },
                'expected_value': {
                    'balance': round(expected_balance, 2),
                    'return_pct': round(expected_return_pct, 2)
                },
                # Sample equity curves for visualization (10 random simulations)
                'sample_equity_curves': [
                    sim['equity_curve'] for sim in random.sample(simulation_results, min(10, len(simulation_results)))
                ]
            }
            
            return JsonResponse(response_data)
            
        except Account.DoesNotExist:
            return JsonResponse({'error': 'Account not found'}, status=404)
        except Exception as e:
            import traceback
            return JsonResponse({
                'error': str(e),
                'traceback': traceback.format_exc()
            }, status=500)
    
    return JsonResponse({'error': 'Method not allowed'}, status=405)
    

def obtain_dataset(asset, interval, num_days):
    # Calculate the end and start dates
    import datetime
    end_date = (datetime.datetime.now() + datetime.timedelta(days=1)).strftime("%Y-%m-%d")
    start_date = (datetime.datetime.now() - datetime.timedelta(days=num_days)).strftime("%Y-%m-%d")

    # Download data using yfinance
    forex_asset = f"{asset}=X"
    data = yf.download(forex_asset, start=start_date, end=end_date, interval=interval)
    return data

import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import pandas as pd


def generate_candlestick_chart(data, save_path="candlestick_chart.png"):
    try:
        # Ensure the data has required columns and clean up
        data = data[['Open', 'High', 'Low', 'Close']]

        fig, ax = plt.subplots(figsize=(10, 6))

        for idx, row in enumerate(data.itertuples(index=False)):
            # Access the values by position
            open_price = row[0]
            high_price = row[1]
            low_price = row[2]
            close_price = row[3]

            # Determine the color of the candlestick
            color = 'green' if close_price > open_price else 'red'

            # Draw the candlestick body (rectangle)
            body = Rectangle(
                (idx - 0.4, min(open_price, close_price)),  # Bottom-left corner
                0.8,  # Width
                abs(close_price - open_price),  # Height
                color=color
            )
            ax.add_patch(body)

            # Draw the wick (high-low line)
            ax.plot(
                [idx, idx],  # X-coordinates
                [low_price, high_price],  # Y-coordinates
                color=color
            )

        # Set labels and title
        ax.set_title("Candlestick Chart", fontsize=16)
        ax.set_xlabel("Date", fontsize=12)
        ax.set_ylabel("Price", fontsize=12)

        # Remove x-axis labels
        ax.set_xticks([])

        plt.tight_layout()

        # Save the chart
        plt.savefig(save_path)
        plt.close(fig)  # Close the figure to free up memory

        print(f"Chart saved at: {save_path}")
        return save_path

    except Exception as e:
        print(f"Error generating candlestick chart: {e}")
        return None



def analyse_image_from_file(image_path, news_data):
    try:
        # Read the image and encode in base64
        with open(image_path, "rb") as image_file:
            image_data = image_file.read()
        return analyse_image(image_data, news_data)
    except Exception as e:
        print(f"Error in image analysis from file: {e}")





@dataclass
class TraderMessage:
    trader_id: str
    content: str
    message_type: str = "discussion"  # Can be "discussion" or "consensus"
    responding_to: Optional[str] = None

class ChartAnnotator:
    def __init__(self, data: pd.DataFrame, fig_size: Tuple[int, int] = (12, 8)):
        self.data = data
        self.fig_size = fig_size

    def extract_price_levels(self, consensus_text: str) -> Dict[str, float]:
        """Extract price levels from consensus text using regex."""
        price_patterns = {
            'support': r'support.*?(\d+\.?\d*)',
            'resistance': r'resistance.*?(\d+\.?\d*)',
            'entry': r'entry.*?(\d+\.?\d*)',
            'stop_loss': r'stop[- ]?loss.*?(\d+\.?\d*)',
            'target': r'target.*?(\d+\.?\d*)'
        }

        levels = {}
        for level_type, pattern in price_patterns.items():
            matches = re.findall(pattern, consensus_text.lower())
            if matches:
                levels[level_type] = float(matches[0])

        return levels

    def draw_annotated_chart(self, consensus_text: str, save_path: str = "annotated_chart.png"):
        """Create an annotated candlestick chart based on consensus analysis."""
        # Create figure and axis
        fig, ax = plt.subplots(figsize=self.fig_size)

        # Extract price levels from consensus
        levels = self.extract_price_levels(consensus_text)

        # Plot candlesticks
        for idx, row in enumerate(self.data.itertuples(index=False)):
            open_price, high_price, low_price, close_price = row[0], row[1], row[2], row[3]

            # Candlestick body
            color = 'green' if close_price > open_price else 'red'
            body = Rectangle(
                (idx - 0.4, min(open_price, close_price)),
                0.8,
                abs(close_price - open_price),
                color=color
            )
            ax.add_patch(body)

            # Wick
            ax.plot([idx, idx], [low_price, high_price], color=color)

        # Add horizontal lines for support and resistance
        max_price = self.data['High'].max()
        min_price = self.data['Low'].min()
        price_range = max_price - min_price
        x_range = len(self.data)

        # Plot levels with different styles
        level_styles = {
            'support': {'color': 'green', 'linestyle': '--', 'alpha': 0.6, 'label': 'Support'},
            'resistance': {'color': 'red', 'linestyle': '--', 'alpha': 0.6, 'label': 'Resistance'},
            'entry': {'color': 'blue', 'linestyle': '-', 'alpha': 0.8, 'label': 'Entry'},
            'stop_loss': {'color': 'red', 'linestyle': ':', 'alpha': 0.8, 'label': 'Stop Loss'},
            'target': {'color': 'green', 'linestyle': ':', 'alpha': 0.8, 'label': 'Target'}
        }

        for level_type, price in levels.items():
            if level_type in level_styles:
                style = level_styles[level_type]
                ax.axhline(y=price, **style)

                # Add price label
                ax.text(x_range + 0.5, price, f'{level_type.replace("_", " ").title()}: {price:.4f}',
                       verticalalignment='center')

        # Add trend arrows if mentioned in consensus
        if 'uptrend' in consensus_text.lower():
            self._add_trend_arrow(ax, 'up', x_range)
        elif 'downtrend' in consensus_text.lower():
            self._add_trend_arrow(ax, 'down', x_range)

        # Formatting
        ax.set_title("Consensus Trading Analysis", fontsize=16)
        ax.set_xlabel("Date", fontsize=12)
        ax.set_ylabel("Price", fontsize=12)
        ax.set_xticks(range(len(self.data.index)))
        ax.set_xticklabels(self.data.index.strftime('%Y-%m-%d'), rotation=45, ha='right')

        # Add legend
        ax.legend()

        # Adjust layout and save
        plt.tight_layout()
        plt.savefig(save_path)
        plt.close(fig)

        return save_path

    def _add_trend_arrow(self, ax, direction: str, x_range: int):
        """Add trend arrow to the chart."""
        y_range = ax.get_ylim()
        y_mid = (y_range[0] + y_range[1]) / 2
        arrow_length = x_range * 0.2

        if direction == 'up':
            dx = arrow_length
            dy = (y_range[1] - y_range[0]) * 0.1
            color = 'green'
        else:
            dx = arrow_length
            dy = -(y_range[1] - y_range[0]) * 0.1
            color = 'red'

        arrow = Arrow(x_range * 0.1, y_mid, dx, dy,
                     width=arrow_length * 0.1,
                     color=color, alpha=0.5)
        ax.add_patch(arrow)

def get_economic_events_for_currency(currency_code):
    """Get recent economic events for a specified currency."""
    try:
        # Get events from the last 30 days
        thirty_days_ago = timezone.now() - timezone.timedelta(days=90)
        events = EconomicEvent.objects.filter(
            currency=currency_code,
            date_time__gte=thirty_days_ago
        ).order_by('-date_time')
        
        if not events:
            return "No recent economic events found for this currency."
        
        # Format the events data
        events_text = f"Recent Economic Events for {currency_code}:\n\n"
        for event in events:
            impact_symbol = "" if event.impact == "high" else "" if event.impact == "medium" else ""
            events_text += f"Date: {event.date_time.strftime('%Y-%m-%d %H:%M')}\n"
            events_text += f"Event: {event.event_name} {impact_symbol}\n"
            events_text += f"Actual: {event.actual or 'N/A'}\n"
            events_text += f"Forecast: {event.forecast or 'N/A'}\n"
            events_text += f"Previous: {event.previous or 'N/A'}\n\n"
        
        return events_text
    
    except Exception as e:
        return f"Error retrieving economic events for {currency_code}: {str(e)}"


def extract_currencies_from_pair(forex_pair):
    """Extract base and quote currencies from a forex pair."""
    # Remove any '=X' suffix that might be added for yfinance
    clean_pair = forex_pair.replace('=X', '')
    
    # Dictionary mapping common forex pairs to their base and quote currencies
    forex_pairs_map = {
        'EURUSD': ('EUR', 'USD'),
        'GBPUSD': ('GBP', 'USD'),
        'USDJPY': ('USD', 'JPY'),
        'AUDUSD': ('AUD', 'USD'),
        'USDCHF': ('USD', 'CHF'),
        'NZDUSD': ('NZD', 'USD'),
        'USDCAD': ('USD', 'CAD'),
        'EURJPY': ('EUR', 'JPY'),
        'GBPJPY': ('GBP', 'JPY'),
        'EURGBP': ('EUR', 'GBP'),
        'EURAUD': ('EUR', 'AUD'),
        'EURCAD': ('EUR', 'CAD'),
        'EURCHF': ('EUR', 'CHF'),
        'GBPAUD': ('GBP', 'AUD'),
        'GBPCAD': ('GBP', 'CAD'),
        'GBPCHF': ('GBP', 'CHF'),
        'AUDCAD': ('AUD', 'CAD'),
        'AUDCHF': ('AUD', 'CHF'),
        'AUDJPY': ('AUD', 'JPY'),
        'CADCHF': ('CAD', 'CHF'),
        'CADJPY': ('CAD', 'JPY'),
        'CHFJPY': ('CHF', 'JPY'),
        'NZDCAD': ('NZD', 'CAD'),
        'NZDCHF': ('NZD', 'CHF'),
        'NZDJPY': ('NZD', 'JPY')
    }
    
    if clean_pair in forex_pairs_map:
        return forex_pairs_map[clean_pair]
    else:
        # Fallback: assume first 3 chars are base, last 3 are quote
        if len(clean_pair) >= 6:
            return (clean_pair[:3], clean_pair[3:6])
        else:
            return ('USD', 'USD')  # Default fallback


def get_economic_events_for_pair(forex_pair):
    """Get economic events for both currencies in a forex pair."""
    try:
        base_currency, quote_currency = extract_currencies_from_pair(forex_pair)
        
        # Get events for both currencies
        base_events = get_economic_events_for_currency(base_currency)
        quote_events = get_economic_events_for_currency(quote_currency)
        
        # Combine the events
        combined_events = f"=== ECONOMIC EVENTS FOR {forex_pair} ===\n\n"
        combined_events += f"BASE CURRENCY ({base_currency}):\n"
        combined_events += base_events + "\n"
        combined_events += f"QUOTE CURRENCY ({quote_currency}):\n"
        combined_events += quote_events + "\n"
        
        return combined_events
    
    except Exception as e:
        return f"Error retrieving economic events for pair {forex_pair}: {str(e)}"



@csrf_exempt
def fetch_news_data_api(request):
    """Enhanced news data function that includes economic events."""
    if request.method != 'POST':
        return JsonResponse({'error': 'Only POST requests allowed'}, status=405)

    try:
        body = json.loads(request.body)
        assets_to_fetch = body.get('assets', [])
        user_email = body.get('user_email', None)
    except json.JSONDecodeError:
        return JsonResponse({'error': 'Invalid JSON'}, status=400)

    if not assets_to_fetch or not user_email:
        return JsonResponse({'error': 'Missing assets or user_email'}, status=400)

    all_news_data = []

    conn = http.client.HTTPSConnection('api.marketaux.com')
    params_template = {
        'api_token': 'xH2KZ1sYqHmNRpfBVfb9C1BbItHMtlRIdZQoRlYw',
        'langauge': 'en',
        'limit': 3,
    }

    for asset in assets_to_fetch:
        params = params_template.copy()
        params['symbols'] = asset
        conn.request('GET', '/v1/news/all?{}'.format(urllib.parse.urlencode(params)))
        res = conn.getresponse()
        data = res.read().decode('utf-8')
        news_data = json.loads(data)

        for article in news_data.get('data', []):
            # Extract highlights properly
            highlights = ''
            if article.get('entities'):
                entity_highlights = article.get('entities')[0].get('highlights', '')
                # Handle different highlight formats
                if isinstance(entity_highlights, str):
                    highlights = entity_highlights
                elif isinstance(entity_highlights, dict):
                    # Extract the actual highlight text
                    highlights = entity_highlights.get('highlight', '') or entity_highlights.get('text', '')
                elif isinstance(entity_highlights, list) and len(entity_highlights) > 0:
                    highlights = entity_highlights[0] if isinstance(entity_highlights[0], str) else str(entity_highlights[0])
            
            news_entry_data = {
                'asset': asset,
                'title': article.get('title', ''),
                'description': article.get('description', ''),
                'source': article.get('source', ''),
                'url': article.get('url', ''),
                'highlights': highlights,  # Now always a string
            }
            all_news_data.append(news_entry_data)

    economic_events_data = []
    for asset in assets_to_fetch:
        economic_events = get_economic_events_for_pair(asset)
        economic_events_data.append({
            'asset': asset,
            'economic_events': economic_events
        })

    return JsonResponse({
        'message': all_news_data,
        'economic_events': economic_events_data
    }, safe=False)



def fetch_news_data(assets, user_email):
    """Enhanced news data function that includes economic events."""
    all_news_data = []

    # List of assets to fetch news data for
    assets_to_fetch = assets

    # Establish a connection to the API
    conn = http.client.HTTPSConnection('api.marketaux.com')

    # Define query parameters
    params_template = {
        'api_token': 'xH2KZ1sYqHmNRpfBVfb9C1BbItHMtlRIdZQoRlYw',
        'langauge': 'en',
        'limit': 3,
    }

    # Iterate through the assets and make API requests
    for asset in assets_to_fetch:
        # Update the symbol in the query parameters
        params = params_template.copy()
        params['symbols'] = asset

        # Send a GET request
        conn.request('GET', '/v1/news/all?{}'.format(urllib.parse.urlencode(params)))

        # Get the response
        res = conn.getresponse()

        # Read the response data
        data = res.read()

        # Decode the data from bytes to a string
        data_str = data.decode('utf-8')

        # Parse the JSON data
        news_data = json.loads(data_str)

        # Iterate through the news articles and save specific fields to the database
        for article in news_data['data']:
            title = article['title']
            description = article['description']
            source = article['source']
            url = article['url']
            highlights = article['entities'][0]['highlights'] if article.get('entities') else ''

            # Create a dictionary with the specific fields
            news_entry_data = {
                'asset': asset,
                'title': title,
                'description': description,
                'source': source,
                'url': url,
                'highlights': highlights,
            }
            all_news_data.append(news_entry_data)

    # Add economic events data for each asset
    economic_events_data = []
    for asset in assets_to_fetch:
        economic_events = get_economic_events_for_pair(asset)
        economic_events_data.append({
            'asset': asset,
            'economic_events': economic_events
        })

    return {
        'message': all_news_data,
        'economic_events': economic_events_data
    }


def analyse_image(image_data, news_data):
    """Enhanced image analysis function that includes economic events."""
    try:
        # Getting the base64 string
        base64_image = base64.b64encode(image_data).decode('utf-8')

        api_key = os.environ['OPENAI_API_KEY']

        # Extract discussion prompt if it exists
        discussion_prompt = news_data.get('discussion_prompt', '')

        # Extract economic events data
        economic_events_text = ""
        if 'economic_events' in news_data:
            for event_data in news_data['economic_events']:
                economic_events_text += event_data['economic_events'] + "\n"

        # Extract regular news data
        regular_news_text = ""
        if 'message' in news_data:
            for news_item in news_data['message']:
                regular_news_text += f"Title: {news_item['title']}\n"
                regular_news_text += f"Description: {news_item['description']}\n"
                regular_news_text += f"Source: {news_item['source']}\n"
                if news_item['highlights']:
                    regular_news_text += f"Highlights: {news_item['highlights']}\n"
                regular_news_text += "\n"

        # Construct a more interactive prompt
        prompt = f"""
        {discussion_prompt}

        Based on the trading chart image and the following fundamental data, provide an analysis that addresses the above context.

        ECONOMIC EVENTS DATA:
        {economic_events_text}

        RECENT NEWS DATA:
        {regular_news_text}

        Your response should be in JSON format with two keys:
        1. 'analysis': A detailed analysis that:
           - Incorporates both technical chart analysis and fundamental economic events
           - Directly responds to any previous trader's points if they exist
           - Explains how recent economic events might impact price action
           - Considers the timing and impact level of economic releases
           - Points out any correlation between economic events and chart patterns
           - Explains your reasoning for agreeing or disagreeing with previous analyses
        2. 'recommendation': Either 'buy', 'sell', or 'neutral'

        Make sure to format as valid JSON and avoid line breaks in the text.
        Pay special attention to high-impact economic events () as they can significantly move the market.
        """

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }

        payload = {
            "model": "gpt-4o-mini",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": prompt
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            "max_tokens": 1500  # Increased to accommodate economic analysis
        }

        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload)

        json_data = response.json()
        final_response = json_data['choices'][0]['message']['content']
        return final_response

    except Exception as e:
        print(f"Error occurred in analyse image function: {e}")
        return json.dumps({
            "analysis": f"Error occurred in analysis: {str(e)}",
            "recommendation": "neutral"
        })


def tradergpt(asset, interval, num_days, user_email):
    """Enhanced tradergpt function with economic events integration."""
    try:
        # Step 1: Fetch dataset and generate chart
        data = obtain_dataset(asset, interval, num_days)
        chart_path = generate_candlestick_chart(data)

        # Step 2: Fetch enhanced news data (including economic events)
        news_data = fetch_news_data([asset], user_email)

        # Step 3: Analyse chart with economic events
        chart_analysis = analyse_image_from_file(chart_path, news_data)

        # Combine analysis
        return {
            "chart_analysis": chart_analysis,
            "economic_events": news_data.get('economic_events', [])
        }
    except Exception as e:
        print(f"Error in combined analysis: {e}")


class MultiTraderDialogue:
    def __init__(self, trader1_settings: dict, trader2_settings: dict, max_messages: int = 6):
        self.trader1_settings = trader1_settings
        self.trader2_settings = trader2_settings
        self.max_messages = max_messages
        self.messages = []

        # Initialize market data for both traders
        self.trader1_data = obtain_dataset(
            trader1_settings['asset'],
            trader1_settings['interval'],
            trader1_settings['numDays']
        )
        self.trader2_data = obtain_dataset(
            trader2_settings['asset'],
            trader2_settings['interval'],
            trader2_settings['numDays']
        )

        # Generate charts for both traders
        self.trader1_chart = generate_candlestick_chart(self.trader1_data)
        self.trader2_chart = generate_candlestick_chart(self.trader2_data)

        # Initialize enhanced news data for both assets (including economic events)
        assets = list(set([trader1_settings['asset'], trader2_settings['asset']]))
        self.news_data = fetch_news_data(assets, user_email=None)

        # Initialize chart annotators
        self.trader1_annotator = ChartAnnotator(self.trader1_data)
        self.trader2_annotator = ChartAnnotator(self.trader2_data)

        # Define trader personalities based on settings
        self.trader_personalities = {
            "Trader1": {
                "style": trader1_settings.get('style', 'Conservative'),
                "focus": trader1_settings.get('focus', 'long-term trends and fundamental analysis'),
                "risk_tolerance": trader1_settings.get('risk_tolerance', 'low'),
                "settings": trader1_settings,
                "data": self.trader1_data,
                "chart": self.trader1_chart
            },
            "Trader2": {
                "style": trader2_settings.get('style', 'Aggressive'),
                "focus": trader2_settings.get('focus', 'short-term momentum and technical patterns'),
                "risk_tolerance": trader2_settings.get('risk_tolerance', 'high'),
                "settings": trader2_settings,
                "data": self.trader2_data,
                "chart": self.trader2_chart
            }
        }

    def _create_discussion_prompt(self, trader_id: str, previous_message: Optional[TraderMessage] = None) -> str:
        personality = self.trader_personalities[trader_id]
        settings = personality['settings']
        data = personality['data']

        # Get economic events for this trader's asset
        economic_events_for_asset = ""
        for event_data in self.news_data.get('economic_events', []):
            if event_data['asset'] == settings['asset']:
                economic_events_for_asset = event_data['economic_events']
                break

        base_prompt = f"""
        As a {personality['style']} trader with {personality['risk_tolerance']} risk tolerance,
        analyzing {settings['asset']} on the {settings['interval']} timeframe 
        with a {settings['numDays']}-day lookback period, focusing on {personality['focus']},
        and considering both technical analysis and fundamental economic events,
        """

        if previous_message:
            base_prompt += f"""
            analyze this chart and respond to the following analysis from another trader:

            Previous Analysis: {previous_message.content}

            Consider:
            1. What points do you agree with and why?
            2. What factors might need additional consideration?
            3. How does your timeframe and trading style inform your perspective?
            4. Are there any differences in market behavior between your timeframe and the other trader's timeframe?
            5. How does your risk tolerance affect your view of the suggested trades?
            6. How do the recent economic events support or contradict the previous analysis?
            7. Are there any upcoming economic releases that could impact the trade setup?

            ECONOMIC EVENTS CONTEXT:
            {economic_events_for_asset}

            Aim to find common ground while highlighting important considerations from your perspective.
            Pay special attention to how economic fundamentals align with or contradict technical patterns.
            """
        else:
            base_prompt += f"""
            provide your initial analysis of this chart, incorporating both technical and fundamental analysis.

            ECONOMIC EVENTS CONTEXT:
            {economic_events_for_asset}

            Consider how recent economic events might have influenced the current price action and 
            what they suggest for future price movements.
            """

        # Add market context
        current_close = data['Close'].iloc[-1].item()
        current_open = data['Open'].iloc[-1].item()
        price_change = current_close - current_open
        price_change_pct = (price_change / current_open) * 100

        market_context = f"""
        Current market context for {settings['asset']} ({settings['interval']} timeframe):
        - Price change: {price_change_pct:.2f}%
        - Current price: {current_close:.4f}
        """

        return base_prompt + "\n" + market_context

    def _create_consensus_prompt(self) -> str:
        previous_analyses = "\n".join([
            f"{msg.trader_id} ({self.trader_personalities[msg.trader_id]['settings']['interval']} timeframe, {self.trader_personalities[msg.trader_id]['style']} style): {msg.content}"
            for msg in self.messages if msg.trader_id in ['Trader1', 'Trader2']
        ])

        # Combine all economic events data
        all_economic_events = ""
        for event_data in self.news_data.get('economic_events', []):
            all_economic_events += f"\n{event_data['asset']}:\n{event_data['economic_events']}\n"

        return f"""
        Review the following discussion about market analysis from different timeframes and trading styles:

        {previous_analyses}

        COMBINED ECONOMIC EVENTS DATA:
        {all_economic_events}

        As a group of traders analyzing multiple timeframes and incorporating fundamental analysis, 
        we need to reach a final consensus that considers both technical and fundamental factors.

        Please provide specific levels in your analysis:
        1. Key support and resistance levels from both timeframes
        2. Suggested entry price considering both analyses and economic events
        3. Stop-loss level that respects both timeframes and fundamental risks
        4. Target price based on both technical and fundamental perspectives
        5. Overall trend direction (uptrend/downtrend) on each timeframe
        6. Economic event risks and opportunities

        Also include:
        1. Points of agreement between different timeframe analyses
        2. How different risk tolerances and trading styles are balanced
        3. How technical analysis aligns with or contradicts fundamental economic data
        4. Final recommendation that considers both timeframes and economic events
        5. Risk management suggestions that account for both technical and fundamental risks
        6. Timing considerations based on upcoming economic releases

        Format your response as JSON with 'analysis' and 'recommendation' keys.
        Include numerical price levels in your analysis for chart annotation.
        Give special weight to high-impact economic events () in your analysis.
        """

    def _generate_response(self, trader_id: str, previous_message: Optional[TraderMessage] = None,
                         message_type: str = "discussion") -> str:
        if message_type == "consensus":
            prompt = self._create_consensus_prompt()
            chart_path = self.trader1_chart
        else:
            prompt = self._create_discussion_prompt(trader_id, previous_message)
            chart_path = self.trader_personalities[trader_id]['chart']

        modified_news = self.news_data.copy()
        modified_news['discussion_prompt'] = prompt

        return analyse_image_from_file(chart_path, modified_news)

    def conduct_dialogue(self) -> Tuple[List[TraderMessage], str]:
        # First message from Trader1
        initial_message = TraderMessage(
            trader_id="Trader1",
            content=self._generate_response("Trader1"),
            message_type="discussion"
        )
        self.messages.append(initial_message)

        # Continue discussion
        current_msg_count = 1
        discussion_messages = self.max_messages - 1

        while current_msg_count < discussion_messages:
            current_trader = "Trader2" if current_msg_count % 2 == 1 else "Trader1"
            previous_message = self.messages[-1]

            response = TraderMessage(
                trader_id=current_trader,
                content=self._generate_response(current_trader, previous_message),
                message_type="discussion",
                responding_to=previous_message.trader_id
            )
            self.messages.append(response)
            current_msg_count += 1

        # Final consensus
        consensus = TraderMessage(
            trader_id="Consensus",
            content=self._generate_response(
                trader_id="Consensus",
                message_type="consensus"
            ),
            message_type="consensus"
        )
        self.messages.append(consensus)

        # Create annotated chart based on consensus
        annotated_chart_path = self.trader1_annotator.draw_annotated_chart(
            consensus.content,
            save_path=f"annotated_multi_timeframe_chart.png"
        )

        return self.messages, annotated_chart_path


@csrf_exempt
def get_trader_analysis(request):
    """Enhanced trader analysis endpoint with economic events integration."""
    try:
        if request.method == 'POST':
            data = json.loads(request.body)
            traders_settings = data.get('traders', {})
            
            # Validate trader settings
            if 'trader1' not in traders_settings or 'trader2' not in traders_settings:
                return JsonResponse({
                    'status': 'error',
                    'message': 'Settings for both traders are required.',
                    'type': 'ValidationError'
                }, status=400)
            
            # Initialize multi-trader dialogue with enhanced economic events
            dialogue = MultiTraderDialogue(
                trader1_settings=traders_settings['trader1'],
                trader2_settings=traders_settings['trader2']
            )
            
            # Run the dialogue
            conversation, chart_path = dialogue.conduct_dialogue()
            
            # Convert the conversation to a serializable format
            conversation_data = []
            for msg in conversation:
                if isinstance(msg.content, str):
                    content = msg.content.replace('```json\n', '').replace('\n```', '')
                    try:
                        parsed_content = json.loads(content)
                        if 'analysis' in parsed_content:
                            if isinstance(parsed_content['analysis'], str):
                                parsed_content['analysis'] = parsed_content['analysis'][:2000]  # Increased limit for economic analysis
                            else:
                                parsed_content['analysis'] = str(parsed_content['analysis'])[:2000]
                        content = parsed_content
                    except json.JSONDecodeError:
                        content = content[:2000]  # Increased limit for economic analysis
                else:
                    content = msg.content

                conversation_data.append({
                    'trader_id': msg.trader_id,
                    'content': content,
                    'message_type': msg.message_type,
                    'responding_to': msg.responding_to,
                    'settings': {
                        'asset': dialogue.trader_personalities[msg.trader_id]['settings']['asset'] if msg.trader_id in dialogue.trader_personalities else None,
                        'interval': dialogue.trader_personalities[msg.trader_id]['settings']['interval'] if msg.trader_id in dialogue.trader_personalities else None,
                        'numDays': dialogue.trader_personalities[msg.trader_id]['settings']['numDays'] if msg.trader_id in dialogue.trader_personalities else None,
                        'style': dialogue.trader_personalities[msg.trader_id]['style'] if msg.trader_id in dialogue.trader_personalities else None,
                        'focus': dialogue.trader_personalities[msg.trader_id]['focus'] if msg.trader_id in dialogue.trader_personalities else None,
                        'risk_tolerance': dialogue.trader_personalities[msg.trader_id]['risk_tolerance'] if msg.trader_id in dialogue.trader_personalities else None,
                    } if msg.trader_id != 'Consensus' else None
                })

            # Process the chart image
            image = Image.open(chart_path)
            if image.mode == 'RGBA':
                image = image.convert('RGB')
            
            max_size = (800, 800)
            image.thumbnail(max_size)
            
            compressed_image_io = io.BytesIO()
            image.save(compressed_image_io, format='JPEG', quality=50)
            compressed_image_io.seek(0)
            
            encoded_image = base64.b64encode(compressed_image_io.read()).decode('utf-8')
            
            if os.path.exists(chart_path):
                os.remove(chart_path)
            
            response_data = {
                'status': 'success',
                'conversation': conversation_data,
                'chart_image': encoded_image,
                'analysis_summary': {
                    'trader1': traders_settings['trader1'],
                    'trader2': traders_settings['trader2']
                },
                'economic_events_included': True,  # Flag to indicate economic events are included
                'currencies_analyzed': []  # Will be populated with currency pairs
            }
            
            # Add information about which currencies were analyzed
            for asset in [traders_settings['trader1']['asset'], traders_settings['trader2']['asset']]:
                base_curr, quote_curr = extract_currencies_from_pair(asset)
                response_data['currencies_analyzed'].append({
                    'pair': asset,
                    'base_currency': base_curr,
                    'quote_currency': quote_curr
                })
            
            return JsonResponse(response_data)
        else:
            return JsonResponse({
                'status': 'error',
                'message': 'Invalid request method.',
                'type': 'InvalidRequestMethod'
            }, status=400)
            
    except Exception as e:
        return JsonResponse({
            'status': 'error',
            'message': str(e),
            'type': type(e).__name__
        }, status=500)


# Additional utility function to help with currency mapping updates
def update_currency_mapping():
    """Utility function to add more currency pairs to the mapping if needed."""
    additional_pairs = {
        'GBPNZD': ('GBP', 'NZD'),
        'AUDNZD': ('AUD', 'NZD'),
        'EURCZK': ('EUR', 'CZK'),
        'USDPLN': ('USD', 'PLN'),
        'USDHUF': ('USD', 'HUF'),
        'USDTRY': ('USD', 'TRY'),
        'USDZAR': ('USD', 'ZAR'),
        'USDMXN': ('USD', 'MXN'),
        'USDSEK': ('USD', 'SEK'),
        'USDNOK': ('USD', 'NOK'),
        'USDDKK': ('USD', 'DKK')
    }
    
    return additional_pairs

def bullish_market_sentiment(asset):
    
    news_data = fetch_news_data([asset], None)
    prompt = f'''
    Please give me the sentiment reading for this news data in one word.

    Either Bullish, Bearish or Neutral

    {news_data}

    '''
    sentiment = chat_gpt(prompt)
    if sentiment.lower() == 'bullish':
        return True
    else:
        return False


def bearish_market_sentiment(asset):

    news_data = fetch_news_data([asset], None)
    prompt = f'''
    Please give me the sentiment reading for this news data in one word.

    Either Bullish, Bearish or Neutral

    {news_data}

    '''
    sentiment = chat_gpt(prompt)
    if sentiment.lower() == 'bearish':
        return True
    else:
        return False


@csrf_exempt
def save_backtest_model_data(request):
    if request.method == 'POST':
        try:
            data = json.loads(request.body.decode('utf-8'))
            backtest = BacktestModels.objects.create(
                chosen_dataset=data.get('chosen_dataset'),
                generated_code=data.get('generated_code'),
                model_backtested=data.get('model_backtested', False),
                dataset_start=data.get('dataset_start'),
                dataset_end=data.get('dataset_end'),
                initial_capital=data.get('initial_capital')
            )
            return JsonResponse({'message': 'Backtest data saved successfully!', 'id': backtest.id}, status=201)
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=400)
    return JsonResponse({'error': 'Invalid request method'}, status=405)


@csrf_exempt
def generate_idea(request):
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            
            # Create a new idea
            idea = IdeaModel.objects.create(
                idea_category=data.get('idea_category'),
                idea_text=data.get('idea_text'),
                idea_tracker=data.get('idea_tracker', 'Pending')
            )
            
            return JsonResponse({
                'success': True,
                'message': 'Idea created successfully',
                'idea': {
                    'id': idea.id,
                    'idea_category': idea.idea_category,
                    'idea_text': idea.idea_text,
                    'idea_tracker': idea.idea_tracker,
                    'created_at': idea.created_at.isoformat()
                }
            })
        except Exception as e:
            return JsonResponse({
                'success': False,
                'message': str(e)
            }, status=400)
    
    return JsonResponse({
        'success': False,
        'message': 'Invalid request method'
    }, status=405)


@csrf_exempt
def fetch_ideas(request):
    if request.method == 'GET':
        try:
            # Get all ideas, ordered by creation date (newest first)
            ideas = IdeaModel.objects.all().order_by('-created_at')
            
            # Serialize the ideas
            ideas_list = [{
                'id': idea.id,
                'idea_category': idea.idea_category,
                'idea_text': idea.idea_text,
                'idea_tracker': idea.idea_tracker,
                'created_at': idea.created_at.isoformat()
            } for idea in ideas]
            
            return JsonResponse(ideas_list, safe=False)
        except Exception as e:
            return JsonResponse({
                'success': False,
                'message': str(e)
            }, status=500)
    
    return JsonResponse({
        'success': False,
        'message': 'Invalid request method'
    }, status=405)


@csrf_exempt
def delete_idea(request):
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            idea_id = data.get('idea_id')
            
            # Find and delete the idea
            idea = IdeaModel.objects.get(id=idea_id)
            idea.delete()
            
            return JsonResponse({'status': 'success', 'message': 'Idea deleted successfully'})
        except IdeaModel.DoesNotExist:
            return JsonResponse({'status': 'error', 'message': 'Idea not found'}, status=404)
        except Exception as e:
            return JsonResponse({'status': 'error', 'message': str(e)}, status=500)
    
    return JsonResponse({'status': 'error', 'message': 'Method not allowed'}, status=405)


@csrf_exempt
def update_idea_tracker(request):
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            idea_id = data.get('idea_id')
            new_tracker = data.get('idea_tracker')
            
            # Find and update the idea
            idea = IdeaModel.objects.get(id=idea_id)
            idea.idea_tracker = new_tracker
            idea.save()
            
            return JsonResponse({'status': 'success', 'message': 'Idea tracker updated successfully'})
        except IdeaModel.DoesNotExist:
            return JsonResponse({'status': 'error', 'message': 'Idea not found'}, status=404)
        except Exception as e:
            return JsonResponse({'status': 'error', 'message': str(e)}, status=500)
    
    return JsonResponse({'status': 'error', 'message': 'Method not allowed'}, status=405)


@csrf_exempt
def update_idea(request):
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            idea_id = data.get('idea_id')
            
            # Find the idea by ID
            idea = IdeaModel.objects.get(id=idea_id)
            
            # Update the idea fields
            idea.idea_category = data.get('idea_category', idea.idea_category)
            idea.idea_text = data.get('idea_text', idea.idea_text)
            idea.idea_tracker = data.get('idea_tracker', idea.idea_tracker)
            
            # Save the updated idea
            idea.save()
            
            return JsonResponse({
                'status': 'success', 
                'message': 'Idea updated successfully',
                'idea': {
                    'id': idea.id,
                    'idea_category': idea.idea_category,
                    'idea_text': idea.idea_text,
                    'idea_tracker': idea.idea_tracker,
                    'created_at': idea.created_at.isoformat()
                }
            })
        except IdeaModel.DoesNotExist:
            return JsonResponse({
                'status': 'error', 
                'message': 'Idea not found'
            }, status=404)
        except Exception as e:
            return JsonResponse({
                'status': 'error', 
                'message': str(e)
            }, status=500)
    
    return JsonResponse({
        'status': 'error', 
        'message': 'Method not allowed'
    }, status=405)


@csrf_exempt
def get_ai_account_summary(request):
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            account_name = data.get('account_name')
            metrics = data.get('metrics', {})
            trades = data.get('trades', [])
            
            # Create a prompt for the AI with professional guidance
            prompt = f"""
            Prepare a comprehensive trading performance analysis for account '{account_name}' with the following requirements:
            - Maintain a professional and analytical tone
            - Use clear, concise language
            - Provide objective insights and data-driven recommendations
            
            Performance Metrics Overview:
            - Win Rate: {metrics.get('winRate', 0)}%
            - Average Win: ${metrics.get('averageWin', 0)}
            - Average Loss: ${metrics.get('averageLoss', 0)}
            - Profit Factor: {metrics.get('profitFactor', 0)}
            - Number of Wins: {metrics.get('numberOfWins', 0)}
            - Number of Losses: {metrics.get('numberOfLosses', 0)}

            Trading Data: \n\n{trades}\n\n
            
            Analysis Guidelines:
            1. Deliver a precise performance assessment
            2. Identify key performance trends and patterns
            3. Develop specific, actionable improvement strategies
            4. Maintain a concise format (under 250 words)
            5. Use professional terminology
            6. Highlight both strengths and areas for potential improvement
            7. AVOID USING ASTERISKS, HASHTAGS, OR MARKDOWN FORMATTING

            
            Presentation Instructions:
            - AVOID USING ASTERISKS, HASHTAGS, OR MARKDOWN FORMATTING
            - Use subtle emojis sparingly for visual emphasis 
            - Ensure a structured, professional presentation

            AVOID USING ASTERISKS, HASHTAGS, OR MARKDOWN FORMATTING

            """
            
            # Get AI summary
            summary = chat_gpt(prompt)
            
            return JsonResponse({'summary': summary})
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=500)
    
    return JsonResponse({'error': 'Invalid request method'}, status=400)


@csrf_exempt
def save_quiz(request):
    if request.method == 'POST':
        try:
            # Parse the incoming JSON data
            data = json.loads(request.body)
            
            # Extract quiz details
            quiz_name = data.get('quiz_name', 'Unnamed Quiz')
            total_questions = data.get('total_questions', 0)
            correct_answers = data.get('correct_answers', 0)
            questions = data.get('questions', [])

            # Create SavedQuiz instance
            saved_quiz = SavedQuiz.objects.create(
                quiz_name=quiz_name,
                total_questions=total_questions,
                correct_answers=correct_answers
            )

            # Create SavedQuizQuestion instances
            for question_data in questions:
                SavedQuizQuestion.objects.create(
                    saved_quiz=saved_quiz,
                    question=question_data.get('question', ''),
                    selected_answer=question_data.get('selectedAnswer', ''),
                    correct_answer=question_data.get('correctAnswer', ''),
                    is_correct=question_data.get('isCorrect', False)
                )

            return JsonResponse({
                'status': 'success', 
                'message': 'Quiz saved successfully',
                'saved_quiz_id': saved_quiz.id
            }, status=201)

        except json.JSONDecodeError:
            return JsonResponse({
                'status': 'error', 
                'message': 'Invalid JSON data'
            }, status=400)
        except Exception as e:
            return JsonResponse({
                'status': 'error', 
                'message': str(e)
            }, status=400)
    
    return JsonResponse({
        'status': 'error', 
        'message': 'Invalid request method'
    }, status=405)


@csrf_exempt
def fetch_saved_quizzes(request):
    if request.method != 'GET':
        return JsonResponse({
            'status': 'error',
            'message': 'Only GET method is allowed'
        }, status=405)  # Method Not Allowed status code

    try:
        # Fetch all saved quizzes, ordered by most recent first
        saved_quizzes = SavedQuiz.objects.order_by('-created_at')
        
        # Prepare the response data
        quizzes_data = []
        for quiz in saved_quizzes:
            # Get associated questions for each quiz
            questions = quiz.questions.all()
            
            quiz_details = {
                'id': quiz.id,
                'quiz_name': quiz.quiz_name,
                'total_questions': quiz.total_questions,
                'correct_answers': quiz.correct_answers,
                'created_at': quiz.created_at.isoformat(),
                'questions': [
                    {
                        'question': q.question,
                        'selected_answer': q.selected_answer,
                        'correct_answer': q.correct_answer,
                        'is_correct': q.is_correct
                    } for q in questions
                ]
            }
            quizzes_data.append(quiz_details)
        
        return JsonResponse({
            'status': 'success',
            'quizzes': quizzes_data
        }, safe=True)
    
    except Exception as e:
        return JsonResponse({
            'status': 'error',
            'message': str(e)
        }, status=500)


@csrf_exempt
def delete_quiz(request, quiz_id):
    """
    Delete a saved quiz by its ID
    """
    if request.method == 'DELETE':
        try:
            # Get the quiz or return 404 if not found
            quiz = get_object_or_404(SavedQuiz, id=quiz_id)
            
            # Delete the quiz (this will also delete related questions due to CASCADE)
            quiz.delete()
            
            return JsonResponse({
                'status': 'success', 
                'message': 'Quiz deleted successfully'
            }, status=200)
        
        except Exception as e:
            return JsonResponse({
                'status': 'error', 
                'message': str(e)
            }, status=500)
    
    return JsonResponse({
        'status': 'error', 
        'message': 'Invalid request method'
    }, status=405)


import logging

logger = logging.getLogger(__name__)

@csrf_exempt
def delete_music(request):
    MusicModel.objects.all().delete()
    return JsonResponse({'message': 'Songs Deleted!'})


@csrf_exempt
def save_music(request):
    if request.method != 'POST':
        return JsonResponse({'error': 'Only POST method is allowed'}, status=405)
    
    try:
        name = request.POST.get('name')
        file = request.FILES.get('file')
        
        if not name or not file:
            return JsonResponse({'error': 'Name and file are required'}, status=400)
        
        # Read the file content
        file_data = file.read()
        file_name = file.name
        content_type = file.content_type or 'audio/mpeg'
        
        # Check if a song with this name already exists
        existing_song = MusicModel.objects.filter(name=name).first()
        if existing_song:
            # If it exists, update the record
            existing_song.file_data = file_data
            existing_song.file_name = file_name
            existing_song.content_type = content_type
            existing_song.save()
            return JsonResponse({'success': True, 'message': f'Updated song: {name}'})
        
        # Create new song record
        music = MusicModel(
            name=name, 
            file_data=file_data,
            file_name=file_name,
            content_type=content_type
        )
        music.save()
        
        return JsonResponse({'success': True, 'message': f'Saved song: {name}'})
    
    except Exception as e:
        logger.error(f"Error saving music: {str(e)}")
        return JsonResponse({'error': str(e)}, status=500)

from django.urls import reverse


@csrf_exempt
def fetch_music(request):
    try:
        songs = MusicModel.objects.all()
        songs_data = []
        
        for song in songs:
            # Generate a URL to stream this song
            stream_url = request.build_absolute_uri(
                reverse('stream_music', args=[song.id])
            )
            
            songs_data.append({
                'id': song.id,
                'name': song.name,
                'file': stream_url,
                'file_name': song.file_name,
                'updated_at': song.updated_at.isoformat()
            })
        
        return JsonResponse({'songs': songs_data}, safe=False)
    except Exception as e:
        logger.error(f"Error fetching music: {str(e)}")
        return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
def stream_music(request, song_id):
    try:
        song = get_object_or_404(MusicModel, id=song_id)
        
        # Create a response with the binary data
        response = HttpResponse(song.file_data, content_type=song.content_type)
        response['Content-Disposition'] = f'inline; filename="{song.file_name}"'
        
        return response
    except Exception as e:
        logger.error(f"Error streaming music: {str(e)}")
        return HttpResponse(status=500)        


@csrf_exempt
def fetch_asset_update(request):
    """Fetch current data for all tracked assets"""
    try:
        assets = AssetsTracker.objects.all()
        asset_data = []

        for asset_obj in assets:
            asset = asset_obj.asset
            
            try:
                # Use your existing obtain_dataset function - more reliable
                data = obtain_dataset(asset, "1d", 5)  # Get daily data for the last 5 days
                
                if not data.empty and len(data) >= 2:
                    # Get the most recent price
                    current_price = float(data['Close'].iloc[-1])
                    
                    # Get the previous candle's close for comparison
                    previous_price = float(data['Close'].iloc[-2])
                    
                    # Calculate the percentage change
                    percent_change = round(((current_price - previous_price) / previous_price) * 100, 2)

                    # Format the timestamp in a readable format
                    last_updated = data.index[-1].strftime("%Y-%m-%d %H:%M")

                    asset_data.append({
                        'id': asset_obj.id,
                        'asset': asset,
                        'current_price': round(current_price, 4),
                        'percent_change': percent_change,
                        'last_updated': last_updated
                    })
                else:
                    # Handle case where we don't have enough data
                    asset_data.append({
                        'id': asset_obj.id,
                        'asset': asset,
                        'current_price': 0,
                        'percent_change': 0,
                        'error': "Insufficient data"
                    })
            except Exception as e:
                print(f"Error getting data for {asset}: {str(e)}")
                # Include the asset in the response anyway with default values
                asset_data.append({
                    'id': asset_obj.id,
                    'asset': asset,
                    'current_price': 0,
                    'percent_change': 0,
                    'error': str(e)
                })

        return JsonResponse(asset_data, safe=False)
    except Exception as e:
        print(f"Error in fetch_asset_update: {str(e)}")
        return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def get_tracked_assets(request):
    """Get all tracked assets"""
    try:
        assets = AssetsTracker.objects.all()
        asset_data = []
        
        for asset_obj in assets:
            asset = asset_obj.asset
            
            try:
                # Use obtain_dataset function with daily interval to get day-over-day changes
                data = obtain_dataset(asset, "1d", 5)  # Get daily data for the last 5 days
                
                if not data.empty and len(data) >= 2:
                    # Get the most recent price
                    current_price = float(data['Close'].iloc[-1])
                    
                    # Get yesterday's close for day-over-day comparison
                    yesterday_close = float(data['Close'].iloc[-2])
                    
                    # Calculate the percentage change
                    percent_change = round(((current_price - yesterday_close) / yesterday_close) * 100, 2)
                    
                    asset_data.append({
                        'id': asset_obj.id,
                        'asset': asset,
                        'current_price': round(current_price, 4),
                        'percent_change': percent_change,
                        'last_updated': data.index[-1].strftime("%Y-%m-%d")
                    })
                else:
                    # Handle case where we don't have enough data
                    asset_data.append({
                        'id': asset_obj.id,
                        'asset': asset,
                        'current_price': 0,
                        'percent_change': 0,
                        'error': "Insufficient data"
                    })
            except Exception as e:
                print(f"Error getting data for {asset}: {str(e)}")
                # Include the asset in the response anyway with default values
                asset_data.append({
                    'id': asset_obj.id,
                    'asset': asset,
                    'current_price': 0,
                    'percent_change': 0,
                    'error': str(e)
                })
        
        return JsonResponse(asset_data, safe=False)
    except Exception as e:
        print(f"Error in get_tracked_assets: {str(e)}")
        return JsonResponse({'error': str(e)}, status=500)
        
        
@csrf_exempt
@require_http_methods(["POST"])
def add_tracked_asset(request):
    """Add a new asset to track"""
    
    data = json.loads(request.body)
    asset = data.get('asset')
        
    if not asset:
        return JsonResponse({'error': 'Asset is required'}, status=400)
        
    # Check if asset already exists
    if AssetsTracker.objects.filter(asset=asset).exists():
        return JsonResponse({'error': 'Asset already tracked'}, status=400)
        
    # Create new asset tracker
    asset_tracker = AssetsTracker.objects.create(asset=asset)
        
    return JsonResponse({
        'id': asset_tracker.id,
        'asset': asset_tracker.asset
    })
    # except Exception as e:
    #     return JsonResponse({'error': str(e)}, status=500)

@csrf_exempt
@require_http_methods(["POST"])
def remove_tracked_asset(request):
    """Remove a tracked asset"""
    
    data = json.loads(request.body)
    asset_id = data.get('id')
        
    if not asset_id:
        return JsonResponse({'error': 'Asset ID is required'}, status=400)
        
    # Delete the asset tracker
    AssetsTracker.objects.filter(id=asset_id).delete()
        
    return JsonResponse({'success': True})
    # except Exception as e:
    #     return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def get_trade_ideas(request):
    trade_ideas = TradeIdea.objects.all().order_by('-date_created')
    data = list(trade_ideas.values())
    return JsonResponse({'trade_ideas': data})

@csrf_exempt
@require_http_methods(["POST"])
def create_trade_idea(request):
    try:
        data = json.loads(request.body)
        trade_idea = TradeIdea.objects.create(
            heading=data.get('heading'),
            asset=data.get('asset'),
            trade_idea=data.get('trade_idea'),
            trade_status=data.get('trade_status', 'pending'),
            target_price=data.get('target_price'),
            stop_loss=data.get('stop_loss'),
            entry_price=data.get('entry_price'),
            outcome=data.get('outcome', 'pending')  # Added outcome field
        )
        return JsonResponse({
            'success': True,
            'trade_idea_id': trade_idea.id
        })
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=400)

@csrf_exempt
@require_http_methods(["PUT"])
def update_trade_idea(request, id):
    try:
        data = json.loads(request.body)
        trade_idea = TradeIdea.objects.get(id=id)
        
        # Update fields if they exist in request
        if 'heading' in data:
            trade_idea.heading = data['heading']
        if 'asset' in data:
            trade_idea.asset = data['asset']
        if 'trade_idea' in data:
            trade_idea.trade_idea = data['trade_idea']
        if 'trade_status' in data:
            trade_idea.trade_status = data['trade_status']
        if 'target_price' in data:
            trade_idea.target_price = data['target_price']
        if 'stop_loss' in data:
            trade_idea.stop_loss = data['stop_loss']
        if 'entry_price' in data:
            trade_idea.entry_price = data['entry_price']
        if 'outcome' in data:  # Added outcome field handling
            trade_idea.outcome = data['outcome']
            
        trade_idea.save()
        
        return JsonResponse({
            'success': True,
            'trade_idea_id': trade_idea.id
        })
    except TradeIdea.DoesNotExist:
        return JsonResponse({
            'success': False,
            'error': 'Trade idea not found'
        }, status=404)
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=400)

@csrf_exempt
@require_http_methods(["DELETE"])
def delete_trade_idea(request, id):
    try:
        trade_idea = TradeIdea.objects.get(id=id)
        trade_idea.delete()
        return JsonResponse({
            'success': True
        })
    except TradeIdea.DoesNotExist:
        return JsonResponse({
            'success': False,
            'error': 'Trade idea not found'
        }, status=404)
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=400)



from django.conf import settings
from datetime import datetime, date
from decimal import Decimal
import base64
from django.core.files.base import ContentFile

@csrf_exempt
@require_http_methods(["GET"])
def get_prop_firms(request):
    firms = PropFirm.objects.all()
    data = []
    
    for firm in firms:
        firm_data = {
            'id': firm.id,
            'name': firm.name,
            'website': firm.website,
            'description': firm.description
        }
        try:

            if firm.logo:
                # Read the file content directly from the model
                image_file = firm.logo
                encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
                image_type = firm.logo.name.split('.')[-1].lower()
                firm_data['logo'] = f"data:image/{image_type};base64,{encoded_string}"
            else:
                firm_data['logo'] = None
        except Exception as e:
            print(f'Error occured in get_prop_firms function: {e}')
            firm_data['logo'] = None


            
        data.append(firm_data)
    
    return JsonResponse({'firms': data})


@csrf_exempt
@require_http_methods(["POST"])
def create_prop_firm(request):
    try:
        # Check if the request has files
        if request.FILES and 'logo' in request.FILES:
            # Handle form data with file upload
            name = request.POST.get('name')
            website = request.POST.get('website')
            description = request.POST.get('description')
            
            # Create prop firm
            firm = PropFirm.objects.create(
                name=name,
                website=website,
                description=description
            )
            
            # Handle logo upload
            firm.logo = request.FILES['logo']
            firm.save()
        else:
            # Handle JSON data without file upload
            data = json.loads(request.body)
            name = data.get('name')
            website = data.get('website')
            description = data.get('description')
            
            # Create prop firm
            firm = PropFirm.objects.create(
                name=name,
                website=website,
                description=description
            )
        
        return JsonResponse({
            'success': True,
            'firm': {
                'id': firm.id,
                'name': firm.name,
                'logo': firm.logo.url if firm.logo else None,
                'website': firm.website,
                'description': firm.description
            }
        })
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=400)


@csrf_exempt
@require_http_methods(["GET"])
def get_prop_accounts(request):
    accounts = PropFirmAccount.objects.all()
    data = []
    
    for account in accounts:
        account_data = {
            'id': account.id,
            'prop_firm': {
                'id': account.prop_firm.id,
                'name': account.prop_firm.name,
                'logo': account.prop_firm.logo.url if account.prop_firm.logo else None
            },
            'account_name': account.account_name,
            'account_id': account.account_id,
            'account_type': account.account_type,
            'status': account.status,
            'initial_balance': float(account.initial_balance),
            'current_balance': float(account.current_balance),
            'current_equity': float(account.current_equity),
            'daily_loss_limit': float(account.daily_loss_limit) if account.daily_loss_limit else None,
            'max_loss_limit': float(account.max_loss_limit) if account.max_loss_limit else None,
            'profit_target': float(account.profit_target) if account.profit_target else None,
            'start_date': account.start_date.isoformat(),
            'end_date': account.end_date.isoformat() if account.end_date else None,
            'days_remaining': account.days_remaining(),
            'percentage_to_target': account.percentage_to_target(),
        }
        data.append(account_data)
    
    return JsonResponse({'accounts': data})


@csrf_exempt
@require_http_methods(["POST"])
def create_prop_account(request):
    try:
        data = json.loads(request.body)
        
        prop_firm = PropFirm.objects.get(id=data.get('prop_firm_id'))
        
        account = PropFirmAccount.objects.create(
            prop_firm=prop_firm,
            account_name=data.get('account_name'),
            account_id=data.get('account_id'),
            account_type=data.get('account_type'),
            status=data.get('status', 'IN_PROGRESS'),
            initial_balance=Decimal(data.get('initial_balance')),
            current_balance=Decimal(data.get('initial_balance')),  # Start with initial balance
            current_equity=Decimal(data.get('initial_balance')),   # Start with initial balance
            daily_loss_limit=Decimal(data.get('daily_loss_limit')) if data.get('daily_loss_limit') else None,
            max_loss_limit=Decimal(data.get('max_loss_limit')) if data.get('max_loss_limit') else None,
            profit_target=Decimal(data.get('profit_target')) if data.get('profit_target') else None,
            start_date=datetime.strptime(data.get('start_date'), '%Y-%m-%d').date(),
            end_date=datetime.strptime(data.get('end_date'), '%Y-%m-%d').date() if data.get('end_date') else None,
        )
        
        # Update metrics
        update_prop_metrics()
        
        return JsonResponse({
            'success': True,
            'account': {
                'id': account.id,
                'account_name': account.account_name
            }
        })
    except PropFirm.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Prop firm not found'}, status=404)
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=400)


@csrf_exempt
@require_http_methods(["POST"])
def update_prop_account_balance(request, account_id):
    try:
        data = json.loads(request.body)
        account = PropFirmAccount.objects.get(id=account_id)
        
        account.current_balance = Decimal(data.get('current_balance'))
        account.current_equity = Decimal(data.get('current_equity'))
        account.save()
        
        # Update metrics
        update_prop_metrics()
        
        return JsonResponse({'success': True})
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=400)


@csrf_exempt
@require_http_methods(["POST"])
def add_prop_trading_day(request, account_id):
    try:
        # Check if the request has form data or JSON
        if request.content_type and 'multipart/form-data' in request.content_type:
            # Handle form data
            account = PropFirmAccount.objects.get(id=account_id)
            
            # Create trading day
            trading_day = TradingDay.objects.create(
                account=account,
                date=datetime.strptime(request.POST.get('date'), '%Y-%m-%d').date(),
                starting_balance=Decimal(request.POST.get('starting_balance')),
                ending_balance=Decimal(request.POST.get('ending_balance')),
                pnl=Decimal(request.POST.get('pnl')),
                session_time_minutes=int(request.POST.get('session_time_minutes', 0)),
                notes=request.POST.get('notes')
            )
            
            # Handle voice memo if included
            if 'voice_memo' in request.FILES:
                trading_day.voice_memo = request.FILES['voice_memo']
                trading_day.save()
        else:
            # Handle JSON data
            data = json.loads(request.body)
            account = PropFirmAccount.objects.get(id=account_id)
            
            # Create trading day
            trading_day = TradingDay.objects.create(
                account=account,
                date=datetime.strptime(data.get('date'), '%Y-%m-%d').date(),
                starting_balance=Decimal(data.get('starting_balance')),
                ending_balance=Decimal(data.get('ending_balance')),
                pnl=Decimal(data.get('pnl')),
                session_time_minutes=int(data.get('session_time_minutes', 0)),
                notes=data.get('notes')
            )
        
        # Update account balance if it's the most recent day
        if trading_day.date == date.today():
            account.current_balance = trading_day.ending_balance
            account.current_equity = trading_day.ending_balance  # Simplified, in real app equity might differ
            account.save()
        
        # Update metrics
        update_prop_metrics()
        
        return JsonResponse({'success': True, 'trading_day_id': trading_day.id})
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=400)


@csrf_exempt
@require_http_methods(["POST"])
def add_prop_trade(request, account_id):
    try:
        data = json.loads(request.body)
        account = PropFirmAccount.objects.get(id=account_id)
        
        # Get trading day if specified
        trading_day = None
        if data.get('trading_day_id'):
            trading_day = TradingDay.objects.get(id=data.get('trading_day_id'), account=account)
        
        # Calculate PnL if exit price is provided
        pnl = None
        if data.get('exit_price'):
            price_diff = Decimal(data.get('exit_price')) - Decimal(data.get('entry_price'))
            if data.get('trade_type') == 'SELL':
                price_diff = -price_diff
            pnl = price_diff * Decimal(data.get('size'))
        
        # Create trade
        trade = PropTrade.objects.create(
            account=account,
            trading_day=trading_day,
            asset=data.get('asset'),
            trade_type=data.get('trade_type'),
            entry_price=Decimal(data.get('entry_price')),
            exit_price=Decimal(data.get('exit_price')) if data.get('exit_price') else None,
            size=Decimal(data.get('size')),
            entry_time=datetime.strptime(data.get('entry_time'), '%Y-%m-%dT%H:%M:%S'),
            exit_time=datetime.strptime(data.get('exit_time'), '%Y-%m-%dT%H:%M:%S') if data.get('exit_time') else None,
            pnl=pnl,
            strategy=data.get('strategy'),
            notes=data.get('notes')
        )
        
        # Update metrics
        update_prop_metrics()
        
        return JsonResponse({'success': True, 'trade_id': trade.id})
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=400)


@csrf_exempt
@require_http_methods(["GET"])
def get_prop_account_analytics(request, account_id):
    try:
        account = PropFirmAccount.objects.get(id=account_id)
        
        # Get trading days
        trading_days = TradingDay.objects.filter(account=account).order_by('date')
        trading_days_data = [{
            'date': day.date.isoformat(),
            'pnl': float(day.pnl),
            'balance': float(day.ending_balance)
        } for day in trading_days]
        
        # Get trades
        trades = PropTrade.objects.filter(account=account, exit_time__isnull=False)
        
        # Calculate analytics
        total_trades = trades.count()
        winning_trades = trades.filter(pnl__gt=0).count()
        win_rate = (winning_trades / total_trades * 100) if total_trades > 0 else 0
        
        avg_win = trades.filter(pnl__gt=0).aggregate(avg=models.Avg('pnl'))['avg'] or 0
        avg_loss = trades.filter(pnl__lt=0).aggregate(avg=models.Avg('pnl'))['avg'] or 0
        risk_reward = abs(float(avg_win) / float(avg_loss)) if avg_loss != 0 else 0
        
        # Group trades by strategy
        strategies = {}
        for trade in trades:
            strategy = trade.strategy or 'Uncategorized'
            if strategy not in strategies:
                strategies[strategy] = {'count': 0, 'pnl': 0}
            strategies[strategy]['count'] += 1
            strategies[strategy]['pnl'] += float(trade.pnl or 0)
        
        # Group trades by asset
        assets = {}
        for trade in trades:
            asset = trade.asset
            if asset not in assets:
                assets[asset] = {'count': 0, 'pnl': 0}
            assets[asset]['count'] += 1
            assets[asset]['pnl'] += float(trade.pnl or 0)
        
        return JsonResponse({
            'success': True,
            'trading_days': trading_days_data,
            'analytics': {
                'total_trades': total_trades,
                'winning_trades': winning_trades,
                'win_rate': win_rate,
                'risk_reward_ratio': risk_reward,
                'percentage_to_target': account.percentage_to_target(),
                'days_remaining': account.days_remaining(),
            },
            'strategies': strategies,
            'assets': assets
        })
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=400)


@csrf_exempt
@require_http_methods(["GET"])
def get_prop_metrics(request):
    try:
        metrics, created = ManagementMetrics.objects.get_or_create(id=1)
        
        return JsonResponse({
            'success': True,
            'metrics': {
                'total_accounts': metrics.total_accounts,
                'total_capital_managed': float(metrics.total_capital_managed),
                'total_profit': float(metrics.total_profit),
                'win_rate': float(metrics.win_rate),
                'avg_risk_reward': float(metrics.avg_risk_reward),
                'avg_session_time': metrics.avg_session_time,
            }
        })
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=400)


def update_prop_metrics():
    """Helper function to update aggregate metrics"""
    from django.db import models
    
    accounts = PropFirmAccount.objects.all()
    
    # Calculate metrics
    total_accounts = accounts.count()
    total_capital = accounts.aggregate(sum=models.Sum('current_balance'))['sum'] or 0
    
    # Calculate profit
    total_profit = 0
    for account in accounts:
        profit = account.current_balance - account.initial_balance
        total_profit += profit
    
    # Calculate win rate and risk/reward
    all_trades = PropTrade.objects.filter(exit_time__isnull=False)
    total_trades = all_trades.count()
    winning_trades = all_trades.filter(pnl__gt=0).count()
    win_rate = (winning_trades / total_trades * 100) if total_trades > 0 else 0
    
    avg_win = all_trades.filter(pnl__gt=0).aggregate(avg=models.Avg('pnl'))['avg'] or 0
    avg_loss = all_trades.filter(pnl__lt=0).aggregate(avg=models.Avg('pnl'))['avg'] or 0
    risk_reward = abs(float(avg_win) / float(avg_loss)) if avg_loss and avg_loss != 0 else 0
    
    # Calculate average session time
    avg_session_time = TradingDay.objects.all().aggregate(avg=models.Avg('session_time_minutes'))['avg'] or 0
    
    # Update or create metrics object
    metrics, created = ManagementMetrics.objects.get_or_create(id=1)
    metrics.total_accounts = total_accounts
    metrics.total_capital_managed = total_capital
    metrics.total_profit = total_profit
    metrics.win_rate = win_rate
    metrics.avg_risk_reward = risk_reward
    metrics.avg_session_time = avg_session_time
    metrics.save()


# Add this function to serve media files in development
def serve_prop_firm_logo(request, path):
    from django.http import FileResponse
    from django.conf import settings
    import os
    
    file_path = os.path.join(settings.MEDIA_ROOT, 'prop_firm_logos', path)
    if os.path.exists(file_path):
        return FileResponse(open(file_path, 'rb'))
    else:
        return JsonResponse({'error': 'File not found'}, status=404)


# views.py
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
import json
from .models import PropFirm, PropFirmManagementMetrics
from django.core.exceptions import ObjectDoesNotExist

@csrf_exempt
def prop_firm_list(request):
    if request.method == 'GET':
        firms = PropFirm.objects.all()
        return JsonResponse([{
            'id': firm.id,
            'name': firm.name,
            'logo': firm.logo,
            'website': firm.website
        } for firm in firms], safe=False)
    
    elif request.method == 'POST':
        try:
            data = json.loads(request.body)
            new_firm = PropFirm.objects.create(
                name=data.get('name'),
                logo=data.get('logo', ''),
                website=data.get('website', '')
            )
            return JsonResponse({
                'id': new_firm.id,
                'name': new_firm.name,
                'logo': new_firm.logo,
                'website': new_firm.website
            }, status=201)
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=400)

@csrf_exempt
def prop_firm_detail(request, firm_id):
    try:
        firm = PropFirm.objects.get(id=firm_id)
    except ObjectDoesNotExist:
        return JsonResponse({'error': 'Prop firm not found'}, status=404)
    
    if request.method == 'GET':
        return JsonResponse({
            'id': firm.id,
            'name': firm.name,
            'logo': firm.logo,
            'website': firm.website
        })
    
    elif request.method == 'PUT':
        try:
            data = json.loads(request.body)
            firm.name = data.get('name', firm.name)
            firm.logo = data.get('logo', firm.logo)
            firm.website = data.get('website', firm.website)
            firm.save()
            return JsonResponse({
                'id': firm.id,
                'name': firm.name,
                'logo': firm.logo,
                'website': firm.website
            })
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=400)
    
    elif request.method == 'DELETE':
        firm.delete()
        return JsonResponse({'message': 'Prop firm deleted successfully'})

@csrf_exempt
def metrics_list(request):
    if request.method == 'GET':
        metrics = PropFirmManagementMetrics.objects.all()
        return JsonResponse([{
            'id': metric.id,
            'prop_firm': {
                'id': metric.prop_firm.id,
                'name': metric.prop_firm.name,
                'logo': metric.prop_firm.logo
            },
            'account_type': metric.account_type,
            'status': metric.status,
            'account_id': metric.account_id,
            'starting_balance': float(metric.starting_balance),
            'current_balance': float(metric.current_balance),
            'current_equity': float(metric.current_equity),
            'profit_target': float(metric.profit_target) if metric.profit_target else None,
            'max_drawdown': float(metric.max_drawdown) if metric.max_drawdown else None,
            'start_date': metric.start_date.isoformat(),
            'notes': metric.notes
        } for metric in metrics], safe=False)
    
    elif request.method == 'POST':
        try:
            data = json.loads(request.body)
            prop_firm = PropFirm.objects.get(id=data.get('prop_firm_id'))
            
            new_metric = PropFirmManagementMetrics.objects.create(
                prop_firm=prop_firm,
                account_type=data.get('account_type'),
                status=data.get('status'),
                account_id=data.get('account_id', ''),
                starting_balance=data.get('starting_balance'),
                current_balance=data.get('current_balance'),
                current_equity=data.get('current_equity'),
                profit_target=data.get('profit_target', None),
                max_drawdown=data.get('max_drawdown', None),
                start_date=data.get('start_date'),
                notes=data.get('notes', '')
            )
            
            return JsonResponse({
                'id': new_metric.id,
                'prop_firm': {
                    'id': new_metric.prop_firm.id,
                    'name': new_metric.prop_firm.name
                },
                'account_type': new_metric.account_type,
                'status': new_metric.status
            }, status=201)
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=400)

@csrf_exempt
def metric_detail(request, metric_id):
    try:
        metric = PropFirmManagementMetrics.objects.get(id=metric_id)
    except ObjectDoesNotExist:
        return JsonResponse({'error': 'Metric not found'}, status=404)
    
    if request.method == 'GET':
        return JsonResponse({
            'id': metric.id,
            'prop_firm': {
                'id': metric.prop_firm.id,
                'name': metric.prop_firm.name,
                'logo': metric.prop_firm.logo
            },
            'account_type': metric.account_type,
            'status': metric.status,
            'account_id': metric.account_id,
            'starting_balance': float(metric.starting_balance),
            'current_balance': float(metric.current_balance),
            'current_equity': float(metric.current_equity),
            'profit_target': float(metric.profit_target) if metric.profit_target else None,
            'max_drawdown': float(metric.max_drawdown) if metric.max_drawdown else None,
            'start_date': metric.start_date.isoformat(),
            'notes': metric.notes
        })
    
    elif request.method == 'PUT':
        try:
            data = json.loads(request.body)
            
            if 'prop_firm_id' in data:
                metric.prop_firm = PropFirm.objects.get(id=data.get('prop_firm_id'))
            
            fields = ['account_type', 'status', 'account_id', 'starting_balance', 
                     'current_balance', 'current_equity', 'profit_target', 
                     'max_drawdown', 'start_date', 'notes']
            
            for field in fields:
                if field in data:
                    setattr(metric, field, data.get(field))
            
            metric.save()
            
            return JsonResponse({
                'id': metric.id,
                'prop_firm': {
                    'id': metric.prop_firm.id,
                    'name': metric.prop_firm.name
                },
                'status': metric.status,
                'current_balance': float(metric.current_balance),
                'current_equity': float(metric.current_equity)
            })
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=400)
    
    elif request.method == 'DELETE':
        metric.delete()
        return JsonResponse({'message': 'Metric deleted successfully'})


from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.shortcuts import get_object_or_404
import json
from datetime import datetime
from .models import EconomicEvent

# Helper function to serialize EconomicEvent objects to dictionaries
def event_to_dict(event):
    return {
        'id': event.id,
        'date_time': event.date_time.isoformat() if hasattr(event.date_time, 'isoformat') else event.date_time,
        'currency': event.currency,
        'impact': event.impact,
        'event_name': event.event_name,
        'actual': event.actual,
        'forecast': event.forecast,
        'previous': event.previous,
        'created_at': event.created_at.isoformat() if hasattr(event.created_at, 'isoformat') else event.created_at,
        'updated_at': event.updated_at.isoformat() if hasattr(event.updated_at, 'isoformat') else event.updated_at
    }

@csrf_exempt
def economic_events_list(request):
    """List all economic events or create a new one"""
    if request.method == 'GET':
        # Filter by date range if provided
        start_date = request.GET.get('start_date')
        end_date = request.GET.get('end_date')
        
        events = EconomicEvent.objects.all()
        
        if start_date and end_date:
            # Format: YYYY-MM-DD
            start_datetime = datetime.strptime(f"{start_date} 00:00:00", "%Y-%m-%d %H:%M:%S")
            end_datetime = datetime.strptime(f"{end_date} 23:59:59", "%Y-%m-%d %H:%M:%S")
            events = events.filter(date_time__range=(start_datetime, end_datetime))
        
        events_data = [event_to_dict(event) for event in events]
        return JsonResponse(events_data, safe=False)
    
    elif request.method == 'POST':
        try:
            data = json.loads(request.body)
            
            # Create a new event
            event = EconomicEvent(
                date_time=data['date_time'],
                currency=data['currency'],
                impact=data['impact'],
                event_name=data['event_name'],
                actual=data.get('actual'),
                forecast=data.get('forecast'),
                previous=data.get('previous')
            )
            event.save()
            
            return JsonResponse(event_to_dict(event), status=201)
        except (KeyError, json.JSONDecodeError):
            return JsonResponse({'error': 'Invalid data provided'}, status=400)
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=400)

@csrf_exempt
def economic_event_detail(request, pk):
    """Retrieve, update or delete an economic event"""
    event = get_object_or_404(EconomicEvent, pk=pk)
    
    if request.method == 'GET':
        return JsonResponse(event_to_dict(event))
    
    elif request.method == 'PUT':
        try:
            data = json.loads(request.body)
            
            # Update fields
            if 'date_time' in data:
                event.date_time = data['date_time']
            if 'currency' in data:
                event.currency = data['currency']
            if 'impact' in data:
                event.impact = data['impact']
            if 'event_name' in data:
                event.event_name = data['event_name']
            if 'actual' in data:
                event.actual = data['actual']
            if 'forecast' in data:
                event.forecast = data['forecast']
            if 'previous' in data:
                event.previous = data['previous']
            
            event.save()
            return JsonResponse(event_to_dict(event))
        except json.JSONDecodeError:
            return JsonResponse({'error': 'Invalid data provided'}, status=400)
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=400)
    
    elif request.method == 'DELETE':
        event.delete()
        return JsonResponse({}, status=204)



# views.py
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.shortcuts import get_object_or_404
import json
from .models import EconomicEvent

@csrf_exempt
@require_http_methods(["GET"])
def data_calendar_economic_events_list(request):
    # Get query parameters
    currency = request.GET.get('currency', '')
    impact = request.GET.get('impact', '')
    
    # Start with all events
    events = EconomicEvent.objects.all()
    
    # Apply filters if provided
    if currency:
        events = events.filter(currency=currency)
    
    if impact:
        events = events.filter(impact=impact)
    
    # Convert to list of dictionaries
    events_data = []
    for event in events:
        events_data.append({
            'id': event.id,
            'date_time': event.date_time.isoformat(),
            'currency': event.currency,
            'impact': event.impact,
            'event_name': event.event_name,
            'actual': event.actual,
            'forecast': event.forecast,
            'previous': event.previous,
        })
    
    return JsonResponse(events_data, safe=False)

@csrf_exempt
@require_http_methods(["GET"])
def data_calendar_economic_event_detail(request, event_id):
    event = get_object_or_404(EconomicEvent, id=event_id)
    
    event_data = {
        'id': event.id,
        'date_time': event.date_time.isoformat(),
        'currency': event.currency,
        'impact': event.impact,
        'event_name': event.event_name,
        'actual': event.actual,
        'forecast': event.forecast,
        'previous': event.previous,
        'created_at': event.created_at.isoformat(),
        'updated_at': event.updated_at.isoformat(),
    }
    
    return JsonResponse(event_data)


@csrf_exempt
@require_http_methods(["GET"])
def unique_economic_events_list(request):
    # Get query parameters
    currency = request.GET.get('currency', '')
    impact = request.GET.get('impact', '')
    search_term = request.GET.get('search', '')
    
    # Get filtered QuerySet
    query = EconomicEvent.objects.all()
    
    # Apply filters if provided
    if currency:
        query = query.filter(currency=currency)
    
    if impact:
        query = query.filter(impact=impact)
        
    if search_term:
        query = query.filter(event_name__icontains=search_term)
    
    # Get distinct event names (case insensitive)
    # Use a dictionary to track unique event names with case insensitivity
    unique_events = {}
    
    for event in query:
        # Use lowercase as the key to ensure case insensitivity
        event_key = event.event_name.lower().strip()
        
        # Only add if we haven't seen this event name before
        if event_key not in unique_events:
            unique_events[event_key] = {
                'id': event.id,
                'event_name': event.event_name,
                'currency': event.currency,
                'impact': event.impact,
            }
    
    # Convert the dictionary values to a list
    events_data = list(unique_events.values())
    
    # Sort the results by event_name
    events_data.sort(key=lambda x: x['event_name'])
    
    return JsonResponse(events_data, safe=False)
    

@csrf_exempt
@require_http_methods(["GET"])
def event_history(request, event_name):
    # Get query parameters for additional filtering
    currency = request.GET.get('currency', '')
    impact = request.GET.get('impact', '')
    
    # Initialize query to get all matching events with the same name
    query = EconomicEvent.objects.filter(event_name=event_name)
    
    # Apply additional filters if provided
    if currency:
        query = query.filter(currency=currency)
    
    if impact:
        query = query.filter(impact=impact)
    
    # Order by date_time
    query = query.order_by('date_time')
    
    # Process the data for charts
    events_data = []
    for event in query:
        # Clean the actual and forecast values for chart display
        actual_value = clean_numeric_value(event.actual)
        forecast_value = clean_numeric_value(event.forecast)
        previous_value = clean_numeric_value(event.previous)
        
        events_data.append({
            'id': event.id,
            'date': event.date_time.strftime('%Y-%m-%d'),
            'currency': event.currency,
            'impact': event.impact,
            'event_name': event.event_name,
            'actual': event.actual,  # Original string value
            'forecast': event.forecast,  # Original string value
            'previous': event.previous,  # Original string value
            'actual_value': actual_value,  # Cleaned numeric value for charts
            'forecast_value': forecast_value,  # Cleaned numeric value for charts
            'previous_value': previous_value,  # Cleaned numeric value for charts
        })
    
    return JsonResponse({
        'event_name': event_name,
        'currency': currency if currency else 'All',
        'impact': impact if impact else 'All',
        'data_points': len(events_data),
        'history': events_data
    })

def clean_numeric_value(value_str):
    """
    Convert string values like '3.2%', '$50.4B', etc. to float values
    for charting purposes
    """
    if not value_str or value_str.strip() == '':
        return None
    
    try:
        # Remove common symbols and convert to float
        cleaned = value_str.replace('%', '').replace('$', '')
        cleaned = cleaned.replace('K', '').replace('M', '').replace('B', '').replace('T', '')
        cleaned = cleaned.replace(',', '')
        return float(cleaned)
    except (ValueError, TypeError):
        return None

@csrf_exempt
def generate_econ_ai_summary(request):
    """Generate an AI summary based on COT data and economic events with improved styling."""
    try:
        if request.method != 'POST':
            return JsonResponse({'error': 'Only POST requests are allowed'}, status=405)
        
        data = json.loads(request.body)
        prompt = data.get('prompt', '')
        api_key = data.get('api_key', '')
        currency_code = data.get('currency_code', '')
        
        if not prompt:
            return JsonResponse({'error': 'Prompt is required'}, status=400)
        
        if not api_key:
            return JsonResponse({'error': 'API key is required'}, status=400)
            
        if not currency_code:
            return JsonResponse({'error': 'Currency code is required'}, status=400)
        
        # Get economic events for the currency
        economic_events = get_economic_events_for_currency(currency_code)
        
        # Append economic events data to the prompt
        prompt_with_events = prompt + "\n\n" + economic_events
        
        # Enhanced system prompt for more engaging responses
        system_prompt = """You are a financial analyst specializing in economic data and market analysis. 
        Provide concise, insightful analyses of economic data and recent economic events.
        
        Follow these style guidelines to make your response more visually appealing:
        1. Use appropriate emojis to highlight key points (1-2 emojis per section, don't overuse)
        2. Add clear section headers with emojis (e.g., " Market Positioning")
        3. Use bullet points for key takeaways
        4. Include a "Bottom Line" summary at the end
        5. Ensure the content is well-organized and easy to scan
        6. Keep the overall analysis professional but engaging
        7. Bold important terms or conclusions
        8. DO NOT USE MARKDOWN FORMATTING
        
        Sections to include:
        -  Current Positioning Analysis
        -  Economic Outlook
        -  Recent Events Impact
        -  Market Implications
        -  Bottom Line
        """
        
        # Set the API key
        openai.api_key = api_key
        
        # Generate summary using GPT-4o-mini with enhanced prompt
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt_with_events}
            ],
            max_tokens=600,  # Slightly increased to accommodate formatting
            temperature=0.7
        )
        
        summary = response.choices[0].message.content.replace('**', '')
        
        return JsonResponse({'summary': summary})
    
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)

# def get_economic_events_for_currency(currency_code):
#     """Get recent economic events for a specified currency."""
#     try:
#         # Get events from the last 30 days
#         thirty_days_ago = timezone.now() - timezone.timedelta(days=90)
#         events = EconomicEvent.objects.filter(
#             currency=currency_code,
#             date_time__gte=thirty_days_ago
#         ).order_by('-date_time')
        
#         if not events:
#             return "No recent economic events found for this currency."
        
#         # Format the events data
#         events_text = "Recent Economic Events for this currency:\n\n"
#         for event in events:
#             impact_symbol = "" if event.impact == "high" else "" if event.impact == "medium" else ""
#             events_text += f"Date: {event.date_time.strftime('%Y-%m-%d %H:%M')}\n"
#             events_text += f"Event: {event.event_name} {impact_symbol}\n"
#             events_text += f"Actual: {event.actual or 'N/A'}\n"
#             events_text += f"Forecast: {event.forecast or 'N/A'}\n"
#             events_text += f"Previous: {event.previous or 'N/A'}\n\n"
        
#         return events_text
    
#     except Exception as e:
#         return f"Error retrieving economic events: {str(e)}"


@csrf_exempt
def generate_econ_cot_data(request):
    try:
        # Get requested assets from POST data if provided
        if request.method == 'POST':
            requested_assets = json.loads(request.body).get('assets', [])
        else:
            # Default assets for GET requests
            requested_assets = [
                'USD INDEX - ICE FUTURES U.S.',
                'EURO FX - CHICAGO MERCANTILE EXCHANGE',
                'BRITISH POUND - CHICAGO MERCANTILE EXCHANGE',
                'CANADIAN DOLLAR - CHICAGO MERCANTILE EXCHANGE',
                'SWISS FRANC - CHICAGO MERCANTILE EXCHANGE',
                'JAPANESE YEN - CHICAGO MERCANTILE EXCHANGE',
                'NZ DOLLAR - CHICAGO MERCANTILE EXCHANGE',
                'AUSTRALIAN DOLLAR - CHICAGO MERCANTILE EXCHANGE',
                'GOLD - COMMODITY EXCHANGE INC.',
                'UST BOND - CHICAGO BOARD OF TRADE',
                'UST 10Y NOTE - CHICAGO BOARD OF TRADE',
                'UST 5Y NOTE - CHICAGO BOARD OF TRADE',
                'NASDAQ MINI - CHICAGO MERCANTILE EXCHANGE',
                'E-MINI S&P 500 -',
                'DOW JONES U.S. REAL ESTATE IDX - CHICAGO BOARD OF TRADE'
            ]

        # Get the current year and previous year
        current_year = pd.Timestamp.now().year
        previous_year = current_year - 1

        # Create list to store DataFrames
        df_list = []

        # Fetch data for previous and current year
        for year in range(previous_year, current_year + 1):
            single_year = cot.cot_year(year, cot_report_type='legacy_futopt')
            df_list.append(single_year)

        # Concatenate all DataFrames
        df = pd.concat(df_list, ignore_index=True)

        # Convert dates to datetime
        df['As of Date in Form YYYY-MM-DD'] = pd.to_datetime(df['As of Date in Form YYYY-MM-DD'])

        # Filter for current year data
        unfiltered_currency_df = df[df['As of Date in Form YYYY-MM-DD'].dt.year == current_year]

        # Filter for requested assets
        unfiltered_currency_df = unfiltered_currency_df[
            unfiltered_currency_df['Market and Exchange Names'].isin(requested_assets)
        ]

        # Remove specific exclusions (e.g., MICRO GOLD)
        unfiltered_currency_df = unfiltered_currency_df[
            unfiltered_currency_df['Market and Exchange Names'] != 'MICRO GOLD - COMMODITY EXCHANGE INC.'
        ]

        # Fill missing values and ensure numeric columns
        numeric_columns = [
            'Noncommercial Positions-Long (All)',
            'Noncommercial Positions-Short (All)',
            'Commercial Positions-Long (All)',
            'Commercial Positions-Short (All)'
        ]
        
        unfiltered_currency_df[numeric_columns] = unfiltered_currency_df[numeric_columns].fillna(0).astype(float)

        # Calculate net positions for unfiltered data
        unfiltered_currency_df['Net Noncommercial Positions'] = (
            unfiltered_currency_df['Noncommercial Positions-Long (All)'] - 
            unfiltered_currency_df['Noncommercial Positions-Short (All)']
        )
        unfiltered_currency_df['Net Commercial Positions'] = (
            unfiltered_currency_df['Commercial Positions-Long (All)'] - 
            unfiltered_currency_df['Commercial Positions-Short (All)']
        )

        # Get the rows with maximum open interest for each market
        idx = unfiltered_currency_df.groupby('Market and Exchange Names')['Open Interest (All)'].idxmax()
        currency_df = unfiltered_currency_df.loc[idx]

        # Calculate total positions
        currency_df['Total Noncommercial Positions'] = (
            currency_df['Noncommercial Positions-Long (All)'] + 
            currency_df['Noncommercial Positions-Short (All)']
        )
        currency_df['Total Commercial Positions'] = (
            currency_df['Commercial Positions-Long (All)'] + 
            currency_df['Commercial Positions-Short (All)']
        )
        currency_df['Total Positions'] = (
            currency_df['Total Noncommercial Positions'] + 
            currency_df['Total Commercial Positions']
        )

        # Calculate percentages
        currency_df['Percentage Noncommercial Long'] = (
            currency_df['Noncommercial Positions-Long (All)'] / 
            currency_df['Total Noncommercial Positions']
        ) * 100
        currency_df['Percentage Noncommercial Short'] = (
            currency_df['Noncommercial Positions-Short (All)'] / 
            currency_df['Total Noncommercial Positions']
        ) * 100
        currency_df['Percentage Commercial Long'] = (
            currency_df['Commercial Positions-Long (All)'] / 
            currency_df['Total Commercial Positions']
        ) * 100
        currency_df['Percentage Commercial Short'] = (
            currency_df['Commercial Positions-Short (All)'] / 
            currency_df['Total Commercial Positions']
        ) * 100

        # Generate plots
        plot_urls = plot_net_positions(unfiltered_currency_df)

        # Prepare response data
        data = {}
        round_off_number = 2

        for asset in requested_assets:
            asset_df = currency_df[currency_df['Market and Exchange Names'] == asset]
            
            if not asset_df.empty:
                latest_data = asset_df.iloc[0]
                data[asset] = {
                    'Date': latest_data['As of Date in Form YYYY-MM-DD'].strftime('%Y-%m-%d'),
                    'Percentage_Noncommercial_Long': round(latest_data['Percentage Noncommercial Long'], round_off_number),
                    'Percentage_Noncommercial_Short': round(latest_data['Percentage Noncommercial Short'], round_off_number),
                    'Percentage_Commercial_Long': round(latest_data['Percentage Commercial Long'], round_off_number),
                    'Percentage_Commercial_Short': round(latest_data['Percentage Commercial Short'], round_off_number),
                    'Plot_URL': plot_urls.get(asset, '')
                }

        return JsonResponse(data)
    
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)

logger = logging.getLogger(__name__)

# @csrf_exempt
# @require_http_methods(["POST"])
# def save_forex_factory_news(request):
#     try:
#         # Parse JSON data from request body
#         data = json.loads(request.body)
#         events_data = data.get('events', [])
        
#         if not events_data:
#             return JsonResponse({
#                 'success': False,
#                 'error': 'No events data provided'
#             }, status=400)
        
#         saved_events = []
#         errors = []
        
#         for i, event_data in enumerate(events_data):
#             try:
#                 # Validate required fields
#                 required_fields = ['date_time', 'currency', 'impact', 'event_name']
#                 missing_fields = [field for field in required_fields if not event_data.get(field)]
                
#                 if missing_fields:
#                     errors.append(f"Event {i+1}: Missing required fields: {', '.join(missing_fields)}")
#                     continue
                
#                 # Parse and validate datetime
#                 date_time_str = event_data.get('date_time')
#                 try:
#                     parsed_datetime = parse_datetime(date_time_str)
#                     if not parsed_datetime:
#                         # Try alternative format if the first parsing fails
#                         from datetime import datetime
#                         parsed_datetime = datetime.fromisoformat(date_time_str.replace('Z', '+00:00'))
#                 except (ValueError, TypeError) as e:
#                     errors.append(f"Event {i+1}: Invalid datetime format: {date_time_str}")
#                     continue
                
#                 # Validate currency
#                 valid_currencies = ['USD', 'EUR', 'GBP', 'JPY', 'AUD', 'CAD', 'CHF', 'CNY']
#                 currency = event_data.get('currency', '').upper()
#                 if currency not in valid_currencies:
#                     errors.append(f"Event {i+1}: Invalid currency: {currency}")
#                     continue
                
#                 # Validate impact
#                 valid_impacts = ['low', 'medium', 'high']
#                 impact = event_data.get('impact', '').lower()
#                 if impact not in valid_impacts:
#                     errors.append(f"Event {i+1}: Invalid impact level: {impact}")
#                     continue
                
#                 # Prepare event data for saving
#                 event_kwargs = {
#                     'date_time': parsed_datetime,
#                     'currency': currency,
#                     'impact': impact,
#                     'event_name': event_data.get('event_name', '').strip(),
#                     'actual': event_data.get('actual') or None,
#                     'forecast': event_data.get('forecast') or None,
#                     'previous': event_data.get('previous') or None,
#                 }
                
#                 # Check for duplicates (same datetime, currency, and event_name)
#                 existing_event = EconomicEvent.objects.filter(
#                     date_time=parsed_datetime,
#                     currency=currency,
#                     event_name=event_kwargs['event_name']
#                 ).first()
                
#                 if existing_event:
#                     # Update existing event
#                     for key, value in event_kwargs.items():
#                         setattr(existing_event, key, value)
#                     existing_event.save()
#                     saved_events.append({
#                         'id': existing_event.id,
#                         'action': 'updated',
#                         'event_name': existing_event.event_name
#                     })
#                     logger.info(f"Updated existing event: {existing_event}")
#                 else:
#                     # Create new event
#                     new_event = EconomicEvent.objects.create(**event_kwargs)
#                     saved_events.append({
#                         'id': new_event.id,
#                         'action': 'created',
#                         'event_name': new_event.event_name
#                     })
#                     logger.info(f"Created new event: {new_event}")
                    
#             except Exception as e:
#                 logger.error(f"Error processing event {i+1}: {str(e)}")
#                 errors.append(f"Event {i+1}: {str(e)}")
#                 continue
        
#         # Prepare response
#         response_data = {
#             'success': True,
#             'saved_count': len(saved_events),
#             'error_count': len(errors),
#             'saved_events': saved_events
#         }
        
#         if errors:
#             response_data['errors'] = errors
#             response_data['message'] = f"Saved {len(saved_events)} events with {len(errors)} errors"
#         else:
#             response_data['message'] = f"Successfully saved {len(saved_events)} events"
        
#         # Return appropriate status code
#         status_code = 200 if saved_events else 400
        
#         return JsonResponse(response_data, status=status_code)
        
#     except json.JSONDecodeError:
#         logger.error("Invalid JSON data received")
#         return JsonResponse({
#             'success': False,
#             'error': 'Invalid JSON data'
#         }, status=400)
        
#     except Exception as e:
#         logger.error(f"Unexpected error in save_forex_factory_news: {str(e)}")
#         return JsonResponse({
#             'success': False,
#             'error': 'Internal server error'
#         }, status=500)


# Optional: Add a view to retrieve saved events for verification
# @csrf_exempt
# @require_http_methods(["GET"])
# def get_forex_factory_events(request):
#     """
#     Retrieve economic events with optional filtering
#     """
#     try:
#         # Get query parameters for filtering
#         currency = request.GET.get('currency')
#         impact = request.GET.get('impact')
#         start_date = request.GET.get('start_date')
#         end_date = request.GET.get('end_date')
#         limit = int(request.GET.get('limit', 100))  # Default limit of 100
        
#         # Start with all events
#         queryset = EconomicEvent.objects.all()
        
#         # Apply filters
#         if currency:
#             queryset = queryset.filter(currency=currency.upper())
        
#         if impact:
#             queryset = queryset.filter(impact=impact.lower())
            
#         if start_date:
#             try:
#                 start_dt = parse_datetime(start_date)
#                 if start_dt:
#                     queryset = queryset.filter(date_time__gte=start_dt)
#             except ValueError:
#                 pass
                
#         if end_date:
#             try:
#                 end_dt = parse_datetime(end_date)
#                 if end_dt:
#                     queryset = queryset.filter(date_time__lte=end_dt)
#             except ValueError:
#                 pass
        
#         # Limit results and order by date
#         events = queryset.order_by('-date_time')[:limit]
        
#         # Serialize events
#         events_data = []
#         for event in events:
#             events_data.append({
#                 'id': event.id,
#                 'date_time': event.date_time.isoformat(),
#                 'currency': event.currency,
#                 'impact': event.impact,
#                 'event_name': event.event_name,
#                 'actual': event.actual,
#                 'forecast': event.forecast,
#                 'previous': event.previous,
#                 'created_at': event.created_at.isoformat(),
#                 'updated_at': event.updated_at.isoformat(),
#             })
        
#         return JsonResponse({
#             'success': True,
#             'events': events_data,
#             'count': len(events_data)
#         })
        
#     except Exception as e:
#         logger.error(f"Error retrieving events: {str(e)}")
#         return JsonResponse({
#             'success': False,
#             'error': 'Failed to retrieve events'
#         }, status=500)


# @csrf_exempt
# @require_http_methods(["POST"])
# def process_forex_screenshot(request):
#     """
#     Process Forex Factory screenshot using OpenAI GPT-4o-mini
#     """
#     try:
#         data = json.loads(request.body)
#         image_base64 = data.get('image')
#         api_key = data.get('api_key')
        
#         if not image_base64 or not api_key:
#             return JsonResponse({
#                 'error': 'Missing image or API key'
#             }, status=400)
        
#         # Initialize OpenAI client with the new syntax
#         client = OpenAI(api_key=api_key)
        
#         # Prepare the prompt for GPT-4o-mini
#         prompt = """
#         Analyze this Forex Factory economic calendar screenshot and extract all economic events.
#         Return a JSON response with an 'events' array containing objects with these fields:
#         - date_time: ISO format datetime (YYYY-MM-DDTHH:MM)
#         - currency: 3-letter currency code (USD, EUR, GBP, JPY, AUD, CAD, CHF, CNY)
#         - impact: 'low', 'medium', or 'high'
#         - event_name: Name of the economic event
#         - actual: Actual value (can be empty string if not available)
#         - forecast: Forecasted value (can be empty string if not available)  
#         - previous: Previous value (can be empty string if not available)
        
#         Example response format:
#         {
#           "events": [
#             {
#               "date_time": "2024-12-01T10:00",
#               "currency": "USD",
#               "impact": "high",
#               "event_name": "Non-Farm Payrolls",
#               "actual": "227K",
#               "forecast": "220K",
#               "previous": "12K"
#             }
#           ]
#         }
        
#         Extract ALL visible events from the screenshot. Pay attention to:
#         - Time stamps and convert to 24-hour format
#         - Currency flags/symbols
#         - Impact levels (usually shown as colored indicators)
#         - Event names
#         - Actual, forecast, and previous values
        
#         Return only valid JSON without any additional text or formatting.
#         """
        
#         # Make API call to OpenAI using the new client syntax
#         response = client.chat.completions.create(
#             model="gpt-4o-mini",
#             messages=[
#                 {
#                     "role": "user",
#                     "content": [
#                         {
#                             "type": "text",
#                             "text": prompt
#                         },
#                         {
#                             "type": "image_url",
#                             "image_url": {
#                                 "url": f"data:image/jpeg;base64,{image_base64}"
#                             }
#                         }
#                     ]
#                 }
#             ],
#             max_tokens=2000,
#             temperature=0.1
#         )
        
#         # Extract and parse the response
#         gpt_response = response.choices[0].message.content.strip()
        
#         # Try to parse JSON response
#         try:
#             parsed_data = json.loads(gpt_response)
#             events = parsed_data.get('events', [])
            
#             # Validate and clean the events data
#             cleaned_events = []
#             for event in events:
#                 try:
#                     # Validate required fields
#                     if not all(field in event for field in ['date_time', 'currency', 'impact', 'event_name']):
#                         continue
                    
#                     # Validate currency
#                     valid_currencies = ['USD', 'EUR', 'GBP', 'JPY', 'AUD', 'CAD', 'CHF', 'CNY']
#                     if event['currency'] not in valid_currencies:
#                         event['currency'] = 'USD'  # Default fallback
                    
#                     # Validate impact
#                     valid_impacts = ['low', 'medium', 'high']
#                     if event['impact'] not in valid_impacts:
#                         event['impact'] = 'medium'  # Default fallback
                    
#                     # Ensure optional fields exist
#                     event['actual'] = event.get('actual', '')
#                     event['forecast'] = event.get('forecast', '')
#                     event['previous'] = event.get('previous', '')
                    
#                     # Validate datetime format
#                     try:
#                         datetime.fromisoformat(event['date_time'].replace('Z', '+00:00'))
#                     except ValueError:
#                         # If invalid datetime, skip this event
#                         continue
                    
#                     cleaned_events.append(event)
                    
#                 except Exception as e:
#                     # Skip invalid events
#                     continue
            
#             return JsonResponse({
#                 'success': True,
#                 'events': cleaned_events,
#                 'message': f'Successfully extracted {len(cleaned_events)} events'
#             })
            
#         except json.JSONDecodeError:
#             return JsonResponse({
#                 'error': 'Failed to parse GPT response as JSON',
#                 'gpt_response': gpt_response
#             }, status=500)
            
#     except Exception as e:
#         # More specific error handling
#         print(f'Error occured in process_fx function: {e}')
#         if "OpenAI" in str(type(e)):
#             return JsonResponse({
#                 'error': f'OpenAI API error: {str(e)}'
#             }, status=500)
#         else:
#             return JsonResponse({
#                 'error': f'Server error: {str(e)}'
#             }, status=500)



logger = logging.getLogger(__name__)

@csrf_exempt
@require_http_methods(["POST"])
def save_forex_factory_news(request):
    """
    Save economic events from forex factory screenshot analysis
    Expected JSON format:
    {
        "events": [
            {
                "date_time": "2024-12-01T14:30:00",
                "currency": "USD",
                "impact": "high",
                "event_name": "Non-Farm Payrolls",
                "actual": "150K",
                "forecast": "160K",
                "previous": "140K"
            }
        ]
    }
    """
    try:
        # Parse JSON data
        data = json.loads(request.body)
        events_data = data.get('events', [])
        
        if not events_data:
            return JsonResponse({
                'success': False,
                'error': 'No events provided'
            }, status=400)
        
        # Validate and save events
        saved_events = []
        errors = []
        
        for i, event_data in enumerate(events_data):
            try:
                # Validate required fields
                required_fields = ['date_time', 'currency', 'impact', 'event_name']
                for field in required_fields:
                    if not event_data.get(field):
                        raise ValidationError(f"Missing required field: {field}")
                
                # Parse datetime
                try:
                    if isinstance(event_data['date_time'], str):
                        # Handle different datetime formats
                        dt_str = event_data['date_time']
                        if 'T' in dt_str:
                            if dt_str.endswith('Z'):
                                dt = datetime.fromisoformat(dt_str.replace('Z', '+00:00'))
                            else:
                                dt = datetime.fromisoformat(dt_str)
                        else:
                            dt = datetime.strptime(dt_str, '%Y-%m-%d %H:%M:%S')
                    else:
                        dt = event_data['date_time']
                except (ValueError, TypeError) as e:
                    raise ValidationError(f"Invalid date_time format: {e}")
                
                # Validate currency
                valid_currencies = ['USD', 'EUR', 'GBP', 'JPY', 'AUD', 'CAD', 'CHF', 'CNY']
                if event_data['currency'] not in valid_currencies:
                    raise ValidationError(f"Invalid currency: {event_data['currency']}")
                
                # Validate impact
                valid_impacts = ['low', 'medium', 'high']
                if event_data['impact'] not in valid_impacts:
                    raise ValidationError(f"Invalid impact: {event_data['impact']}")
                
                # Create or update event
                # Check if event already exists (same datetime, currency, event_name)
                existing_event = EconomicEvent.objects.filter(
                    date_time=dt,
                    currency=event_data['currency'],
                    event_name=event_data['event_name']
                ).first()
                
                if existing_event:
                    # Update existing event
                    existing_event.impact = event_data['impact']
                    existing_event.actual = event_data.get('actual', '') or ''
                    existing_event.forecast = event_data.get('forecast', '') or ''
                    existing_event.previous = event_data.get('previous', '') or ''
                    existing_event.save()
                    saved_events.append({
                        'id': existing_event.id,
                        'action': 'updated',
                        'event_name': existing_event.event_name
                    })
                else:
                    # Create new event
                    new_event = EconomicEvent.objects.create(
                        date_time=dt,
                        currency=event_data['currency'],
                        impact=event_data['impact'],
                        event_name=event_data['event_name'],
                        actual=event_data.get('actual', '') or '',
                        forecast=event_data.get('forecast', '') or '',
                        previous=event_data.get('previous', '') or ''
                    )
                    saved_events.append({
                        'id': new_event.id,
                        'action': 'created',
                        'event_name': new_event.event_name
                    })
                
            except ValidationError as e:
                errors.append(f"Event {i+1}: {str(e)}")
                logger.error(f"Validation error for event {i+1}: {e}")
            except Exception as e:
                errors.append(f"Event {i+1}: Unexpected error - {str(e)}")
                logger.error(f"Unexpected error for event {i+1}: {e}")
        
        # Prepare response
        response_data = {
            'success': True,
            'saved_count': len(saved_events),
            'total_count': len(events_data),
            'saved_events': saved_events
        }
        
        if errors:
            response_data['errors'] = errors
            response_data['error_count'] = len(errors)
        
        status_code = 200 if saved_events else 400
        return JsonResponse(response_data, status=status_code)
        
    except json.JSONDecodeError:
        return JsonResponse({
            'success': False,
            'error': 'Invalid JSON format'
        }, status=400)
    
    except Exception as e:
        logger.error(f"Unexpected error in save_forex_factory_news: {e}")
        return JsonResponse({
            'success': False,
            'error': 'Internal server error'
        }, status=500)


# views.py
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from .models import Account, AccountTrades
import json
from django.core.serializers import serialize
from django.forms.models import model_to_dict

@csrf_exempt
@require_http_methods(["GET"])
def get_trades_by_account_calendar(request):
    """
    Get all trades for a specific account
    """
    try:
        account_name = request.GET.get('account_name')
        
        if not account_name:
            return JsonResponse({
                'error': 'account_name parameter is required'
            }, status=400)
        
        # Get the account
        try:
            account = Account.objects.get(account_name=account_name)
        except Account.DoesNotExist:
            return JsonResponse({
                'error': f'Account with name "{account_name}" not found'
            }, status=404)
        
        # Get all trades for this account
        trades = AccountTrades.objects.filter(account=account).order_by('-date_entered')
        
        # Convert trades to list of dictionaries
        trades_data = []
        for trade in trades:
            trade_dict = {
                'id': trade.id,
                'account_name': trade.account.account_name,
                'asset': trade.asset,
                'order_type': trade.order_type,
                'strategy': trade.strategy,
                'day_of_week_entered': trade.day_of_week_entered,
                'day_of_week_closed': trade.day_of_week_closed,
                'trading_session_entered': trade.trading_session_entered,
                'trading_session_closed': trade.trading_session_closed,
                'outcome': trade.outcome,
                'amount': trade.amount,
                'emotional_bias': trade.emotional_bias,
                'reflection': trade.reflection,
                'date_entered': trade.date_entered.isoformat() if trade.date_entered else None,
            }
            trades_data.append(trade_dict)
        
        return JsonResponse(trades_data, safe=False)
        
    except Exception as e:
        return JsonResponse({
            'error': str(e)
        }, status=500)

@csrf_exempt
@require_http_methods(["GET"])
def get_trades_by_date_range_calendar(request):
    """
    Get trades for a specific account within a date range
    """
    try:
        account_name = request.GET.get('account_name')
        start_date = request.GET.get('start_date')  # Format: YYYY-MM-DD
        end_date = request.GET.get('end_date')      # Format: YYYY-MM-DD
        
        if not account_name:
            return JsonResponse({
                'error': 'account_name parameter is required'
            }, status=400)
        
        # Get the account
        try:
            account = Account.objects.get(account_name=account_name)
        except Account.DoesNotExist:
            return JsonResponse({
                'error': f'Account with name "{account_name}" not found'
            }, status=404)
        
        # Start with base query
        trades_query = AccountTrades.objects.filter(account=account)
        
        # Add date filters if provided
        if start_date:
            trades_query = trades_query.filter(date_entered__date__gte=start_date)
        if end_date:
            trades_query = trades_query.filter(date_entered__date__lte=end_date)
        
        trades = trades_query.order_by('-date_entered')
        
        # Convert trades to list of dictionaries
        trades_data = []
        for trade in trades:
            trade_dict = {
                'id': trade.id,
                'account_name': trade.account.account_name,
                'asset': trade.asset,
                'order_type': trade.order_type,
                'strategy': trade.strategy,
                'day_of_week_entered': trade.day_of_week_entered,
                'day_of_week_closed': trade.day_of_week_closed,
                'trading_session_entered': trade.trading_session_entered,
                'trading_session_closed': trade.trading_session_closed,
                'outcome': trade.outcome,
                'amount': trade.amount,
                'emotional_bias': trade.emotional_bias,
                'reflection': trade.reflection,
                'date_entered': trade.date_entered.isoformat() if trade.date_entered else None,
            }
            trades_data.append(trade_dict)
        
        return JsonResponse(trades_data, safe=False)
        
    except Exception as e:
        return JsonResponse({
            'error': str(e)
        }, status=500)

@csrf_exempt
@require_http_methods(["GET"])
def get_trade_summary_calendar(request):
    """
    Get summary statistics for trades by account
    """
    try:
        account_name = request.GET.get('account_name')
        
        if not account_name:
            return JsonResponse({
                'error': 'account_name parameter is required'
            }, status=400)
        
        # Get the account
        try:
            account = Account.objects.get(account_name=account_name)
        except Account.DoesNotExist:
            return JsonResponse({
                'error': f'Account with name "{account_name}" not found'
            }, status=404)
        
        # Get all trades for this account
        trades = AccountTrades.objects.filter(account=account)
        
        # Calculate summary statistics
        total_trades = trades.count()
        profitable_trades = trades.filter(outcome='Profit').count()
        losing_trades = trades.filter(outcome='Loss').count()
        
        total_profit = sum(trade.amount for trade in trades.filter(outcome='Profit'))
        total_loss = sum(abs(trade.amount) for trade in trades.filter(outcome='Loss'))
        net_profit = total_profit - total_loss
        
        win_rate = (profitable_trades / total_trades * 100) if total_trades > 0 else 0
        
        summary = {
            'account_name': account_name,
            'total_trades': total_trades,
            'profitable_trades': profitable_trades,
            'losing_trades': losing_trades,
            'total_profit': total_profit,
            'total_loss': total_loss,
            'net_profit': net_profit,
            'win_rate': round(win_rate, 2)
        }
        
        return JsonResponse(summary)
        
    except Exception as e:
        return JsonResponse({
            'error': str(e)
        }, status=500)

@csrf_exempt
@require_http_methods(["POST"])
def create_trade_calendar(request):
    """
    Create a new trade
    """
    try:
        data = json.loads(request.body)
        
        account_name = data.get('account_name')
        if not account_name:
            return JsonResponse({
                'error': 'account_name is required'
            }, status=400)
        
        # Get the account
        try:
            account = Account.objects.get(account_name=account_name)
        except Account.DoesNotExist:
            return JsonResponse({
                'error': f'Account with name "{account_name}" not found'
            }, status=404)
        
        # Create the trade
        trade = AccountTrades.objects.create(
            account=account,
            asset=data.get('asset', ''),
            order_type=data.get('order_type', ''),
            strategy=data.get('strategy', ''),
            day_of_week_entered=data.get('day_of_week_entered', ''),
            day_of_week_closed=data.get('day_of_week_closed'),
            trading_session_entered=data.get('trading_session_entered', ''),
            trading_session_closed=data.get('trading_session_closed'),
            outcome=data.get('outcome', ''),
            amount=float(data.get('amount', 0)),
            emotional_bias=data.get('emotional_bias'),
            reflection=data.get('reflection'),
            date_entered=data.get('date_entered')
        )
        
        trade_dict = {
            'id': trade.id,
            'account_name': trade.account.account_name,
            'asset': trade.asset,
            'order_type': trade.order_type,
            'strategy': trade.strategy,
            'day_of_week_entered': trade.day_of_week_entered,
            'day_of_week_closed': trade.day_of_week_closed,
            'trading_session_entered': trade.trading_session_entered,
            'trading_session_closed': trade.trading_session_closed,
            'outcome': trade.outcome,
            'amount': trade.amount,
            'emotional_bias': trade.emotional_bias,
            'reflection': trade.reflection,
            'date_entered': trade.date_entered.isoformat() if trade.date_entered else None,
        }
        
        return JsonResponse({
            'message': 'Trade created successfully',
            'trade': trade_dict
        }, status=201)
        
    except json.JSONDecodeError:
        return JsonResponse({
            'error': 'Invalid JSON data'
        }, status=400)
    except Exception as e:
        return JsonResponse({
            'error': str(e)
        }, status=500)


# In your Django view, make sure the POST handler includes category:

@csrf_exempt
@require_http_methods(["GET", "POST"])
def paper_gpt(request):
    if request.method == 'GET':
        papers = PaperGPT.objects.all()
        papers_data = []
        
        for paper in papers:
            papers_data.append({
                'id': paper.id,
                'title': paper.title,
                'fileName': paper.file_name,
                'fileSize': paper.file_size,
                'uploadDate': paper.upload_date.isoformat(),
                'aiSummary': paper.ai_summary,
                'category': paper.category,  # Make sure this field exists
                'personalNotes': paper.personal_notes,
                'extractedText': paper.extracted_text,
                'fileData': paper.file_data,
            })
        
        return JsonResponse(papers_data, safe=False)
    
    elif request.method == 'POST':
        try:
            data = json.loads(request.body)
            
            # Add validation and debugging
            print("Received data:", data.keys())  # Debug line
            
            paper = PaperGPT.objects.create(
                title=data.get('title', ''),
                file_name=data.get('fileName', ''),
                file_data=data.get('fileData', ''),
                file_size=data.get('fileSize', 0),
                category=data.get('category', ''),  # Handle missing category gracefully
                extracted_text=data.get('extractedText', ''),
                ai_summary=data.get('aiSummary', ''),
                personal_notes=data.get('personalNotes', '')
            )
            return JsonResponse({
                'id': paper.id,
                'title': paper.title,
                'message': 'Paper saved successfully'
            })
        except KeyError as e:
            return JsonResponse({'error': f'Missing required field: {str(e)}'}, status=400)
        except Exception as e:
            print("Error saving paper:", str(e))  # Debug line
            return JsonResponse({'error': str(e)}, status=400)
            

@csrf_exempt
@require_http_methods(["PUT", "DELETE"])
def paper_detail(request, paper_id):
    try:
        paper = PaperGPT.objects.get(id=paper_id)
        
        if request.method == 'PUT':
            data = json.loads(request.body)
            paper.personal_notes = data.get('personalNotes', paper.personal_notes)
            paper.category = data.get('category', paper.category)
            paper.save()
            return JsonResponse({'message': 'Paper updated successfully'})
            
        elif request.method == 'DELETE':
            paper.delete()
            return JsonResponse({'message': 'Paper deleted successfully'})
            
    except PaperGPT.DoesNotExist:
        return JsonResponse({'error': 'Paper not found'}, status=404)
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=400)

@csrf_exempt
@require_http_methods(["GET"])
def get_categories(request):
    """Get all unique categories"""
    categories = PaperGPT.objects.values_list('category', flat=True).distinct()
    categories = [cat for cat in categories if cat]  # Remove empty categories
    return JsonResponse(list(categories), safe=False)

@csrf_exempt
def extract_pdf_text(request):
   """Extract text from uploaded PDF"""
   if request.method == 'POST':
       try:
           import PyPDF2
           import io
           
           # Get the uploaded file
           pdf_file = request.FILES.get('pdf')
           if not pdf_file:
               return JsonResponse({'error': 'No PDF file provided'}, status=400)
           
           # Extract text using PyPDF2
           pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file.read()))
           extracted_text = ""
           
           for page_num in range(len(pdf_reader.pages)):
               page = pdf_reader.pages[page_num]
               extracted_text += page.extract_text()
           
           return JsonResponse({'extracted_text': extracted_text})
           
       except Exception as e:
           return JsonResponse({'error': f'Error extracting text: {str(e)}'}, status=400)

@csrf_exempt
def generate_paper_summary(request):
   """Generate AI summary using OpenAI"""
   if request.method == 'POST':
       try:
           data = json.loads(request.body)
           text = data.get('text', '')
           
           if not text:
               return JsonResponse({'error': 'No text provided'}, status=400)
           
           # Get OpenAI API key from settings or environment
           client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
           
           response = client.chat.completions.create(
               model="gpt-4o-mini",
               messages=[
                   {
                       "role": "system", 
                       "content": "You are an academic research assistant. Provide concise, insightful summaries of research papers highlighting key findings, methodology, and implications."
                   },
                   {
                       "role": "user", 
                       "content": f"Please summarize this research paper: {text[:4000]}"  # Limit text length
                   }
               ],
               max_tokens=500,
               temperature=0.7
           )
           
           summary = response.choices[0].message.content
           return JsonResponse({'summary': summary})
           
       except Exception as e:
           return JsonResponse({'error': f'Error generating summary: {str(e)}'}, status=400)


@csrf_exempt
@require_http_methods(["GET"])
def retrieve_economic_data_for_selected_currency(request, currency_code):
    """
    Retrieve economic data for a specific currency to be used by EconomicsGPT
    """
    try:
        # Get recent economic events for the selected currency (last 30 days)
        thirty_days_ago = datetime.now() - timedelta(days=30)
        
        economic_events = EconomicEvent.objects.filter(
            currency=currency_code.upper(),
            date_time__gte=thirty_days_ago
        ).order_by('-date_time')
        
        # Format the data for the AI model
        formatted_data = []
        for event in economic_events:
            formatted_data.append({
                'date_time': event.date_time.strftime('%Y-%m-%d %H:%M'),
                'currency': event.currency,
                'impact': event.impact,
                'event_name': event.event_name,
                'actual': event.actual,
                'forecast': event.forecast,
                'previous': event.previous,
                'impact_level': event.impact,
            })
        
        # Add summary statistics
        total_events = len(formatted_data)
        high_impact_events = len([e for e in formatted_data if e['impact'] == 'high'])
        medium_impact_events = len([e for e in formatted_data if e['impact'] == 'medium'])
        low_impact_events = len([e for e in formatted_data if e['impact'] == 'low'])
        
        response_data = {
            'currency': currency_code.upper(),
            'data_period': '30 days',
            'total_events': total_events,
            'impact_summary': {
                'high_impact': high_impact_events,
                'medium_impact': medium_impact_events,
                'low_impact': low_impact_events
            },
            'economic_events': formatted_data,
            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        return JsonResponse(response_data, safe=False)
        
    except Exception as e:
        return JsonResponse({
            'error': f'Failed to retrieve economic data: {str(e)}',
            'currency': currency_code.upper()
        }, status=500)


import matplotlib.dates as mdates
from io import BytesIO
@csrf_exempt
def generate_dynamic_chart(request):
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            currency_pair = data.get('currency_pair')
            timeframe = data.get('timeframe')
            lookback = data.get('lookback')
            
            # Convert lookback to number of days
            lookback_days_map = {
                '1d': 1,
                '7d': 7,
                '30d': 30,
                '90d': 90,
                '1y': 365
            }
            
            # Use your existing function to get the dataset
            hist = obtain_dataset(currency_pair, timeframe, lookback_days_map[lookback])
            
            # Check if data is available
            if hist.empty:
                return JsonResponse({
                    'success': False,
                    'error': f'No data available for {currency_pair}'
                }, status=400)
            
            # Create candlestick chart using your existing function logic
            fig, ax = plt.subplots(figsize=(12, 8))
            
            # Clean up the data
            hist = hist[['Open', 'High', 'Low', 'Close']]
            
            for idx, row in enumerate(hist.itertuples(index=False)):
                # Access the values by position
                open_price = row[0]
                high_price = row[1]
                low_price = row[2]
                close_price = row[3]
                
                # Determine the color of the candlestick
                color = '#10b981' if close_price > open_price else '#ef4444'
                
                # Draw the candlestick body (rectangle)
                body = Rectangle(
                    (idx - 0.4, min(open_price, close_price)),  # Bottom-left corner
                    0.8,  # Width
                    abs(close_price - open_price),  # Height
                    color=color
                )
                ax.add_patch(body)
                
                # Draw the wick (high-low line)
                ax.plot(
                    [idx, idx],  # X-coordinates
                    [low_price, high_price],  # Y-coordinates
                    color=color,
                    linewidth=1
                )
            
            # Set labels and title
            ax.set_title(f'{currency_pair} Candlestick Chart - {timeframe} timeframe, {lookback} lookback', 
                        fontsize=16, fontweight='bold', color='#1e40af')
            ax.set_xlabel('Time', fontsize=12)
            ax.set_ylabel('Price', fontsize=12)
            ax.grid(True, alpha=0.3)
            
            # Format x-axis with actual dates (show fewer labels to avoid crowding)
            step = max(1, len(hist) // 10)  # Show max 10 labels
            ax.set_xticks(range(0, len(hist), step))
            ax.set_xticklabels([hist.index[i].strftime('%Y-%m-%d %H:%M') for i in range(0, len(hist), step)], 
                             rotation=45)
            
            plt.tight_layout()
            
            # Convert to base64
            buffer = BytesIO()
            plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
            buffer.seek(0)
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
            plt.close()
            
            return JsonResponse({
                'success': True,
                'chart_image': image_base64,
                'currency_pair': currency_pair,
                'timeframe': timeframe,
                'lookback': lookback
            })
            
        except Exception as e:
            return JsonResponse({
                'success': False,
                'error': str(e)
            }, status=400)
    
    return JsonResponse({'error': 'Method not allowed'}, status=405)
           

import uuid
import random
from datetime import datetime, timedelta
from django.utils import timezone
from django.views.decorators.csrf import csrf_exempt
from django.http import JsonResponse
import json

def get_economic_events_for_pair_two(currency_pair):
    """Get relevant economic events for a currency pair"""
    try:
        # Extract currencies from pair (e.g., EURUSD -> EUR, USD)
        base_currency = currency_pair[:3]
        quote_currency = currency_pair[3:6]
        
        # Get events for the next 7 days for both currencies
        start_date = timezone.now()
        end_date = start_date + timedelta(days=7)
        
        events = EconomicEvent.objects.filter(
            currency__in=[base_currency, quote_currency],
            date_time__gte=start_date,
            date_time__lte=end_date
        ).order_by('date_time')[:5]
        
        return [
            {
                'date_time': event.date_time.isoformat(),
                'currency': event.currency,
                'impact': event.impact,
                'event_name': event.event_name,
                'actual': event.actual,
                'forecast': event.forecast,
                'previous': event.previous,
            }
            for event in events
        ]
    except Exception as e:
        return []

def generate_trader_gpt_analysis(currency_pair, news_data, economic_events):
    """Generate realistic TraderGPT analysis based on actual market data"""
    
    # Extract currencies
    base_currency = currency_pair[:3]
    quote_currency = currency_pair[3:6]
    
    # Analyze news sentiment
    bullish_keywords = ['strong', 'growth', 'rise', 'positive', 'boost', 'rally', 'gains', 'support', 'optimistic']
    bearish_keywords = ['weak', 'decline', 'fall', 'negative', 'concern', 'drop', 'losses', 'pressure', 'pessimistic']
    
    news_text = ' '.join([item.get('title', '') + ' ' + item.get('description', '') for item in news_data]).lower()
    
    bullish_count = sum(1 for keyword in bullish_keywords if keyword in news_text)
    bearish_count = sum(1 for keyword in bearish_keywords if keyword in news_text)
    
    # Determine sentiment based on news and economic events
    if bullish_count > bearish_count:
        sentiment = 'bullish'
        confidence = min(95, 65 + (bullish_count - bearish_count) * 5)
    elif bearish_count > bullish_count:
        sentiment = 'bearish'
        confidence = min(95, 65 + (bearish_count - bullish_count) * 5)
    else:
        sentiment = 'neutral'
        confidence = random.randint(50, 70)
    
    # Analyze economic events impact
    high_impact_events = [event for event in economic_events if event.get('impact') == 'high']
    medium_impact_events = [event for event in economic_events if event.get('impact') == 'medium']
    
    # Generate entry strategy based on sentiment and events
    entry_strategies = {
        'bullish': [
            f"Buy on dips near support levels with confirmation from {base_currency} strength indicators",
            f"Breakout strategy above resistance with {base_currency} momentum confirmation",
            f"Accumulate on pullbacks during {base_currency} positive economic releases"
        ],
        'bearish': [
            f"Sell rallies near resistance with {quote_currency} strength confirmation",
            f"Short breakdowns below support with {base_currency} weakness signals",
            f"Scale in on bounces during {base_currency} negative economic data"
        ],
        'neutral': [
            f"Range trading strategy between support and resistance levels",
            f"Wait for clear directional break with volume confirmation",
            f"Scalping approach around key economic event times"
        ]
    }
    
    # Risk assessment
    risk_level = 'high' if len(high_impact_events) > 1 else 'medium' if len(medium_impact_events) > 2 else 'low'
    
    # Time horizon based on economic events
    if len(high_impact_events) > 0:
        time_horizon = '1-3 days (Event-driven)'
    elif sentiment == 'neutral':
        time_horizon = '4-8 hours (Intraday)'
    else:
        time_horizon = '3-7 days (Short-term swing)'
    
    # Generate target price (simplified)
    current_price_simulation = random.uniform(0.8, 1.8)  # Simulated current price
    if sentiment == 'bullish':
        target_multiplier = random.uniform(1.002, 1.015)
    elif sentiment == 'bearish':
        target_multiplier = random.uniform(0.985, 0.998)
    else:
        target_multiplier = random.uniform(0.995, 1.005)
    
    target_price = f"{current_price_simulation * target_multiplier:.4f}"
    
    # Key factors analysis
    key_factors_components = []
    
    if news_data:
        key_factors_components.append(f"{base_currency} economic fundamentals")
    
    if high_impact_events:
        key_factors_components.append("High-impact economic events this week")
    
    if 'USD' in currency_pair:
        key_factors_components.append("USD monetary policy expectations")
    
    key_factors_components.extend([
        "Technical support/resistance levels",
        "Market risk sentiment",
        "Central bank communications"
    ])
    
    key_factors = ". ".join(key_factors_components[:4]) + "."
    
    return {
        'currency_pair': currency_pair,
        'sentiment': sentiment,
        'confidence_score': confidence,
        'entry_strategy': random.choice(entry_strategies[sentiment]),
        'risk_level': risk_level,
        'time_horizon': time_horizon,
        'target_price': target_price,
        'key_factors': key_factors,
        'economic_events': economic_events,
        'recent_news': news_data
    }

@csrf_exempt
def advanced_trader_gpt_forex_analysis_endpoint(request):
    """Advanced TraderGPT forex analysis endpoint"""
    if request.method != 'POST':
        return JsonResponse({'error': 'Only POST method allowed'}, status=405)
    
    try:
        data = json.loads(request.body)
        currency_pairs = data.get('currency_pairs', [])
        user_email = data.get('user_email', 'anonymous@example.com')
        
        if not currency_pairs:
            return JsonResponse({'error': 'No currency pairs provided'}, status=400)
        
        # Create analysis session
        session_id = str(uuid.uuid4())
        session = TraderGPTForexAnalysisSession.objects.create(
            session_id=session_id,
            user_email=user_email,
            currency_pairs=currency_pairs,
            status='pending'
        )
        
        analyses = []
        
        for pair in currency_pairs:
            try:
                # Get news data for the currency pair
                news_data = fetch_news_data([pair], user_email)
                pair_news = [item for item in news_data.get('message', []) if item.get('asset') == pair]
                
                # Get economic events
                economic_events = get_economic_events_for_pair_two(pair)
                
                # Generate TraderGPT analysis
                analysis = generate_trader_gpt_analysis(pair, pair_news, economic_events)
                
                # Save analysis result
                analysis_result = TraderGPTForexAnalysisResult.objects.create(
                    analysis_session=session,
                    currency_pair=pair,
                    sentiment=analysis['sentiment'],
                    confidence_score=analysis['confidence_score'],
                    entry_strategy=analysis['entry_strategy'],
                    risk_level=analysis['risk_level'],
                    time_horizon=analysis['time_horizon'],
                    target_price=analysis['target_price'],
                    key_factors=analysis['key_factors']
                )
                
                # Link news articles
                for news_item in pair_news[:3]:  # Limit to top 3 news items
                    TraderGPTAnalysisNewsLink.objects.create(
                        analysis_result=analysis_result,
                        title=news_item.get('title', ''),
                        description=news_item.get('description', ''),
                        source=news_item.get('source', ''),
                        url=news_item.get('url', ''),
                        highlights=news_item.get('highlights', ''),
                        relevance_score=random.randint(70, 95)
                    )
                
                # Link economic events
                for event_data in economic_events[:3]:  # Limit to top 3 events
                    try:
                        event = EconomicEvent.objects.filter(
                            currency=event_data['currency'],
                            event_name=event_data['event_name'],
                            date_time__date=datetime.fromisoformat(event_data['date_time'].replace('Z', '+00:00')).date()
                        ).first()
                        
                        if event:
                            TraderGPTAnalysisEconomicEventLink.objects.create(
                                analysis_result=analysis_result,
                                economic_event=event,
                                relevance_score=random.randint(75, 95),
                                impact_assessment=f"Expected to impact {pair} volatility"
                            )
                    except Exception as e:
                        continue
                
                analyses.append(analysis)
                
            except Exception as e:
                # If individual pair analysis fails, continue with others
                continue
        
        # Update session status
        session.status = 'completed' if analyses else 'failed'
        session.save()
        
        return JsonResponse({
            'session_id': session_id,
            'analyses': analyses,
            'timestamp': timezone.now().isoformat(),
            'total_pairs_analyzed': len(analyses)
        })
        
    except Exception as e:
        return JsonResponse({'error': f'Analysis failed: {str(e)}'}, status=500)

def fetch_trader_gpt_analysis_history_endpoint(request):
    """Fetch TraderGPT analysis history for a user"""
    try:
        user_email = request.GET.get('user_email', 'anonymous@example.com')
        limit = int(request.GET.get('limit', 10))
        
        sessions = TraderGPTForexAnalysisSession.objects.filter(
            user_email=user_email,
            status='completed'
        ).order_by('-created_at')[:limit]
        
        history = []
        for session in sessions:
            results = session.results.all()
            session_data = {
                'session_id': session.session_id,
                'timestamp': session.analysis_timestamp.isoformat(),
                'currency_pairs': session.currency_pairs,
                'total_analyses': results.count(),
                'analyses': [
                    {
                        'currency_pair': result.currency_pair,
                        'sentiment': result.sentiment,
                        'confidence_score': result.confidence_score,
                        'risk_level': result.risk_level,
                        'time_horizon': result.time_horizon
                    }
                    for result in results
                ]
            }
            history.append(session_data)
        
        return JsonResponse({
            'history': history,
            'total_sessions': len(history)
        })
        
    except Exception as e:
        return JsonResponse({'error': f'Failed to fetch history: {str(e)}'}, status=500)




# views.py - Add these views to your existing views file

import json
import time
import http.client
import urllib.parse
import threading
from datetime import datetime, timedelta
from django.shortcuts import render
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils import timezone
from django.db import transaction
from django.conf import settings
from .models import (
    WatchedTradingAsset, 
    TraderGPTAnalysisRecord, 
    AnalysisExecutionLog,
    EconomicEvent
)
import openai
import logging

logger = logging.getLogger(__name__)

# Global scheduler instance
trader_analysis_scheduler = None

@csrf_exempt
@require_http_methods(["GET"])
def fetch_watched_trading_assets_view(request):
    """Fetch all watched trading assets"""
    try:
        watched_assets = WatchedTradingAsset.objects.filter(is_active=True).order_by('asset')
        assets_data = [
            {
                'id': asset.id,
                'asset': asset.asset,
                'created_at': asset.created_at.isoformat()
            }
            for asset in watched_assets
        ]
        
        return JsonResponse({
            'success': True,
            'watched_assets': assets_data
        })
    except Exception as e:
        logger.error(f"Error fetching watched assets: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def add_trading_asset_to_watch_view(request):
    """Add a trading asset to the watch list"""
    try:
        data = json.loads(request.body)
        asset = data.get('asset')
        
        if not asset:
            return JsonResponse({'success': False, 'error': 'Asset is required'}, status=400)
        
        # Check if asset is already being watched
        if WatchedTradingAsset.objects.filter(asset=asset, is_active=True).exists():
            return JsonResponse({'success': False, 'error': 'Asset is already being watched'}, status=400)
        
        # Create or reactivate the watched asset
        watched_asset, created = WatchedTradingAsset.objects.get_or_create(
            asset=asset,
            defaults={'is_active': True}
        )
        
        if not created:
            watched_asset.is_active = True
            watched_asset.save()
        
        return JsonResponse({
            'success': True,
            'message': f'{asset} added to watch list',
            'asset_id': watched_asset.id
        })
    except Exception as e:
        logger.error(f"Error adding asset to watch: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["DELETE"])
def remove_watched_trading_asset_view(request):
    """Remove a trading asset from the watch list"""
    try:
        data = json.loads(request.body)
        asset_id = data.get('asset_id')
        
        if not asset_id:
            return JsonResponse({'success': False, 'error': 'Asset ID is required'}, status=400)
        
        watched_asset = WatchedTradingAsset.objects.get(id=asset_id)
        watched_asset.is_active = False
        watched_asset.save()
        
        return JsonResponse({
            'success': True,
            'message': 'Asset removed from watch list'
        })
    except WatchedTradingAsset.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Asset not found'}, status=404)
    except Exception as e:
        logger.error(f"Error removing watched asset: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def fetch_trader_gpt_analyses_view(request):
    """Fetch the latest TraderGPT analyses for all watched assets"""
    try:
        # Get the most recent analysis for each asset
        analyses = []
        watched_assets = WatchedTradingAsset.objects.filter(is_active=True)
        
        for asset in watched_assets:
            latest_analysis = TraderGPTAnalysisRecord.objects.filter(
                asset=asset.asset
            ).first()
            
            if latest_analysis:
                analyses.append({
                    'id': latest_analysis.id,
                    'asset': latest_analysis.asset,
                    'market_sentiment': latest_analysis.market_sentiment,
                    'confidence_score': latest_analysis.confidence_score,
                    'risk_level': latest_analysis.risk_level,
                    'time_horizon': latest_analysis.time_horizon,
                    'entry_strategy': latest_analysis.entry_strategy,
                    'key_factors': latest_analysis.key_factors,
                    'stop_loss_level': latest_analysis.stop_loss_level,
                    'take_profit_level': latest_analysis.take_profit_level,
                    'support_level': latest_analysis.support_level,
                    'resistance_level': latest_analysis.resistance_level,
                    'analysis_timestamp': latest_analysis.analysis_timestamp.isoformat(),
                    'updated_at': latest_analysis.updated_at.isoformat(),
                })
        
        return JsonResponse({
            'success': True,
            'analyses': analyses
        })
    except Exception as e:
        logger.error(f"Error fetching analyses: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


logger = logging.getLogger(__name__)


def validate_choice_field(value, valid_choices, default):
    """Validate that a value matches one of the valid choices"""
    if value and str(value).lower() in [choice[0] for choice in valid_choices]:
        return str(value).lower()
    return default


@csrf_exempt
@require_http_methods(["POST"])
def run_fresh_trader_analysis_view(request):
    """Run a fresh analysis for a specific asset"""
    try:
        data = json.loads(request.body)
        asset = data.get('asset')
        
        if not asset:
            return JsonResponse({'success': False, 'error': 'Asset is required'}, status=400)
        
        # Check if asset is being watched
        if not WatchedTradingAsset.objects.filter(asset=asset, is_active=True).exists():
            return JsonResponse({'success': False, 'error': 'Asset is not in watch list'}, status=400)
        
        # Create execution log
        execution_log = AnalysisExecutionLog.objects.create(
            asset=asset,
            status='running'
        )
        
        start_time = time.time()
        
        try:
            # Run the analysis
            analysis_result = execute_trader_gpt_analysis_for_asset(asset)
            
            if analysis_result['success']:
                execution_log.status = 'completed'
                execution_log.completed_at = timezone.now()
                execution_log.execution_time_seconds = time.time() - start_time
                execution_log.save()
                
                return JsonResponse({
                    'success': True,
                    'message': f'Fresh analysis completed for {asset}',
                    'analysis_id': analysis_result['analysis_id']
                })
            else:
                execution_log.status = 'failed'
                execution_log.error_message = analysis_result.get('error', 'Unknown error')
                execution_log.execution_time_seconds = time.time() - start_time
                execution_log.save()
                
                return JsonResponse({
                    'success': False,
                    'error': analysis_result.get('error', 'Analysis failed')
                }, status=500)
                
        except Exception as analysis_error:
            execution_log.status = 'failed'
            execution_log.error_message = str(analysis_error)
            execution_log.execution_time_seconds = time.time() - start_time
            execution_log.save()
            raise
            
    except Exception as e:
        logger.error(f"Error running fresh analysis: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


def get_economic_events_objects_for_currency(currency_code):
    """Get recent economic events objects for a specified currency (for data processing)."""
    try:
        # Get events from the last 30 days
        thirty_days_ago = timezone.now() - timezone.timedelta(days=30)
        events = EconomicEvent.objects.filter(
            currency=currency_code,
            date_time__gte=thirty_days_ago
        ).order_by('-date_time')
        
        return events
    
    except Exception as e:
        logger.error(f"Error retrieving economic events objects for {currency_code}: {str(e)}")
        return EconomicEvent.objects.none()  # Return empty queryset



def get_economic_events_objects_for_pair(forex_pair):
    """Get economic events objects for both currencies in a forex pair."""
    try:
        base_currency, quote_currency = extract_currencies_from_pair(forex_pair)
        
        # Get events for both currencies
        base_events = get_economic_events_objects_for_currency(base_currency)
        quote_events = get_economic_events_objects_for_currency(quote_currency)
        
        # Combine the querysets
        from django.db.models import Q
        combined_events = EconomicEvent.objects.filter(
            Q(currency=base_currency) | Q(currency=quote_currency),
            date_time__gte=timezone.now() - timezone.timedelta(days=90)
        ).order_by('-date_time')
        
        return combined_events
    
    except Exception as e:
        logger.error(f"Error retrieving economic events objects for pair {forex_pair}: {str(e)}")
        return EconomicEvent.objects.none()


def execute_trader_gpt_analysis_for_asset(asset):
    """Execute TraderGPT analysis for a specific asset"""
    try:
        # Fetch news and economic data
        user_email = "system@tradergpt.com"  # System email for automated analysis
        news_and_events_data = fetch_news_data([asset], user_email)
        
        # Get recent economic events for the asset (formatted string for GPT)
        recent_events_text = get_economic_events_for_pair(asset)
        
        # Get economic events objects for data storage
        recent_events_objects = get_economic_events_objects_for_pair(asset)
        
        # Prepare the prompt for GPT
        prompt = f"""
        Analyze the {asset} currency pair and provide a comprehensive trading analysis.
        
        Recent News Data:
        {json.dumps(news_and_events_data.get('message', [])[:5], indent=2)}
        
        Economic Events for asset:
        {recent_events_text}
        
        Please provide your analysis in the following JSON format with strict character limits:
        {{
            "market_sentiment": "bullish|bearish|neutral (must be exactly one of these three words)",
            "confidence_score": 85,
            "risk_level": "low|medium|high (must be exactly one of these three words)",
            "time_horizon": "short|medium|long (must be exactly one of these three words)",
            "entry_strategy": "Detailed entry strategy with specific levels (max 1000 chars)",
            "key_factors": "Key factors influencing this analysis (max 1000 chars)",
            "stop_loss_level": "Recommended stop loss level (max 200 chars)",
            "take_profit_level": "Recommended take profit level (max 200 chars)",
            "support_level": "Current support level (max 200 chars)",
            "resistance_level": "Current resistance level (max 200 chars)",
            "summary": "Brief overall summary of the analysis (max 500 chars)"
        }}
        
        IMPORTANT: 
        - Use ONLY the exact words for sentiment (bullish/bearish/neutral), risk_level (low/medium/high), and time_horizon (short/medium/long)
        - Keep all responses within the specified character limits
        - Use concise, specific language with actual price levels where possible
        - Base your analysis on current market conditions, news sentiment, economic events, and technical factors
        """
        
        # Call GPT
        gpt_response = chat_gpt(prompt)
        
        # Try to parse JSON from the response
        try:
            # Extract JSON from the response (in case there's additional text)
            start_idx = gpt_response.find('{')
            end_idx = gpt_response.rfind('}') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = gpt_response[start_idx:end_idx]
                analysis_data = json.loads(json_str)
            else:
                # If JSON extraction fails, try parsing the whole response
                analysis_data = json.loads(gpt_response)
                
        except json.JSONDecodeError as json_error:
            logger.error(f"JSON parsing error for {asset}: {str(json_error)}")
            logger.error(f"GPT Response: {gpt_response}")
            
            # If JSON parsing fails, create a basic analysis
            analysis_data = {
                "market_sentiment": "neutral",
                "confidence_score": 50,
                "risk_level": "medium",
                "time_horizon": "medium",
                "entry_strategy": "Wait for clearer market signals before entering position",
                "key_factors": "Analysis could not be parsed properly from GPT response",
                "stop_loss_level": "TBD - Analysis parsing failed",
                "take_profit_level": "TBD - Analysis parsing failed",
                "support_level": "TBD - Analysis parsing failed",
                "resistance_level": "TBD - Analysis parsing failed",
                "summary": "Analysis parsing failed, manual review required"
            }
        
        # Validate choice fields against model choices
        sentiment = validate_choice_field(
            analysis_data.get('market_sentiment'),
            TraderGPTAnalysisRecord.SENTIMENT_CHOICES,
            'neutral'
        )
        
        risk_level = validate_choice_field(
            analysis_data.get('risk_level'),
            TraderGPTAnalysisRecord.RISK_CHOICES,
            'medium'
        )
        
        time_horizon = validate_choice_field(
            analysis_data.get('time_horizon'),
            TraderGPTAnalysisRecord.TIME_HORIZON_CHOICES,
            'medium'
        )
        
        # Ensure confidence score is within valid range
        confidence_score = analysis_data.get('confidence_score', 50)
        if isinstance(confidence_score, str):
            try:
                confidence_score = int(confidence_score)
            except ValueError:
                confidence_score = 50
        confidence_score = min(100, max(1, confidence_score))
        
        # Create the analysis record with validated and truncated data
        analysis_record = TraderGPTAnalysisRecord.objects.create(
            asset=asset,
            market_sentiment=sentiment,
            confidence_score=confidence_score,
            risk_level=risk_level,
            time_horizon=time_horizon,
            entry_strategy=str(analysis_data.get('entry_strategy', ''))[:1000],
            key_factors=str(analysis_data.get('key_factors', ''))[:1000],
            stop_loss_level=str(analysis_data.get('stop_loss_level', ''))[:200],
            take_profit_level=str(analysis_data.get('take_profit_level', ''))[:200],
            support_level=str(analysis_data.get('support_level', ''))[:200],
            resistance_level=str(analysis_data.get('resistance_level', ''))[:200],
            raw_analysis=gpt_response,
            news_data_used=news_and_events_data.get('message', []),
            economic_events_used=[{
                'date': event.date_time.isoformat(),
                'currency': event.currency,
                'event': event.event_name,
                'impact': event.impact,
                'actual': event.actual,
                'forecast': event.forecast,
                'previous': event.previous
            } for event in recent_events_objects[:10]],  # Now using the objects queryset
            analysis_timestamp=timezone.now()
        )
        
        logger.info(f"Successfully created analysis record {analysis_record.id} for {asset}")
        
        return {
            'success': True,
            'analysis_id': analysis_record.id,
            'message': f'Analysis completed for {asset}'
        }
        
    except Exception as e:
        logger.error(f"Error in execute_trader_gpt_analysis_for_asset for {asset}: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }


def run_scheduled_trader_gpt_analyses():
    """
    Scheduled function to run TraderGPT analyses for all watched assets.
    This should be called by a scheduler (like Celery, cron, or Django-RQ)
    """
    try:
        watched_assets = WatchedTradingAsset.objects.filter(is_active=True)
        
        if not watched_assets.exists():
            logger.info("No watched assets found for analysis")
            return
        
        logger.info(f"Starting scheduled analysis for {watched_assets.count()} assets")
        
        results = {
            'successful': 0,
            'failed': 0,
            'errors': []
        }
        
        for asset in watched_assets:
            try:
                # Check if we already have a recent analysis (within last 4 hours)
                recent_analysis = TraderGPTAnalysisRecord.objects.filter(
                    asset=asset.asset,
                    analysis_timestamp__gte=timezone.now() - timedelta(hours=4)
                ).exists()
                
                if not recent_analysis:
                    logger.info(f"Running analysis for {asset.asset}")
                    result = execute_trader_gpt_analysis_for_asset(asset.asset)
                    
                    if result['success']:
                        results['successful'] += 1
                        logger.info(f"Analysis completed for {asset.asset}")
                    else:
                        results['failed'] += 1
                        results['errors'].append(f"{asset.asset}: {result.get('error', 'Unknown error')}")
                        logger.error(f"Analysis failed for {asset.asset}: {result.get('error')}")
                else:
                    logger.info(f"Skipping {asset.asset} - recent analysis exists")
                    
                # Add a small delay between analyses to avoid rate limits
                time.sleep(2)
                
            except Exception as e:
                results['failed'] += 1
                results['errors'].append(f"{asset.asset}: {str(e)}")
                logger.error(f"Error analyzing {asset.asset}: {str(e)}")
        
        logger.info(f"Scheduled analysis completed. Successful: {results['successful']}, Failed: {results['failed']}")
        return results
        
    except Exception as e:
        logger.error(f"Error in run_scheduled_trader_gpt_analyses: {str(e)}")
        return {
            'successful': 0,
            'failed': 0,
            'errors': [str(e)]
        }


@csrf_exempt
@require_http_methods(["POST"])
def trigger_bulk_analysis_view(request):
    """Manual trigger for bulk analysis of all watched assets"""
    try:
        results = run_scheduled_trader_gpt_analyses()
        
        return JsonResponse({
            'success': True,
            'message': 'Bulk analysis completed',
            'results': results
        })
    except Exception as e:
        logger.error(f"Error in trigger_bulk_analysis_view: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


def get_economic_events_for_pair_council(currency_pair):
    """
    Enhanced function to get relevant economic events for a currency pair
    """
    try:
        # Extract currencies from the pair (e.g., EURUSD -> EUR, USD)
        if len(currency_pair) == 6:
            base_currency = currency_pair[:3]
            quote_currency = currency_pair[3:]
        else:
            # Handle other formats if needed
            return []
        
        # Get events for both currencies in the pair
        relevant_currencies = [base_currency, quote_currency]
        
        # Get upcoming events (next 7 days) and recent events (past 3 days)
        start_date = timezone.now() - timedelta(days=3)
        end_date = timezone.now() + timedelta(days=7)
        
        events = EconomicEvent.objects.filter(
            currency__in=relevant_currencies,
            date_time__range=[start_date, end_date]
        ).order_by('date_time')
        
        return [{
            'date_time': event.date_time.isoformat(),
            'currency': event.currency,
            'impact': event.impact,
            'event_name': event.event_name,
            'actual': event.actual,
            'forecast': event.forecast,
            'previous': event.previous,
        } for event in events]
        
    except Exception as e:
        logger.error(f"Error getting economic events for {currency_pair}: {str(e)}")
        return []


class TraderAnalysisScheduler:
    """
    Simple threading-based scheduler for TraderGPT analyses
    """
    def __init__(self, interval_hours=4):
        self.interval_hours = interval_hours
        self.running = False
        self.thread = None
        self.last_run = None
        
    def start(self):
        """Start the scheduler"""
        if self.running:
            logger.info("Scheduler is already running")
            return
            
        self.running = True
        self.thread = threading.Thread(target=self._run_scheduler, daemon=True)
        self.thread.start()
        logger.info(f"TraderGPT analysis scheduler started with {self.interval_hours} hour intervals")
        
    def stop(self):
        """Stop the scheduler"""
        if not self.running:
            return
            
        self.running = False
        if self.thread and self.thread.is_alive():
            self.thread.join(timeout=5)
        logger.info("TraderGPT analysis scheduler stopped")
        
    def is_running(self):
        """Check if scheduler is running"""
        return self.running and self.thread and self.thread.is_alive()
        
    def get_status(self):
        """Get scheduler status"""
        return {
            'running': self.is_running(),
            'interval_hours': self.interval_hours,
            'last_run': self.last_run.isoformat() if self.last_run else None,
            'next_run': (self.last_run + timedelta(hours=self.interval_hours)).isoformat() if self.last_run else None
        }
        
    def _run_scheduler(self):
        """Internal method to run the scheduled analysis"""
        logger.info("Starting TraderGPT analysis scheduler loop")
        
        while self.running:
            try:
                current_time = timezone.now()
                
                # Check if it's time to run (either first run or interval has passed)
                if (self.last_run is None or 
                    current_time >= self.last_run + timedelta(hours=self.interval_hours)):
                    
                    logger.info("Running scheduled TraderGPT analysis...")
                    self.last_run = current_time
                    
                    results = run_scheduled_trader_gpt_analyses()
                    logger.info(f"Scheduled analysis completed: {results}")
                
                # Sleep for 1 minute before checking again
                for _ in range(60):  # 60 seconds = 1 minute
                    if not self.running:
                        break
                    time.sleep(1)
                    
            except Exception as e:
                logger.error(f"Error in scheduler loop: {str(e)}")
                # Sleep for 5 minutes on error before retrying
                for _ in range(300):  # 300 seconds = 5 minutes
                    if not self.running:
                        break
                    time.sleep(1)


def start_trader_analysis_scheduler():
    """Start the global scheduler instance"""
    global trader_analysis_scheduler
    
    if trader_analysis_scheduler is None:
        trader_analysis_scheduler = TraderAnalysisScheduler(interval_hours=4)
    
    if not trader_analysis_scheduler.is_running():
        trader_analysis_scheduler.start()
        return True
    return False


def stop_trader_analysis_scheduler():
    """Stop the global scheduler instance"""
    global trader_analysis_scheduler
    
    if trader_analysis_scheduler and trader_analysis_scheduler.is_running():
        trader_analysis_scheduler.stop()
        return True
    return False


def get_scheduler_status():
    """Get the current scheduler status"""
    global trader_analysis_scheduler
    
    if trader_analysis_scheduler is None:
        return {
            'running': False,
            'interval_hours': 8,
            'last_run': None,
            'next_run': None
        }
    
    return trader_analysis_scheduler.get_status()


@csrf_exempt
@require_http_methods(["POST"])
def start_scheduler_view(request):
    """API endpoint to start the scheduler"""
    try:
        started = start_trader_analysis_scheduler()
        
        if started:
            return JsonResponse({
                'success': True,
                'message': 'Scheduler started successfully',
                'status': get_scheduler_status()
            })
        else:
            return JsonResponse({
                'success': True,
                'message': 'Scheduler is already running',
                'status': get_scheduler_status()
            })
    except Exception as e:
        logger.error(f"Error starting scheduler: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def stop_scheduler_view(request):
    """API endpoint to stop the scheduler"""
    try:
        stopped = stop_trader_analysis_scheduler()
        
        return JsonResponse({
            'success': True,
            'message': 'Scheduler stopped successfully' if stopped else 'Scheduler was not running',
            'status': get_scheduler_status()
        })
    except Exception as e:
        logger.error(f"Error stopping scheduler: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def scheduler_status_view(request):
    """API endpoint to get scheduler status"""
    try:
        return JsonResponse({
            'success': True,
            'status': get_scheduler_status()
        })
    except Exception as e:
        logger.error(f"Error getting scheduler status: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


# Auto-start scheduler when Django starts (optional)
def auto_start_scheduler():
    """Auto-start the scheduler when Django starts"""
    try:
        # Only start if we're in production or if AUTO_START_SCHEDULER is True
        if getattr(settings, 'AUTO_START_TRADER_SCHEDULER', False):
            start_trader_analysis_scheduler()
            logger.info("Auto-started TraderGPT analysis scheduler")
    except Exception as e:
        logger.error(f"Error auto-starting scheduler: {str(e)}")


# Call auto-start when this module is imported (optional)
# Uncomment the line below if you want the scheduler to auto-start
# auto_start_scheduler()

# Add this to your existing views.py where you have the BackgroundScheduler defined

# Assuming you already have this at the top of your views.py:
# from apscheduler.schedulers.background import BackgroundScheduler
# scheduler = BackgroundScheduler()
# scheduler.start()

# Add this job registration after your scheduler.start() line:

def setup_trader_gpt_analysis_scheduler():
    """Setup the TraderGPT analysis job in the existing scheduler"""
    try:
        # Add job to run every 4 hours
        scheduler.add_job(
            func=run_scheduled_trader_gpt_analyses,
            trigger='interval',
            hours=8,
            id='trader_gpt_analysis_job',
            name='TraderGPT Analysis Job',
            replace_existing=True,
            max_instances=1
        )
        logger.info("TraderGPT analysis job added to scheduler - runs every 8 hours")
        
        # Optional: Add a job to clean up old analyses (runs daily at 2 AM)
        scheduler.add_job(
            func=cleanup_old_analyses,
            trigger='cron',
            hour=2,
            minute=0,
            id='cleanup_old_analyses_job',
            name='Cleanup Old Analyses Job',
            replace_existing=True,
            max_instances=1
        )
        logger.info("Cleanup job added to scheduler - runs daily at 2 AM")
        
    except Exception as e:
        logger.error(f"Error setting up TraderGPT scheduler: {str(e)}")

def cleanup_old_analyses():
    """Clean up analyses older than 30 days"""
    try:
        cutoff_date = timezone.now() - timedelta(days=30)
        deleted_count = TraderGPTAnalysisRecord.objects.filter(
            created_at__lt=cutoff_date
        ).count()
        
        TraderGPTAnalysisRecord.objects.filter(
            created_at__lt=cutoff_date
        ).delete()
        
        # Also clean up old execution logs
        AnalysisExecutionLog.objects.filter(
            started_at__lt=cutoff_date
        ).delete()
        
        logger.info(f"Cleaned up {deleted_count} old analysis records")
        
    except Exception as e:
        logger.error(f"Error cleaning up old analyses: {str(e)}")

# Call this function after your scheduler.start() line
setup_trader_gpt_analysis_scheduler()

# Optional: Add these utility functions to manage the scheduler

def start_trader_analysis_job():
    """Manually start the trader analysis job"""
    try:
        scheduler.add_job(
            func=run_scheduled_trader_gpt_analyses,
            trigger='interval',
            hours=8,
            id='trader_gpt_analysis_job',
            name='TraderGPT Analysis Job',
            replace_existing=True,
            max_instances=1
        )
        return True
    except Exception as e:
        logger.error(f"Error starting trader analysis job: {str(e)}")
        return False

def stop_trader_analysis_job():
    """Manually stop the trader analysis job"""
    try:
        scheduler.remove_job('trader_gpt_analysis_job')
        return True
    except Exception as e:
        logger.error(f"Error stopping trader analysis job: {str(e)}")
        return False

@csrf_exempt
@require_http_methods(["POST"])
def manage_scheduler_view(request):
    """Endpoint to manage the scheduler (start/stop/status)"""
    try:
        data = json.loads(request.body)
        action = data.get('action')
        
        if action == 'start':
            success = start_trader_analysis_job()
            message = 'Scheduler started successfully' if success else 'Failed to start scheduler'
        elif action == 'stop':
            success = stop_trader_analysis_job()
            message = 'Scheduler stopped successfully' if success else 'Failed to stop scheduler'
        elif action == 'status':
            jobs = scheduler.get_jobs()
            trader_job = next((job for job in jobs if job.id == 'trader_gpt_analysis_job'), None)
            success = True
            message = {
                'running': trader_job is not None,
                'next_run_time': trader_job.next_run_time.isoformat() if trader_job and trader_job.next_run_time else None,
                'total_jobs': len(jobs)
            }
        else:
            return JsonResponse({'success': False, 'error': 'Invalid action'}, status=400)
        
        return JsonResponse({
            'success': success,
            'message': message
        })
        
    except Exception as e:
        logger.error(f"Error managing scheduler: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)

# If you want to run the analysis immediately on startup (optional)
def run_initial_analysis():
    """Run analysis once on startup"""
    try:
        # Add a one-time job to run analysis after 30 seconds
        scheduler.add_job(
            func=run_scheduled_trader_gpt_analyses,
            trigger='date',
            run_date=timezone.now() + timedelta(seconds=30),
            id='initial_analysis_job',
            name='Initial Analysis Job'
        )
        logger.info("Initial analysis scheduled to run in 30 seconds")
    except Exception as e:
        logger.error(f"Error scheduling initial analysis: {str(e)}")

# Uncomment the line below if you want to run analysis immediately on startup
# run_initial_analysis()

# Add these views to your views.py

import json
import time
import uuid
import logging
from datetime import datetime
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils import timezone
from django.core.paginator import Paginator
from django.db.models import Q

# Import your existing models and functions
from .models import (
    WatchedTradingAsset, 
    AITradingCouncilConversation, 
    AITradingCouncilParticipant,
    TraderGPTAnalysisRecord
)

logger = logging.getLogger(__name__)

def execute_ai_trading_council_conversation():
    """Execute an AI Trading Council conversation with all active watched assets"""
    try:
        # Get all active watched assets
        watched_assets = WatchedTradingAsset.objects.filter(is_active=True)
        
        if not watched_assets.exists():
            return {
                'success': False,
                'error': 'No active watched assets found for council discussion'
            }
        
        # Create conversation record
        conversation_id = f"council_{uuid.uuid4().hex[:12]}_{int(time.time())}"
        
        conversation = AITradingCouncilConversation.objects.create(
            conversation_id=conversation_id,
            title=f"AI Trading Council Discussion - {timezone.now().strftime('%Y-%m-%d %H:%M')}",
            participating_assets=[asset.asset for asset in watched_assets],
            total_participants=watched_assets.count(),
            status='running'
        )
        
        start_time = time.time()
        
        try:
            # Execute the conversation
            conversation_result = run_council_discussion(conversation, watched_assets)
            
            if conversation_result['success']:
                # Update conversation with results
                conversation.status = 'completed'
                conversation.completed_at = timezone.now()
                conversation.execution_time_seconds = time.time() - start_time
                conversation.conversation_data = conversation_result['conversation_data']
                conversation.conversation_summary = conversation_result['summary']
                conversation.overall_economic_outlook = conversation_result['economic_outlook']
                conversation.global_market_sentiment = conversation_result['market_sentiment']
                conversation.market_volatility_level = conversation_result['volatility_level']
                conversation.major_economic_themes = conversation_result['themes']
                conversation.currency_strength_rankings = conversation_result['currency_rankings']
                conversation.risk_factors_identified = conversation_result['risk_factors']
                conversation.opportunity_areas = conversation_result['opportunities']
                conversation.bullish_sentiment_count = conversation_result['sentiment_counts']['bullish']
                conversation.bearish_sentiment_count = conversation_result['sentiment_counts']['bearish']
                conversation.neutral_sentiment_count = conversation_result['sentiment_counts']['neutral']
                conversation.average_confidence_score = conversation_result['avg_confidence']
                conversation.save()
                
                return {
                    'success': True,
                    'conversation_id': conversation.conversation_id,
                    'message': 'AI Trading Council conversation completed successfully'
                }
            else:
                conversation.status = 'failed'
                conversation.error_message = conversation_result.get('error', 'Unknown error')
                conversation.execution_time_seconds = time.time() - start_time
                conversation.save()
                
                return {
                    'success': False,
                    'error': conversation_result.get('error', 'Council conversation failed')
                }
                
        except Exception as conversation_error:
            conversation.status = 'failed'
            conversation.error_message = str(conversation_error)
            conversation.execution_time_seconds = time.time() - start_time
            conversation.save()
            raise
            
    except Exception as e:
        logger.error(f"Error executing AI Trading Council conversation: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }


def run_council_discussion(conversation, watched_assets):
    """Run the actual AI council discussion"""
    try:
        conversation_turns = []
        participants_data = {}
        sentiment_counts = {'bullish': 0, 'bearish': 0, 'neutral': 0}
        total_confidence = 0
        confidence_count = 0
        
        # Get recent analysis for each asset to base the discussion on
        for asset in watched_assets:
            recent_analysis = TraderGPTAnalysisRecord.objects.filter(
                asset=asset.asset
            ).order_by('-analysis_timestamp').first()
            
            if recent_analysis:
                participant_name = f"{asset.get_asset_display()} Analyst"
                participants_data[asset.asset] = {
                    'name': participant_name,
                    'analysis': recent_analysis,
                    'turns_count': 0
                }
        
        # Run 3 rounds of discussion
        for round_num in range(1, 4):
            round_prompt = get_round_prompt(round_num, participants_data, conversation_turns)
            
            for asset_code, participant_data in participants_data.items():
                if participant_data['analysis']:
                    # Generate participant's response for this round
                    participant_response = generate_participant_response(
                        participant_data, 
                        round_num, 
                        conversation_turns,
                        round_prompt
                    )
                    
                    conversation_turns.append({
                        'round': round_num,
                        'participant': participant_data['name'],
                        'asset': asset_code,
                        'message': participant_response['message'],
                        'timestamp': timezone.now().isoformat(),
                        'sentiment': participant_response['sentiment'],
                        'confidence': participant_response['confidence']
                    })
                    
                    # Update counts
                    sentiment_counts[participant_response['sentiment']] += 1
                    total_confidence += participant_response['confidence']
                    confidence_count += 1
                    participants_data[asset_code]['turns_count'] += 1
        
        # Generate overall summary and insights
        summary_data = generate_conversation_summary(conversation_turns, participants_data)
        
        # Create participant records
        for asset_code, participant_data in participants_data.items():
            if participant_data['analysis']:
                AITradingCouncilParticipant.objects.create(
                    conversation=conversation,
                    asset_code=asset_code,
                    participant_name=participant_data['name'],
                    market_sentiment=participant_data['analysis'].market_sentiment,
                    confidence_score=participant_data['analysis'].confidence_score,
                    risk_assessment=participant_data['analysis'].risk_level,
                    key_insights=extract_participant_insights(asset_code, conversation_turns),
                    turns_spoken=participant_data['turns_count']
                )
        
        return {
            'success': True,
            'conversation_data': {'turns': conversation_turns},
            'summary': summary_data['summary'],
            'economic_outlook': summary_data['economic_outlook'],
            'market_sentiment': summary_data['market_sentiment'],
            'volatility_level': summary_data['volatility_level'],
            'themes': summary_data['themes'],
            'currency_rankings': summary_data['currency_rankings'],
            'risk_factors': summary_data['risk_factors'],
            'opportunities': summary_data['opportunities'],
            'sentiment_counts': sentiment_counts,
            'avg_confidence': total_confidence / confidence_count if confidence_count > 0 else 0
        }
        
    except Exception as e:
        logger.error(f"Error in council discussion: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }


def get_round_prompt(round_num, participants_data, conversation_turns):
    """Get the prompt for each discussion round"""
    if round_num == 1:
        return "Present your current market analysis and outlook for your currency pair."
    elif round_num == 2:
        return "Discuss how global economic factors are affecting your market and respond to other analysts' viewpoints."
    else:
        return "Provide your final assessment and key recommendations based on the full discussion."


def generate_participant_response(participant_data, round_num, conversation_turns, round_prompt):
    """Generate a participant's response using GPT"""
    try:
        analysis = participant_data['analysis']
        participant_name = participant_data['name']
        
        # Build context from previous turns
        previous_context = ""
        if conversation_turns:
            recent_turns = conversation_turns[-6:]  # Last 6 turns for context
            for turn in recent_turns:
                previous_context += f"{turn['participant']}: {turn['message'][:200]}...\n"
        
        prompt = f"""
        You are {participant_name}, an expert analyst specializing in {analysis.asset}.
        
        Current Analysis Data:
        - Market Sentiment: {analysis.market_sentiment}
        - Confidence: {analysis.confidence_score}%
        - Risk Level: {analysis.risk_level}
        - Entry Strategy: {analysis.entry_strategy}
        - Key Factors: {analysis.key_factors}
        
        Previous Discussion Context:
        {previous_context}
        
        Round {round_num} Task: {round_prompt}
        
        Provide your response as a trading expert in exactly this JSON format:
        {{
            "message": "Your response as the {participant_name} (max 500 characters, professional trading discussion style)",
            "sentiment": "bullish|bearish|neutral",
            "confidence": {analysis.confidence_score},
            "key_point": "One key insight from your analysis (max 200 characters)"
        }}
        
        Keep responses concise, professional, and focused on trading insights.
        """
        
        # Call GPT (using your existing chat_gpt function)
        gpt_response = chat_gpt(prompt)
        
        try:
            # Parse JSON response
            start_idx = gpt_response.find('{')
            end_idx = gpt_response.rfind('}') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = gpt_response[start_idx:end_idx]
                response_data = json.loads(json_str)
            else:
                response_data = json.loads(gpt_response)
                
        except json.JSONDecodeError:
            # Fallback response if JSON parsing fails
            response_data = {
                "message": f"Based on my {analysis.asset} analysis, I maintain a {analysis.market_sentiment} outlook with {analysis.confidence_score}% confidence.",
                "sentiment": analysis.market_sentiment,
                "confidence": analysis.confidence_score,
                "key_point": "Technical and fundamental factors support current assessment"
            }
        
        return response_data
        
    except Exception as e:
        logger.error(f"Error generating participant response: {str(e)}")
        return {
            "message": f"Currently analyzing {participant_data['analysis'].asset} market conditions.",
            "sentiment": "neutral",
            "confidence": 50,
            "key_point": "Analysis in progress"
        }


def generate_conversation_summary(conversation_turns, participants_data):
    """Generate overall conversation summary and insights"""
    try:
        # Prepare conversation summary for GPT
        conversation_text = ""
        for turn in conversation_turns:
            conversation_text += f"{turn['participant']}: {turn['message']}\n"
        
        prompt = f"""
        Analyze this AI Trading Council discussion and provide insights about the global economy:
        
        Discussion:
        {conversation_text}
        
        Provide analysis in this JSON format:
        {{
            "summary": "Comprehensive summary of the discussion and key conclusions (max 1000 chars)",
            "economic_outlook": "very_positive|positive|neutral|negative|very_negative",
            "market_sentiment": "bullish|bearish|neutral",
            "volatility_level": "low|medium|high|extreme",
            "themes": ["theme1", "theme2", "theme3"],
            "currency_rankings": {{"strongest": "USD", "weakest": "EUR", "neutral": ["GBP", "JPY"]}},
            "risk_factors": ["risk1", "risk2", "risk3"],
            "opportunities": ["opportunity1", "opportunity2"]
        }}
        """
        
        gpt_response = chat_gpt(prompt)
        
        try:
            start_idx = gpt_response.find('{')
            end_idx = gpt_response.rfind('}') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = gpt_response[start_idx:end_idx]
                summary_data = json.loads(json_str)
            else:
                summary_data = json.loads(gpt_response)
        except json.JSONDecodeError:
            # Fallback summary
            summary_data = {
                "summary": "AI Trading Council discussed current market conditions across multiple currency pairs with varying outlooks.",
                "economic_outlook": "neutral",
                "market_sentiment": "neutral",
                "volatility_level": "medium",
                "themes": ["Market Analysis", "Economic Indicators", "Risk Assessment"],
                "currency_rankings": {"strongest": "USD", "weakest": "EUR", "neutral": []},
                "risk_factors": ["Market Volatility", "Economic Uncertainty"],
                "opportunities": ["Technical Setups", "Fundamental Shifts"]
            }
        
        return summary_data
        
    except Exception as e:
        logger.error(f"Error generating conversation summary: {str(e)}")
        return {
            "summary": "Discussion summary unavailable due to processing error.",
            "economic_outlook": "neutral",
            "market_sentiment": "neutral",
            "volatility_level": "medium",
            "themes": [],
            "currency_rankings": {},
            "risk_factors": [],
            "opportunities": []
        }


def extract_participant_insights(asset_code, conversation_turns):
    """Extract key insights made by a specific participant"""
    insights = []
    for turn in conversation_turns:
        if turn['asset'] == asset_code:
            insights.append(turn['message'][:200])
    return insights


@csrf_exempt
@require_http_methods(["POST"])
def run_manual_council_conversation_view(request):
    """API endpoint to manually trigger a council conversation"""
    try:
        result = execute_ai_trading_council_conversation()
        
        if result['success']:
            return JsonResponse({
                'success': True,
                'message': result['message'],
                'conversation_id': result['conversation_id']
            })
        else:
            return JsonResponse({
                'success': False,
                'error': result['error']
            }, status=500)
            
    except Exception as e:
        logger.error(f"Error in manual council conversation view: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def get_council_conversations_view(request):
    """API endpoint to get council conversations with pagination"""
    try:
        page = int(request.GET.get('page', 1))
        page_size = int(request.GET.get('page_size', 10))
        
        conversations = AITradingCouncilConversation.objects.all().order_by('-created_at')
        paginator = Paginator(conversations, page_size)
        page_obj = paginator.get_page(page)
        
        conversations_data = []
        for conversation in page_obj:
            conversations_data.append({
                'id': conversation.id,
                'conversation_id': conversation.conversation_id,
                'title': conversation.title,
                'status': conversation.status,
                'created_at': conversation.created_at.isoformat(),
                'completed_at': conversation.completed_at.isoformat() if conversation.completed_at else None,
                'participating_assets': conversation.participating_assets,
                'total_participants': conversation.total_participants,
                'conversation_summary': conversation.conversation_summary,
                'overall_economic_outlook': conversation.overall_economic_outlook,
                'global_market_sentiment': conversation.global_market_sentiment,
                'market_volatility_level': conversation.market_volatility_level,
                'major_economic_themes': conversation.major_economic_themes,
                'currency_strength_rankings': conversation.currency_strength_rankings,
                'risk_factors_identified': conversation.risk_factors_identified,
                'opportunity_areas': conversation.opportunity_areas,
                'bullish_sentiment_count': conversation.bullish_sentiment_count,
                'bearish_sentiment_count': conversation.bearish_sentiment_count,
                'neutral_sentiment_count': conversation.neutral_sentiment_count,
                'average_confidence_score': conversation.average_confidence_score,
                'execution_time_seconds': conversation.execution_time_seconds,
                'conversation_turns_count': conversation.get_conversation_turns_count(),
                'dominant_sentiment': conversation.get_dominant_sentiment()
            })
        
        return JsonResponse({
            'success': True,
            'conversations': conversations_data,
            'pagination': {
                'current_page': page,
                'total_pages': paginator.num_pages,
                'total_conversations': paginator.count,
                'has_next': page_obj.has_next(),
                'has_previous': page_obj.has_previous()
            }
        })
        
    except Exception as e:
        logger.error(f"Error getting council conversations: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def get_council_conversation_details_view(request, conversation_id):
    """API endpoint to get detailed conversation data"""
    try:
        conversation = AITradingCouncilConversation.objects.get(conversation_id=conversation_id)
        participants = AITradingCouncilParticipant.objects.filter(conversation=conversation)
        
        participants_data = []
        for participant in participants:
            participants_data.append({
                'asset_code': participant.asset_code,
                'participant_name': participant.participant_name,
                'market_sentiment': participant.market_sentiment,
                'confidence_score': participant.confidence_score,
                'risk_assessment': participant.risk_assessment,
                'key_insights': participant.key_insights,
                'turns_spoken': participant.turns_spoken
            })
        
        return JsonResponse({
            'success': True,
            'conversation': {
                'conversation_id': conversation.conversation_id,
                'title': conversation.title,
                'status': conversation.status,
                'created_at': conversation.created_at.isoformat(),
                'completed_at': conversation.completed_at.isoformat() if conversation.completed_at else None,
                'conversation_data': conversation.conversation_data,
                'conversation_summary': conversation.conversation_summary,
                'overall_economic_outlook': conversation.overall_economic_outlook,
                'global_market_sentiment': conversation.global_market_sentiment,
                'market_volatility_level': conversation.market_volatility_level,
                'major_economic_themes': conversation.major_economic_themes,
                'currency_strength_rankings': conversation.currency_strength_rankings,
                'risk_factors_identified': conversation.risk_factors_identified,
                'opportunity_areas': conversation.opportunity_areas,
                'execution_time_seconds': conversation.execution_time_seconds
            },
            'participants': participants_data
        })
        
    except AITradingCouncilConversation.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Conversation not found'}, status=404)
    except Exception as e:
        logger.error(f"Error getting conversation details: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)

# Add this to your scheduler setup (where you have BackgroundScheduler)

import logging
from apscheduler.schedulers.background import BackgroundScheduler
from django.conf import settings

logger = logging.getLogger(__name__)

def schedule_ai_council_conversations():
    """Schedule AI Trading Council conversations"""
    try:
        # Import here to avoid circular imports
        from .views import execute_ai_trading_council_conversation
        
        logger.info("Starting scheduled AI Trading Council conversation...")
        result = execute_ai_trading_council_conversation()
        
        if result['success']:
            logger.info(f"Scheduled AI Council conversation completed: {result['conversation_id']}")
        else:
            logger.error(f"Scheduled AI Council conversation failed: {result['error']}")
            
    except Exception as e:
        logger.error(f"Error in scheduled AI Council conversation: {str(e)}")

# Add this job to your existing scheduler
# scheduler = BackgroundScheduler()  # You already have this

# Schedule AI Council conversations to run every 24 hours
scheduler.add_job(
    schedule_ai_council_conversations,
    'interval',
    hours=24,
    id='ai_trading_council_conversations',
    replace_existing=True,
    max_instances=1
)

# # You can also add different schedules:

# # Run daily at 9 AM UTC
# scheduler.add_job(
#     schedule_ai_council_conversations,
#     'cron',
#     hour=9,
#     minute=0,
#     id='daily_council_morning',
#     replace_existing=True,
#     max_instances=1
# )

# # Run daily at 3 PM UTC (market close)
# scheduler.add_job(
#     schedule_ai_council_conversations,
#     'cron',
#     hour=15,
#     minute=0,
#     id='daily_council_afternoon',
#     replace_existing=True,
#     max_instances=1
# )


from django.shortcuts import render, get_object_or_404
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from django.db.models import Q
import json
import base64
import os
from .models import FirmCompliance

@csrf_exempt
@require_http_methods(["GET", "POST"])
def firm_compliance_list(request):
    """List all firm compliance records or create a new one"""
    
    if request.method == "GET":
        # Get search query if provided
        search_query = request.GET.get('search', '')
        
        # Filter records based on search query
        if search_query:
            records = FirmCompliance.objects.filter(
                Q(firm_name__icontains=search_query) | 
                Q(personal_notes__icontains=search_query)
            )
        else:
            records = FirmCompliance.objects.all()
        
        # Serialize data
        data = []
        for record in records:
            data.append({
                'id': str(record.id),
                'firm_name': record.firm_name,
                'personal_notes': record.personal_notes,
                'logo_url': record.logo_url,  # This will now return base64 data URL
                'created_at': record.created_at.isoformat(),
                'updated_at': record.updated_at.isoformat(),
            })
        
        return JsonResponse({
            'success': True,
            'data': data,
            'count': len(data)
        })
    
    elif request.method == "POST":
        try:
            # Handle both JSON and form data
            if request.content_type == 'application/json':
                data = json.loads(request.body)
                firm_name = data.get('firm_name', '').strip()
                personal_notes = data.get('personal_notes', '').strip()
                logo_data = data.get('logo_data')  # Base64 encoded image
                logo_filename = data.get('logo_filename')
            else:
                firm_name = request.POST.get('firm_name', '').strip()
                personal_notes = request.POST.get('personal_notes', '').strip()
                logo_file = request.FILES.get('firm_logo')
            
            # Validate required fields
            if not firm_name:
                return JsonResponse({
                    'success': False,
                    'message': 'Firm name is required'
                }, status=400)
            
            # Create new record
            record = FirmCompliance.objects.create(
                firm_name=firm_name,
                personal_notes=personal_notes
            )
            
            # Handle logo upload
            if request.content_type == 'application/json' and logo_data and logo_filename:
                # Handle base64 encoded image
                try:
                    format, imgstr = logo_data.split(';base64,')
                    ext = format.split('/')[-1]
                    
                    # Ensure we have a valid file extension
                    if ext not in ['jpeg', 'jpg', 'png', 'gif', 'webp']:
                        ext = 'png'  # default to png
                    
                    filename = f"{record.id}_{logo_filename}"
                    if not filename.lower().endswith(('.' + ext)):
                        filename = f"{filename}.{ext}"
                    
                    logo_file = ContentFile(
                        base64.b64decode(imgstr),
                        name=filename
                    )
                    record.firm_logo.save(logo_file.name, logo_file, save=True)
                except Exception as e:
                    print(f"Error saving logo: {e}")
                    # If logo upload fails, continue without it
                    pass
            elif 'firm_logo' in request.FILES:
                # Handle direct file upload
                record.firm_logo = request.FILES['firm_logo']
                record.save()
            
            return JsonResponse({
                'success': True,
                'message': 'Firm compliance record created successfully',
                'data': {
                    'id': str(record.id),
                    'firm_name': record.firm_name,
                    'personal_notes': record.personal_notes,
                    'logo_url': record.logo_url,  # This will now return base64 data URL
                    'created_at': record.created_at.isoformat(),
                    'updated_at': record.updated_at.isoformat(),
                }
            })
            
        except json.JSONDecodeError:
            return JsonResponse({
                'success': False,
                'message': 'Invalid JSON data'
            }, status=400)
        except Exception as e:
            return JsonResponse({
                'success': False,
                'message': f'Error creating record: {str(e)}'
            }, status=500)

@csrf_exempt
@require_http_methods(["GET", "PUT", "DELETE"])
def firm_compliance_detail(request, compliance_id):
    """Get, update, or delete a specific firm compliance record"""
    
    try:
        record = get_object_or_404(FirmCompliance, id=compliance_id)
    except Exception:
        return JsonResponse({
            'success': False,
            'message': 'Record not found'
        }, status=404)
    
    if request.method == "GET":
        return JsonResponse({
            'success': True,
            'data': {
                'id': str(record.id),
                'firm_name': record.firm_name,
                'personal_notes': record.personal_notes,
                'logo_url': record.logo_url,  # This will now return base64 data URL
                'created_at': record.created_at.isoformat(),
                'updated_at': record.updated_at.isoformat(),
            }
        })
    
    elif request.method == "PUT":
        try:
            data = json.loads(request.body)
            
            # Update fields if provided
            if 'firm_name' in data:
                firm_name = data['firm_name'].strip()
                if not firm_name:
                    return JsonResponse({
                        'success': False,
                        'message': 'Firm name cannot be empty'
                    }, status=400)
                record.firm_name = firm_name
            
            if 'personal_notes' in data:
                record.personal_notes = data['personal_notes'].strip()
            
            # Handle logo update
            if 'logo_data' in data and 'logo_filename' in data:
                try:
                    format, imgstr = data['logo_data'].split(';base64,')
                    ext = format.split('/')[-1]
                    
                    # Ensure we have a valid file extension
                    if ext not in ['jpeg', 'jpg', 'png', 'gif', 'webp']:
                        ext = 'png'  # default to png
                    
                    # Delete old logo if exists
                    if record.firm_logo:
                        default_storage.delete(record.firm_logo.name)
                    
                    filename = f"{record.id}_{data['logo_filename']}"
                    if not filename.lower().endswith(('.' + ext)):
                        filename = f"{filename}.{ext}"
                    
                    logo_file = ContentFile(
                        base64.b64decode(imgstr),
                        name=filename
                    )
                    record.firm_logo.save(logo_file.name, logo_file, save=False)
                except Exception as e:
                    print(f"Error updating logo: {e}")
                    pass
            
            record.save()
            
            return JsonResponse({
                'success': True,
                'message': 'Record updated successfully',
                'data': {
                    'id': str(record.id),
                    'firm_name': record.firm_name,
                    'personal_notes': record.personal_notes,
                    'logo_url': record.logo_url,  # This will now return base64 data URL
                    'created_at': record.created_at.isoformat(),
                    'updated_at': record.updated_at.isoformat(),
                }
            })
            
        except json.JSONDecodeError:
            return JsonResponse({
                'success': False,
                'message': 'Invalid JSON data'
            }, status=400)
        except Exception as e:
            return JsonResponse({
                'success': False,
                'message': f'Error updating record: {str(e)}'
            }, status=500)
    
    elif request.method == "DELETE":
        try:
            # Delete the logo file if it exists
            if record.firm_logo:
                default_storage.delete(record.firm_logo.name)
            
            record.delete()
            
            return JsonResponse({
                'success': True,
                'message': 'Record deleted successfully'
            })
            
        except Exception as e:
            return JsonResponse({
                'success': False,
                'message': f'Error deleting record: {str(e)}'
            }, status=500)

import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from collections import defaultdict
from django.db.models import Q
from rest_framework.decorators import api_view
from rest_framework.response import Response
import yfinance as yf

def clean_numeric_value_esi(value_str):
    """
    Convert string values like '3.2%', '$50.4B', etc. to float values
    Enhanced version for ESI calculation
    """
    if not value_str or value_str.strip() == '' or value_str.lower() in ['n/a', 'na', '-']:
        return None
    
    try:
        # Handle special cases
        value_str = str(value_str).strip()
        
        # Remove common symbols
        multipliers = {'K': 1000, 'M': 1000000, 'B': 1000000000, 'T': 1000000000000}
        multiplier = 1
        
        # Check for multiplier suffixes
        for suffix, mult in multipliers.items():
            if value_str.upper().endswith(suffix):
                multiplier = mult
                value_str = value_str[:-1]
                break
        
        # Remove other symbols
        cleaned = value_str.replace('%', '').replace('$', '').replace(',', '')
        cleaned = cleaned.replace('', '').replace('', '').replace('', '')
        
        # Handle negative values
        is_negative = cleaned.startswith('-') or cleaned.startswith('(')
        cleaned = cleaned.replace('-', '').replace('(', '').replace(')', '')
        
        # Convert to float
        result = float(cleaned) * multiplier
        return -result if is_negative else result
        
    except (ValueError, TypeError, AttributeError):
        return None

def calculate_percentage_deviation(actual, forecast):
    """
    Calculate percentage deviation from forecast
    Returns standardized deviation score
    """
    if actual is None or forecast is None:
        return 0
    
    actual_val = clean_numeric_value_esi(actual)
    forecast_val = clean_numeric_value_esi(forecast)
    
    if actual_val is None or forecast_val is None:
        return 0
    
    # Handle division by zero
    if forecast_val == 0:
        return 100 if actual_val > 0 else -100 if actual_val < 0 else 0
    
    # Calculate percentage deviation
    deviation = ((actual_val - forecast_val) / abs(forecast_val)) * 100
    
    # Cap extreme values to prevent outliers from skewing the index
    return max(-200, min(200, deviation))

def get_impact_weight(impact):
    """
    Return weight based on impact level
    """
    weights = {
        'high': 3.0,
        'medium': 2.0,
        'low': 1.0
    }
    return weights.get(impact.lower(), 1.0)

def normalize_esi_scores(scores):
    """
    Normalize ESI scores using z-score normalization
    Then scale to a more interpretable range
    """
    if not scores:
        return scores
    
    # Calculate mean and standard deviation
    mean_score = np.mean(scores)
    std_score = np.std(scores)
    
    if std_score == 0:
        return [50 for _ in scores]  # Return neutral scores if no variation
    
    # Apply z-score normalization
    normalized = [(score - mean_score) / std_score for score in scores]
    
    # Scale to 0-100 range with 50 as neutral
    # Values above 50 indicate stronger than average performance
    # Values below 50 indicate weaker than average performance
    scaled = [max(0, min(100, 50 + (norm * 15))) for norm in normalized]
    
    return scaled

def obtain_dataset(asset, interval, num_days):
    """
    Generic function to obtain data from yfinance
    """
    # Calculate the end and start dates
    end_date = (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")
    start_date = (datetime.now() - timedelta(days=num_days)).strftime("%Y-%m-%d")

    # Download data using yfinance - handle both forex and stock indices
    if '=' not in asset and asset.startswith('^'):
        # Stock index (already has ^ symbol)
        data = yf.download(asset, start=start_date, end=end_date, interval=interval)
    elif '=' not in asset:
        # Forex pair - add =X suffix
        forex_asset = f"{asset}=X"
        data = yf.download(forex_asset, start=start_date, end=end_date, interval=interval)
    else:
        # Already formatted
        data = yf.download(asset, start=start_date, end=end_date, interval=interval)
    
    return data

def calculate_relative_volume(volume_data, lookback_period=20):
    """
    Calculate relative volume as current volume / average volume over lookback period
    """
    if len(volume_data) < lookback_period:
        return [1.0] * len(volume_data)  # Return neutral ratio if insufficient data
    
    relative_volumes = []
    for i in range(len(volume_data)):
        if i < lookback_period:
            # Use available data for early points
            avg_volume = np.mean(volume_data[:i+1]) if i > 0 else volume_data[0]
        else:
            # Use rolling window
            avg_volume = np.mean(volume_data[i-lookback_period:i])
        
        current_volume = volume_data[i]
        
        if avg_volume > 0:
            relative_volume = current_volume / avg_volume
        else:
            relative_volume = 1.0
        
        relative_volumes.append(relative_volume)
    
    return relative_volumes

def get_volume_data(assets, date_range):
    """
    Fetch volume data for specified assets and calculate relative volume ratios
    """
    range_days = {
        '7d': 7,
        '30d': 30,
        '90d': 90,
        '180d': 180,
        '365d': 365
    }
    
    days = range_days.get(date_range, 30)
    
    # Add extra days for volume calculation lookback
    volume_lookback_days = days + 25  # Extra days for better volume average calculation
    
    # Determine interval based on date range
    if days <= 7:
        interval = '1h'
    elif days <= 30:
        interval = '1d'
    else:
        interval = '1d'
    
    volume_data = {}
    
    for asset_id in assets:
        try:
            print(f"Fetching volume data for {asset_id} with {volume_lookback_days} days, interval {interval}")
            
            # Use obtain_dataset function
            data = obtain_dataset(asset_id, interval, volume_lookback_days)
            
            if not data.empty:
                print(f"Got {len(data)} data points for volume calculation on {asset_id}")
                
                # Process the volume data
                volume_entries = []
                
                # Handle MultiIndex columns
                if isinstance(data.columns, pd.MultiIndex):
                    volume_col = None
                    close_col = None
                    
                    for col in data.columns:
                        if col[0] == 'Volume':
                            volume_col = col
                        if col[0] == 'Close':
                            close_col = col
                    
                    if volume_col is None:
                        print(f"No Volume column found for {asset_id}")
                        volume_data[asset_id] = []
                        continue
                        
                    # Extract volume and close data
                    for date_idx, volume in data[volume_col].items():
                        try:
                            if hasattr(date_idx, 'strftime'):
                                date_str = date_idx.strftime('%Y-%m-%d')
                            else:
                                date_str = str(date_idx)[:10]
                            
                            if pd.notna(volume) and volume > 0:
                                close_price = data[close_col].loc[date_idx] if close_col else None
                                volume_entries.append({
                                    'date': date_str,
                                    'volume': float(volume),
                                    'close_price': float(close_price) if pd.notna(close_price) else None
                                })
                                
                        except Exception as row_error:
                            print(f"Error processing volume data point for {asset_id}: {str(row_error)}")
                            continue
                
                else:
                    # Handle regular columns
                    data_reset = data.reset_index()
                    
                    for _, row in data_reset.iterrows():
                        try:
                            # Get date
                            date_value = row['Date'] if 'Date' in row else row.index[0]
                            if hasattr(date_value, 'strftime'):
                                date_str = date_value.strftime('%Y-%m-%d')
                            else:
                                date_str = str(date_value)[:10]
                            
                            # Get volume and close price
                            volume = row['Volume'] if 'Volume' in row else None
                            close_price = row['Close'] if 'Close' in row else None
                            
                            if volume is not None and pd.notna(volume) and volume > 0:
                                volume_entries.append({
                                    'date': date_str,
                                    'volume': float(volume),
                                    'close_price': float(close_price) if pd.notna(close_price) else None
                                })
                                
                        except Exception as row_error:
                            print(f"Error processing volume row for {asset_id}: {str(row_error)}")
                            continue
                
                # Sort by date for proper calculation
                volume_entries.sort(key=lambda x: x['date'])
                
                # Calculate relative volumes
                volumes_only = [entry['volume'] for entry in volume_entries]
                relative_volumes = calculate_relative_volume(volumes_only, lookback_period=20)
                
                # Filter to requested date range (remove extra lookback days)
                current_date = datetime.now()
                cutoff_date = (current_date - timedelta(days=days)).strftime('%Y-%m-%d')
                
                filtered_data = []
                for i, entry in enumerate(volume_entries):
                    if entry['date'] >= cutoff_date:
                        filtered_data.append({
                            'date': entry['date'],
                            'volume': entry['volume'],
                            'volume_ratio': relative_volumes[i],
                            'close_price': entry['close_price']
                        })
                
                volume_data[asset_id] = filtered_data
                print(f"Processed {len(filtered_data)} volume data points for {asset_id}")
                        
        except Exception as e:
            print(f"Error fetching volume data for {asset_id}: {str(e)}")
            import traceback
            print(traceback.format_exc())
            volume_data[asset_id] = []
    
    return volume_data

def get_forex_data(forex_pairs, date_range):
    """
    Fetch forex data for specified pairs and date range
    """
    range_days = {
        '7d': 7,
        '30d': 30,
        '90d': 90,
        '180d': 180,
        '365d': 365
    }
    
    days = range_days.get(date_range, 30)
    
    # Determine interval based on date range
    if days <= 7:
        interval = '1h'
    elif days <= 30:
        interval = '1d'
    else:
        interval = '1d'
    
    forex_data = {}
    
    for pair in forex_pairs:
        try:
            print(f"Fetching forex data for {pair} with {days} days, interval {interval}")
            
            # Use obtain_dataset function
            data = obtain_dataset(pair, interval, days)
            
            if not data.empty:
                print(f"Got {len(data)} data points for {pair}")
                
                # Process the data
                forex_data[pair] = []
                
                # Handle MultiIndex columns - flatten them
                if isinstance(data.columns, pd.MultiIndex):
                    close_col = None
                    for col in data.columns:
                        if col[0] == 'Close':
                            close_col = col
                            break
                    
                    if close_col is None:
                        print(f"No Close column found in MultiIndex columns for {pair}")
                        forex_data[pair] = []
                        continue
                        
                    # Extract the close prices and dates
                    for date_idx, close_price in data[close_col].items():
                        try:
                            if hasattr(date_idx, 'strftime'):
                                date_str = date_idx.strftime('%Y-%m-%d')
                            else:
                                date_str = str(date_idx)[:10]
                            
                            if pd.notna(close_price):
                                forex_data[pair].append({
                                    'date': date_str,
                                    'price': float(close_price)
                                })
                                
                        except Exception as row_error:
                            print(f"Error processing data point for {pair}: {str(row_error)}")
                            continue
                
                else:
                    # Handle regular columns
                    data_reset = data.reset_index()
                    
                    for _, row in data_reset.iterrows():
                        try:
                            # Get date
                            date_value = row['Date'] if 'Date' in row else row.index[0]
                            if hasattr(date_value, 'strftime'):
                                date_str = date_value.strftime('%Y-%m-%d')
                            else:
                                date_str = str(date_value)[:10]
                            
                            # Get close price
                            close_price = row['Close'] if 'Close' in row else None
                            
                            if close_price is not None and pd.notna(close_price):
                                forex_data[pair].append({
                                    'date': date_str,
                                    'price': float(close_price)
                                })
                                
                        except Exception as row_error:
                            print(f"Error processing row for {pair}: {str(row_error)}")
                            continue
                
                print(f"Processed {len(forex_data[pair])} valid data points for {pair}")
                        
        except Exception as e:
            print(f"Error fetching forex data for {pair}: {str(e)}")
            forex_data[pair] = []
    
    return forex_data

def get_stock_indices_data(stock_indices, date_range):
    """
    Fetch stock indices data for specified symbols and date range
    """
    range_days = {
        '7d': 7,
        '30d': 30,
        '90d': 90,
        '180d': 180,
        '365d': 365
    }
    
    days = range_days.get(date_range, 30)
    
    # Determine interval based on date range
    if days <= 7:
        interval = '1h'
    elif days <= 30:
        interval = '1d'
    else:
        interval = '1d'
    
    stock_data = {}
    
    for symbol in stock_indices:
        try:
            print(f"Fetching stock index data for {symbol} with {days} days, interval {interval}")
            
            # Use obtain_dataset function
            data = obtain_dataset(symbol, interval, days)
            
            if not data.empty:
                print(f"Got {len(data)} data points for {symbol}")
                
                # Process the data
                stock_data[symbol] = []
                
                # Handle MultiIndex columns
                if isinstance(data.columns, pd.MultiIndex):
                    close_col = None
                    for col in data.columns:
                        if col[0] == 'Close':
                            close_col = col
                            break
                    
                    if close_col is None:
                        print(f"No Close column found in MultiIndex columns for {symbol}")
                        stock_data[symbol] = []
                        continue
                        
                    # Extract the close prices and dates
                    for date_idx, close_price in data[close_col].items():
                        try:
                            if hasattr(date_idx, 'strftime'):
                                date_str = date_idx.strftime('%Y-%m-%d')
                            else:
                                date_str = str(date_idx)[:10]
                            
                            if pd.notna(close_price):
                                stock_data[symbol].append({
                                    'date': date_str,
                                    'value': float(close_price)
                                })
                                
                        except Exception as row_error:
                            print(f"Error processing data point for {symbol}: {str(row_error)}")
                            continue
                
                else:
                    # Handle regular columns
                    data_reset = data.reset_index()
                    
                    for _, row in data_reset.iterrows():
                        try:
                            # Get date
                            date_value = row['Date'] if 'Date' in row else row.index[0]
                            if hasattr(date_value, 'strftime'):
                                date_str = date_value.strftime('%Y-%m-%d')
                            else:
                                date_str = str(date_value)[:10]
                            
                            # Get close price
                            close_price = row['Close'] if 'Close' in row else None
                            
                            if close_price is not None and pd.notna(close_price):
                                stock_data[symbol].append({
                                    'date': date_str,
                                    'value': float(close_price)
                                })
                                
                        except Exception as row_error:
                            print(f"Error processing row for {symbol}: {str(row_error)}")
                            continue
                
                print(f"Processed {len(stock_data[symbol])} valid data points for {symbol}")
                        
        except Exception as e:
            print(f"Error fetching stock index data for {symbol}: {str(e)}")
            import traceback
            print(traceback.format_exc())
            stock_data[symbol] = []
    
    print(f"Final stock_data keys: {list(stock_data.keys())}")
    for symbol, data in stock_data.items():
        print(f"{symbol}: {len(data)} data points")
    
    return stock_data

# ============================================
# ONLY THE NEW/UPDATED FUNCTIONS FOR BACKEND
# ============================================

# 1. NEW FUNCTION: Add this after get_stock_indices_data()
def get_commodity_data(commodities, date_range):
    """
    Fetch commodity price data for specified symbols and date range
    Commodities use yfinance symbols with =F suffix (futures)
    """
    range_days = {
        '7d': 7,
        '30d': 30,
        '90d': 90,
        '180d': 180,
        '365d': 365
    }
    
    days = range_days.get(date_range, 30)
    
    # Determine interval based on date range
    if days <= 7:
        interval = '1h'
    elif days <= 30:
        interval = '1d'
    else:
        interval = '1d'
    
    commodity_data = {}
    
    for symbol in commodities:
        try:
            print(f"Fetching commodity data for {symbol} with {days} days, interval {interval}")
            
            # Use obtain_dataset function (commodities already have =F suffix)
            data = obtain_dataset(symbol, interval, days)
            
            if not data.empty:
                print(f"Got {len(data)} data points for {symbol}")
                
                # Process the data
                commodity_data[symbol] = []
                
                # Handle MultiIndex columns
                if isinstance(data.columns, pd.MultiIndex):
                    close_col = None
                    for col in data.columns:
                        if col[0] == 'Close':
                            close_col = col
                            break
                    
                    if close_col is None:
                        print(f"No Close column found in MultiIndex columns for {symbol}")
                        commodity_data[symbol] = []
                        continue
                        
                    # Extract the close prices and dates
                    for date_idx, close_price in data[close_col].items():
                        try:
                            if hasattr(date_idx, 'strftime'):
                                date_str = date_idx.strftime('%Y-%m-%d')
                            else:
                                date_str = str(date_idx)[:10]
                            
                            if pd.notna(close_price):
                                commodity_data[symbol].append({
                                    'date': date_str,
                                    'price': float(close_price)
                                })
                                
                        except Exception as row_error:
                            print(f"Error processing data point for {symbol}: {str(row_error)}")
                            continue
                
                else:
                    # Handle regular columns
                    data_reset = data.reset_index()
                    
                    for _, row in data_reset.iterrows():
                        try:
                            # Get date
                            date_value = row['Date'] if 'Date' in row else row.index[0]
                            if hasattr(date_value, 'strftime'):
                                date_str = date_value.strftime('%Y-%m-%d')
                            else:
                                date_str = str(date_value)[:10]
                            
                            # Get close price
                            close_price = row['Close'] if 'Close' in row else None
                            
                            if close_price is not None and pd.notna(close_price):
                                commodity_data[symbol].append({
                                    'date': date_str,
                                    'price': float(close_price)
                                })
                                
                        except Exception as row_error:
                            print(f"Error processing row for {symbol}: {str(row_error)}")
                            continue
                
                print(f"Processed {len(commodity_data[symbol])} valid data points for {symbol}")
                        
        except Exception as e:
            print(f"Error fetching commodity data for {symbol}: {str(e)}")
            import traceback
            print(traceback.format_exc())
            commodity_data[symbol] = []
    
    print(f"Final commodity_data keys: {list(commodity_data.keys())}")
    for symbol, data in commodity_data.items():
        print(f"{symbol}: {len(data)} data points")
    
    return commodity_data


# 2. UPDATED FUNCTION: Replace your existing merge_multi_asset_data() with this
def merge_multi_asset_data(esi_data, forex_data, stock_data, volume_data, commodity_data):
    """
    Merge ESI data with forex price data, stock indices data, volume data, and commodity data
    """
    # Create a comprehensive date-based dictionary using YYYY-MM-DD as keys
    merged_data = {}
    
    # First, convert ESI dates to YYYY-MM-DD format for consistent matching
    esi_date_map = {}  # Maps YYYY-MM-DD to MM/DD display format
    
    for point in esi_data:
        display_date = point['date']  # This is MM/DD format
        
        # Try to convert MM/DD to YYYY-MM-DD for matching
        try:
            current_year = datetime.now().year
            if '/' in display_date:
                month_day = display_date
                # Try current year first, then previous year if needed
                for year_offset in [0, -1]:
                    try:
                        test_year = current_year + year_offset
                        full_date_str = f"{month_day}/{test_year}"
                        date_obj = datetime.strptime(full_date_str, '%m/%d/%Y')
                        iso_date = date_obj.strftime('%Y-%m-%d')
                        
                        # Store the mapping
                        esi_date_map[iso_date] = display_date
                        
                        # Store ESI data
                        if iso_date not in merged_data:
                            merged_data[iso_date] = {'date': display_date}
                        
                        # Add all ESI values
                        for key, value in point.items():
                            if key != 'date':
                                merged_data[iso_date][key] = value
                        break
                    except ValueError:
                        continue
            else:
                # If it's already in a different format, store as-is
                merged_data[display_date] = point.copy()
        except Exception as e:
            print(f"Error processing ESI date {display_date}: {str(e)}")
            # Fallback: use original date as key
            merged_data[display_date] = point.copy()
    
    # Create a complete date range for asset data interpolation
    all_asset_dates = set()
    
    # Add forex dates
    for pair, price_data in forex_data.items():
        for price_point in price_data:
            all_asset_dates.add(price_point['date'])
    
    # Add stock index dates
    for symbol, index_data in stock_data.items():
        for index_point in index_data:
            all_asset_dates.add(index_point['date'])
    
    # Add volume dates
    for asset_id, vol_data in volume_data.items():
        for vol_point in vol_data:
            all_asset_dates.add(vol_point['date'])
    
    # Add commodity dates
    for symbol, commodity_price_data in commodity_data.items():
        for commodity_point in commodity_price_data:
            all_asset_dates.add(commodity_point['date'])
    
    # Sort dates for interpolation
    sorted_asset_dates = sorted(all_asset_dates)
    
    # Add forex data with interpolation for missing values
    for pair, price_data in forex_data.items():
        forex_prices_by_date = {point['date']: point['price'] for point in price_data}
        
        all_merged_dates = list(merged_data.keys())
        
        for date_key in all_merged_dates:
            try:
                if date_key in forex_prices_by_date:
                    merged_data[date_key][f"{pair}_price"] = forex_prices_by_date[date_key]
                else:
                    # Interpolation logic for forex
                    try:
                        target_date = datetime.strptime(date_key, '%Y-%m-%d')
                    except:
                        continue
                        
                    closest_before = None
                    closest_after = None
                    closest_before_price = None
                    closest_after_price = None
                    
                    for forex_date_str, price in forex_prices_by_date.items():
                        try:
                            forex_date = datetime.strptime(forex_date_str, '%Y-%m-%d')
                            
                            if forex_date <= target_date:
                                if closest_before is None or forex_date > closest_before:
                                    closest_before = forex_date
                                    closest_before_price = price
                            
                            if forex_date >= target_date:
                                if closest_after is None or forex_date < closest_after:
                                    closest_after = forex_date
                                    closest_after_price = price
                        except:
                            continue
                    
                    # Use interpolation or nearest value
                    if closest_before_price is not None and closest_after_price is not None and closest_before != closest_after:
                        time_diff = (closest_after - closest_before).days
                        target_diff = (target_date - closest_before).days
                        
                        if time_diff > 0:
                            weight = target_diff / time_diff
                            interpolated_price = closest_before_price + (closest_after_price - closest_before_price) * weight
                            merged_data[date_key][f"{pair}_price"] = interpolated_price
                        else:
                            merged_data[date_key][f"{pair}_price"] = closest_before_price
                    elif closest_before_price is not None:
                        merged_data[date_key][f"{pair}_price"] = closest_before_price
                    elif closest_after_price is not None:
                        merged_data[date_key][f"{pair}_price"] = closest_after_price
                        
            except Exception as e:
                print(f"Error processing forex data for {date_key}: {str(e)}")
                continue
        
        # Add forex-only dates
        for forex_date_str, price in forex_prices_by_date.items():
            if forex_date_str not in merged_data:
                try:
                    date_obj = datetime.strptime(forex_date_str, '%Y-%m-%d')
                    display_date = date_obj.strftime('%m/%d')
                    
                    merged_data[forex_date_str] = {
                        'date': display_date,
                        f"{pair}_price": price
                    }
                except Exception as e:
                    print(f"Error adding forex-only date {forex_date_str}: {str(e)}")
                    continue
    
    # Add stock index data with interpolation
    for symbol, index_data in stock_data.items():
        stock_values_by_date = {point['date']: point['value'] for point in index_data}
        
        all_merged_dates = list(merged_data.keys())
        
        for date_key in all_merged_dates:
            try:
                if date_key in stock_values_by_date:
                    merged_data[date_key][f"{symbol}_index"] = stock_values_by_date[date_key]
                else:
                    # Interpolation logic for stock indices
                    try:
                        target_date = datetime.strptime(date_key, '%Y-%m-%d')
                    except:
                        continue
                        
                    closest_before = None
                    closest_after = None
                    closest_before_value = None
                    closest_after_value = None
                    
                    for stock_date_str, value in stock_values_by_date.items():
                        try:
                            stock_date = datetime.strptime(stock_date_str, '%Y-%m-%d')
                            
                            if stock_date <= target_date:
                                if closest_before is None or stock_date > closest_before:
                                    closest_before = stock_date
                                    closest_before_value = value
                            
                            if stock_date >= target_date:
                                if closest_after is None or stock_date < closest_after:
                                    closest_after = stock_date
                                    closest_after_value = value
                        except:
                            continue
                    
                    # Use interpolation or nearest value
                    if closest_before_value is not None and closest_after_value is not None and closest_before != closest_after:
                        time_diff = (closest_after - closest_before).days
                        target_diff = (target_date - closest_before).days
                        
                        if time_diff > 0:
                            weight = target_diff / time_diff
                            interpolated_value = closest_before_value + (closest_after_value - closest_before_value) * weight
                            merged_data[date_key][f"{symbol}_index"] = interpolated_value
                        else:
                            merged_data[date_key][f"{symbol}_index"] = closest_before_value
                    elif closest_before_value is not None:
                        merged_data[date_key][f"{symbol}_index"] = closest_before_value
                    elif closest_after_value is not None:
                        merged_data[date_key][f"{symbol}_index"] = closest_after_value
                        
            except Exception as e:
                print(f"Error processing stock index data for {date_key}: {str(e)}")
                continue
        
        # Add stock-index-only dates
        for stock_date_str, value in stock_values_by_date.items():
            if stock_date_str not in merged_data:
                try:
                    date_obj = datetime.strptime(stock_date_str, '%Y-%m-%d')
                    display_date = date_obj.strftime('%m/%d')
                    
                    merged_data[stock_date_str] = {
                        'date': display_date,
                        f"{symbol}_index": value
                    }
                except Exception as e:
                    print(f"Error adding stock-index-only date {stock_date_str}: {str(e)}")
                    continue
    
    # Add volume data with interpolation
    for asset_id, vol_data in volume_data.items():
        volume_ratios_by_date = {point['date']: point['volume_ratio'] for point in vol_data}
        
        all_merged_dates = list(merged_data.keys())
        
        for date_key in all_merged_dates:
            try:
                if date_key in volume_ratios_by_date:
                    merged_data[date_key][f"{asset_id}_volume_ratio"] = volume_ratios_by_date[date_key]
                else:
                    # Interpolation logic for volume ratios
                    try:
                        target_date = datetime.strptime(date_key, '%Y-%m-%d')
                    except:
                        continue
                        
                    closest_before = None
                    closest_after = None
                    closest_before_ratio = None
                    closest_after_ratio = None
                    
                    for vol_date_str, ratio in volume_ratios_by_date.items():
                        try:
                            vol_date = datetime.strptime(vol_date_str, '%Y-%m-%d')
                            
                            if vol_date <= target_date:
                                if closest_before is None or vol_date > closest_before:
                                    closest_before = vol_date
                                    closest_before_ratio = ratio
                            
                            if vol_date >= target_date:
                                if closest_after is None or vol_date < closest_after:
                                    closest_after = vol_date
                                    closest_after_ratio = ratio
                        except:
                            continue
                    
                    # Use interpolation or nearest value
                    if closest_before_ratio is not None and closest_after_ratio is not None and closest_before != closest_after:
                        time_diff = (closest_after - closest_before).days
                        target_diff = (target_date - closest_before).days
                        
                        if time_diff > 0:
                            weight = target_diff / time_diff
                            interpolated_ratio = closest_before_ratio + (closest_after_ratio - closest_before_ratio) * weight
                            merged_data[date_key][f"{asset_id}_volume_ratio"] = interpolated_ratio
                        else:
                            merged_data[date_key][f"{asset_id}_volume_ratio"] = closest_before_ratio
                    elif closest_before_ratio is not None:
                        merged_data[date_key][f"{asset_id}_volume_ratio"] = closest_before_ratio
                    elif closest_after_ratio is not None:
                        merged_data[date_key][f"{asset_id}_volume_ratio"] = closest_after_ratio
                        
            except Exception as e:
                print(f"Error processing volume data for {date_key}: {str(e)}")
                continue
        
        # Add volume-only dates
        for vol_date_str, ratio in volume_ratios_by_date.items():
            if vol_date_str not in merged_data:
                try:
                    date_obj = datetime.strptime(vol_date_str, '%Y-%m-%d')
                    display_date = date_obj.strftime('%m/%d')
                    
                    merged_data[vol_date_str] = {
                        'date': display_date,
                        f"{asset_id}_volume_ratio": ratio
                    }
                except Exception as e:
                    print(f"Error adding volume-only date {vol_date_str}: {str(e)}")
                    continue
    
    # NEW: Add commodity data with interpolation
    for symbol, commodity_price_data in commodity_data.items():
        commodity_values_by_date = {point['date']: point['price'] for point in commodity_price_data}
        
        all_merged_dates = list(merged_data.keys())
        
        for date_key in all_merged_dates:
            try:
                if date_key in commodity_values_by_date:
                    merged_data[date_key][f"{symbol}_commodity"] = commodity_values_by_date[date_key]
                else:
                    # Interpolation logic for commodities
                    try:
                        target_date = datetime.strptime(date_key, '%Y-%m-%d')
                    except:
                        continue
                        
                    closest_before = None
                    closest_after = None
                    closest_before_value = None
                    closest_after_value = None
                    
                    for commodity_date_str, price in commodity_values_by_date.items():
                        try:
                            commodity_date = datetime.strptime(commodity_date_str, '%Y-%m-%d')
                            
                            if commodity_date <= target_date:
                                if closest_before is None or commodity_date > closest_before:
                                    closest_before = commodity_date
                                    closest_before_value = price
                            
                            if commodity_date >= target_date:
                                if closest_after is None or commodity_date < closest_after:
                                    closest_after = commodity_date
                                    closest_after_value = price
                        except:
                            continue
                    
                    # Use interpolation or nearest value
                    if closest_before_value is not None and closest_after_value is not None and closest_before != closest_after:
                        time_diff = (closest_after - closest_before).days
                        target_diff = (target_date - closest_before).days
                        
                        if time_diff > 0:
                            weight = target_diff / time_diff
                            interpolated_value = closest_before_value + (closest_after_value - closest_before_value) * weight
                            merged_data[date_key][f"{symbol}_commodity"] = interpolated_value
                        else:
                            merged_data[date_key][f"{symbol}_commodity"] = closest_before_value
                    elif closest_before_value is not None:
                        merged_data[date_key][f"{symbol}_commodity"] = closest_before_value
                    elif closest_after_value is not None:
                        merged_data[date_key][f"{symbol}_commodity"] = closest_after_value
                        
            except Exception as e:
                print(f"Error processing commodity data for {date_key}: {str(e)}")
                continue
        
        # Add commodity-only dates
        for commodity_date_str, price in commodity_values_by_date.items():
            if commodity_date_str not in merged_data:
                try:
                    date_obj = datetime.strptime(commodity_date_str, '%Y-%m-%d')
                    display_date = date_obj.strftime('%m/%d')
                    
                    merged_data[commodity_date_str] = {
                        'date': display_date,
                        f"{symbol}_commodity": price
                    }
                except Exception as e:
                    print(f"Error adding commodity-only date {commodity_date_str}: {str(e)}")
                    continue
    
    # Convert back to list format and sort by actual date
    merged_list = []
    sorted_dates = sorted(merged_data.keys(), key=lambda x: datetime.strptime(x, '%Y-%m-%d') if x.count('-') == 2 else datetime.now())
    
    for date_key in sorted_dates:
        data_point = merged_data[date_key].copy()
        merged_list.append(data_point)
    
    # Debug: Print sample of merged data
    print(f"Merged data sample: {merged_list[:3] if merged_list else 'No data'}")
    if commodity_data:
        print(f"Commodity data sample: {list(commodity_data.items())[0] if commodity_data else 'No commodity data'}")
    
    return merged_list


# 3. UPDATED FUNCTION: Replace your existing economic_strength_index() with this
@api_view(['POST'])
def economic_strength_index(request):
    """
    Calculate and return Economic Strength Index for selected currencies
    Now with forex, stock indices, volume, and commodity overlay capability
    """
    try:
        data = json.loads(request.body)
        currencies = data.get('currencies', ['USD'])
        forex_pairs = data.get('forex_pairs', [])
        stock_indices = data.get('stock_indices', [])
        volume_assets = data.get('volume_assets', [])
        commodities = data.get('commodities', [])  # NEW: Get commodities from request
        date_range = data.get('date_range', '30d')
        
        print(f"Received request: currencies={currencies}, forex={forex_pairs}, stocks={stock_indices}, volume={volume_assets}, commodities={commodities}, range={date_range}")
        
        # Calculate date range
        range_days = {
            '7d': 7,
            '30d': 30,
            '90d': 90,
            '180d': 180,
            '365d': 365
        }
        
        days = range_days.get(date_range, 30)
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        # Fetch economic events for selected currencies within date range
        events = EconomicEvent.objects.filter(
            currency__in=currencies,
            date_time__gte=start_date,
            date_time__lte=end_date,
            actual__isnull=False,
            forecast__isnull=False
        ).exclude(
            Q(actual='') | Q(forecast='')
        ).order_by('date_time')
        
        # Group events by currency and date
        currency_data = defaultdict(lambda: defaultdict(list))
        
        for event in events:
            date_key = event.date_time.date().isoformat()
            currency_data[event.currency][date_key].append(event)
        
        # Calculate daily ESI scores for each currency
        chart_data_dict = defaultdict(dict)
        
        # Get all unique dates across all currencies for consistency
        all_dates = set()
        for curr_data in currency_data.values():
            all_dates.update(curr_data.keys())
        
        # If no ESI dates, create a basic date range
        if not all_dates:
            current_date = start_date
            while current_date <= end_date:
                all_dates.add(current_date.strftime('%Y-%m-%d'))
                current_date += timedelta(days=1)
        
        sorted_dates = sorted(all_dates)
        
        for currency in currencies:
            daily_scores = []
            dates = []
            
            for date_str in sorted_dates:
                events_for_date = currency_data[currency].get(date_str, [])
                
                if events_for_date:
                    # Calculate weighted ESI score for this date
                    weighted_deviations = []
                    
                    for event in events_for_date:
                        deviation = calculate_percentage_deviation(event.actual, event.forecast)
                        weight = get_impact_weight(event.impact)
                        weighted_deviations.append(deviation * weight)
                    
                    # Average weighted deviations for the day
                    if weighted_deviations:
                        daily_score = np.mean(weighted_deviations)
                        daily_scores.append(daily_score)
                        dates.append(date_str)
                    else:
                        daily_scores.append(None)
                        dates.append(date_str)
                else:
                    daily_scores.append(None)
                    dates.append(date_str)
            
            # Fill gaps with interpolation BEFORE smoothing and normalization
            filled_scores = []
            for i, score in enumerate(daily_scores):
                if score is not None:
                    filled_scores.append(score)
                else:
                    # Find nearest non-null values for interpolation
                    before_idx = None
                    after_idx = None
                    before_score = None
                    after_score = None
                    
                    # Look backwards for nearest score
                    for j in range(i - 1, -1, -1):
                        if daily_scores[j] is not None:
                            before_idx = j
                            before_score = daily_scores[j]
                            break
                    
                    # Look forwards for nearest score
                    for j in range(i + 1, len(daily_scores)):
                        if daily_scores[j] is not None:
                            after_idx = j
                            after_score = daily_scores[j]
                            break
                    
                    # Interpolate or use nearest value
                    if before_score is not None and after_score is not None:
                        distance_total = after_idx - before_idx
                        distance_from_before = i - before_idx
                        weight = distance_from_before / distance_total if distance_total > 0 else 0
                        interpolated_score = before_score + (after_score - before_score) * weight
                        filled_scores.append(interpolated_score)
                    elif before_score is not None:
                        filled_scores.append(before_score)
                    elif after_score is not None:
                        filled_scores.append(after_score)
                    else:
                        filled_scores.append(0)
            
            daily_scores = filled_scores
            
            # Apply smoothing (7-day moving average) for cleaner visualization
            if len(daily_scores) > 7:
                smoothed_scores = []
                for i in range(len(daily_scores)):
                    start_idx = max(0, i - 3)
                    end_idx = min(len(daily_scores), i + 4)
                    window_scores = daily_scores[start_idx:end_idx]
                    smoothed_scores.append(np.mean(window_scores))
                daily_scores = smoothed_scores
            
            # Store data for each currency with dates
            for i, date_str in enumerate(dates):
                if i < len(daily_scores):
                    chart_data_dict[date_str][currency] = daily_scores[i]
        
        # Convert to chart format with proper date handling
        chart_data = []
        for date_str in sorted_dates:
            point = {'date': datetime.strptime(date_str, '%Y-%m-%d').strftime('%m/%d')}
            
            # Add ESI scores for each currency
            for currency in currencies:
                if currency in chart_data_dict[date_str]:
                    point[currency] = chart_data_dict[date_str][currency]
                else:
                    point[currency] = None
            
            chart_data.append(point)
        
        # Normalize ESI scores across all currencies to 0-100 scale
        all_scores = []
        for point in chart_data:
            for currency in currencies:
                if point.get(currency) is not None:
                    all_scores.append(point[currency])
        
        if all_scores:
            normalized_scores = normalize_esi_scores(all_scores)
            score_idx = 0
            
            for point in chart_data:
                for currency in currencies:
                    if point.get(currency) is not None:
                        point[currency] = normalized_scores[score_idx]
                        score_idx += 1
        
        print(f"Generated {len(chart_data)} ESI data points")
        
        # Fetch forex data if requested
        forex_data = {}
        if forex_pairs:
            forex_data = get_forex_data(forex_pairs, date_range)
            print(f"Fetched forex data for {len(forex_pairs)} pairs")
        
        # Fetch stock indices data if requested
        stock_data = {}
        if stock_indices:
            stock_data = get_stock_indices_data(stock_indices, date_range)
            print(f"Fetched stock data for {len(stock_indices)} indices")
        
        # Fetch volume data if requested
        volume_data = {}
        if volume_assets:
            volume_data = get_volume_data(volume_assets, date_range)
            print(f"Fetched volume data for {len(volume_assets)} assets")
        
        # NEW: Fetch commodity data if requested
        commodity_data = {}
        if commodities:
            commodity_data = get_commodity_data(commodities, date_range)
            print(f"Fetched commodity data for {len(commodities)} commodities")
        
        # Merge ESI data with forex, stock indices, volume, and commodity data
        if forex_data or stock_data or volume_data or commodity_data:
            merged_data = merge_multi_asset_data(chart_data, forex_data, stock_data, volume_data, commodity_data)
            print(f"Merged data contains {len(merged_data)} points")
        else:
            merged_data = chart_data
        
        # Calculate summary statistics
        summary_stats = {}
        for currency in currencies:
            currency_scores = [point.get(currency) for point in merged_data if point.get(currency) is not None]
            if currency_scores:
                summary_stats[currency] = {
                    'average': np.mean(currency_scores),
                    'current': currency_scores[-1] if currency_scores else None,
                    'trend': 'positive' if len(currency_scores) > 1 and currency_scores[-1] > currency_scores[0] else 'negative',
                    'volatility': np.std(currency_scores)
                }
        
        return Response({
            'success': True,
            'chart_data': merged_data,
            'summary': summary_stats,
            'metadata': {
                'date_range': date_range,
                'currencies': currencies,
                'forex_pairs': forex_pairs,
                'stock_indices': stock_indices,
                'volume_assets': volume_assets,
                'commodities': commodities,  # NEW: Include commodities in metadata
                'data_points': len(merged_data),
                'events_processed': events.count()
            }
        })
        
    except Exception as e:
        print(f"Error in economic_strength_index: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return Response({
            'error': str(e),
            'success': False
        }, status=500)

from django.shortcuts import render
from django.core.paginator import Paginator
from django.db.models import Q, Count, Avg, Max, Min

logger = logging.getLogger(__name__)

@csrf_exempt
@require_http_methods(["GET", "POST"])
def snowai_research_logbook_api_entries(request):
    """
    GET: Retrieve ML model entries with filtering and pagination
    POST: Create new ML model entry
    """
    try:
        if request.method == 'GET':
            return snowai_get_ml_entries(request)
        elif request.method == 'POST':
            return snowai_create_ml_entry(request)
    except Exception as e:
        logger.error(f"SnowAI Research Logbook API error: {str(e)}")
        return JsonResponse({'error': 'Internal server error'}, status=500)

def snowai_get_ml_entries(request):
    """Get ML entries with filtering and search"""
    # Get query parameters
    snowai_search_query = request.GET.get('search', '').strip()
    snowai_model_type = request.GET.get('model_type', '')
    snowai_status = request.GET.get('status', '')
    snowai_market_type = request.GET.get('market_type', '')
    snowai_tags_filter = request.GET.get('tags', '')
    snowai_page = int(request.GET.get('page', 1))
    snowai_per_page = int(request.GET.get('per_page', 12))
    snowai_sort_by = request.GET.get('sort_by', '-snowai_created_at')
    
    # Build queryset
    queryset = SnowAIMLModelLogEntry.objects.all()
    
    # Apply filters
    if snowai_search_query:
        queryset = queryset.filter(
            Q(snowai_model_name__icontains=snowai_search_query) |
            Q(snowai_description__icontains=snowai_search_query) |
            Q(snowai_tags__icontains=snowai_search_query) |
            Q(snowai_notes__icontains=snowai_search_query)
        )
    
    if snowai_model_type:
        queryset = queryset.filter(snowai_model_type=snowai_model_type)
    
    if snowai_status:
        queryset = queryset.filter(snowai_status=snowai_status)
    
    if snowai_market_type:
        queryset = queryset.filter(snowai_financial_market_type=snowai_market_type)
    
    if snowai_tags_filter:
        for tag in snowai_tags_filter.split(','):
            queryset = queryset.filter(snowai_tags__icontains=tag.strip())
    
    # Apply sorting
    if snowai_sort_by in ['snowai_created_at', '-snowai_created_at', 'snowai_model_name', '-snowai_model_name', 
                         'snowai_accuracy_score', '-snowai_accuracy_score', 'snowai_r2_score', '-snowai_r2_score']:
        queryset = queryset.order_by(snowai_sort_by)
    
    # Paginate
    paginator = Paginator(queryset, snowai_per_page)
    snowai_page_obj = paginator.get_page(snowai_page)
    
    # Serialize data
    snowai_entries = []
    for entry in snowai_page_obj:
        snowai_entry_data = {
            'id': entry.id,
            'snowai_model_name': entry.snowai_model_name,
            'snowai_model_type': entry.snowai_model_type,
            'snowai_tags': entry.snowai_tags_list,
            'snowai_description': entry.snowai_description,
            'snowai_created_at': entry.snowai_created_at.isoformat(),
            'snowai_updated_at': entry.snowai_updated_at.isoformat(),
            'snowai_status': entry.snowai_status,
            'snowai_financial_market_type': entry.snowai_financial_market_type,
            'snowai_dataset_name': entry.snowai_dataset_name,
            'snowai_framework_used': entry.snowai_framework_used,
            
            # Metrics
            'snowai_accuracy_score': entry.snowai_accuracy_score,
            'snowai_precision_score': entry.snowai_precision_score,
            'snowai_recall_score': entry.snowai_recall_score,
            'snowai_f1_score': entry.snowai_f1_score,
            'snowai_mae_score': entry.snowai_mae_score,
            'snowai_mse_score': entry.snowai_mse_score,
            'snowai_rmse_score': entry.snowai_rmse_score,
            'snowai_r2_score': entry.snowai_r2_score,
            'snowai_auc_score': entry.snowai_auc_score,
            
            # Financial metrics
            'snowai_profit_loss': entry.snowai_profit_loss,
            'snowai_sharpe_ratio': entry.snowai_sharpe_ratio,
            'snowai_max_drawdown': entry.snowai_max_drawdown,
            'snowai_win_rate': entry.snowai_win_rate,
            'snowai_roi_percentage': entry.snowai_roi_percentage,
            
            # Training info
            'snowai_training_duration': entry.snowai_training_duration,
            'snowai_epochs_trained': entry.snowai_epochs_trained,
            
            # Primary metric for display
            'snowai_primary_metric': entry.snowai_get_primary_metric(),
        }
        snowai_entries.append(snowai_entry_data)
    
    return JsonResponse({
        'entries': snowai_entries,
        'pagination': {
            'current_page': snowai_page_obj.number,
            'total_pages': paginator.num_pages,
            'total_entries': paginator.count,
            'has_next': snowai_page_obj.has_next(),
            'has_previous': snowai_page_obj.has_previous(),
        }
    })

def snowai_create_ml_entry(request):
    """Create new ML model entry"""
    try:
        snowai_data = json.loads(request.body)
        
        # Create new entry
        snowai_entry = SnowAIMLModelLogEntry.objects.create(
            snowai_model_name=snowai_data.get('snowai_model_name', ''),
            snowai_model_type=snowai_data.get('snowai_model_type', 'other'),
            snowai_tags=', '.join(snowai_data.get('snowai_tags', [])) if snowai_data.get('snowai_tags') else '',
            snowai_description=snowai_data.get('snowai_description', ''),
            snowai_code_used=snowai_data.get('snowai_code_used', ''),
            snowai_colab_notebook_url=snowai_data.get('snowai_colab_notebook_url', ''),
            snowai_framework_used=snowai_data.get('snowai_framework_used', ''),
            
            # Dataset info
            snowai_dataset_name=snowai_data.get('snowai_dataset_name', ''),
            snowai_dataset_description=snowai_data.get('snowai_dataset_description', ''),
            snowai_dataset_size=snowai_data.get('snowai_dataset_size'),
            snowai_dataset_features=snowai_data.get('snowai_dataset_features'),
            snowai_dataset_source=snowai_data.get('snowai_dataset_source', ''),
            snowai_financial_market_type=snowai_data.get('snowai_financial_market_type', ''),
            
            # Metrics
            snowai_accuracy_score=snowai_data.get('snowai_accuracy_score'),
            snowai_precision_score=snowai_data.get('snowai_precision_score'),
            snowai_recall_score=snowai_data.get('snowai_recall_score'),
            snowai_f1_score=snowai_data.get('snowai_f1_score'),
            snowai_mae_score=snowai_data.get('snowai_mae_score'),
            snowai_mse_score=snowai_data.get('snowai_mse_score'),
            snowai_rmse_score=snowai_data.get('snowai_rmse_score'),
            snowai_r2_score=snowai_data.get('snowai_r2_score'),
            snowai_auc_score=snowai_data.get('snowai_auc_score'),
            snowai_custom_metrics=snowai_data.get('snowai_custom_metrics'),
            
            # Training info
            snowai_training_duration=snowai_data.get('snowai_training_duration'),
            snowai_epochs_trained=snowai_data.get('snowai_epochs_trained'),
            snowai_batch_size=snowai_data.get('snowai_batch_size'),
            snowai_learning_rate=snowai_data.get('snowai_learning_rate'),
            snowai_optimizer_used=snowai_data.get('snowai_optimizer_used', ''),
            
            # Financial metrics
            snowai_profit_loss=snowai_data.get('snowai_profit_loss'),
            snowai_sharpe_ratio=snowai_data.get('snowai_sharpe_ratio'),
            snowai_max_drawdown=snowai_data.get('snowai_max_drawdown'),
            snowai_win_rate=snowai_data.get('snowai_win_rate'),
            snowai_roi_percentage=snowai_data.get('snowai_roi_percentage'),
            
            # Metadata
            snowai_status=snowai_data.get('snowai_status', 'experimental'),
            snowai_notes=snowai_data.get('snowai_notes', ''),
        )
        
        return JsonResponse({
            'success': True,
            'id': snowai_entry.id,
            'message': 'ML model entry created successfully'
        }, status=201)
        
    except json.JSONDecodeError:
        return JsonResponse({'error': 'Invalid JSON data'}, status=400)
    except Exception as e:
        logger.error(f"Error creating ML entry: {str(e)}")
        return JsonResponse({'error': str(e)}, status=500)

@csrf_exempt
@require_http_methods(["GET", "PUT", "DELETE"])
def snowai_research_logbook_api_entry_detail(request, entry_id):
    """
    GET: Retrieve single ML model entry
    PUT: Update ML model entry
    DELETE: Delete ML model entry
    """
    try:
        snowai_entry = SnowAIMLModelLogEntry.objects.get(id=entry_id)
        
        if request.method == 'GET':
            snowai_entry_data = {
                'id': snowai_entry.id,
                'snowai_model_name': snowai_entry.snowai_model_name,
                'snowai_model_type': snowai_entry.snowai_model_type,
                'snowai_tags': snowai_entry.snowai_tags_list,
                'snowai_description': snowai_entry.snowai_description,
                'snowai_code_used': snowai_entry.snowai_code_used,
                'snowai_colab_notebook_url': snowai_entry.snowai_colab_notebook_url,
                'snowai_framework_used': snowai_entry.snowai_framework_used,
                'snowai_created_at': snowai_entry.snowai_created_at.isoformat(),
                'snowai_updated_at': snowai_entry.snowai_updated_at.isoformat(),
                'snowai_status': snowai_entry.snowai_status,
                'snowai_notes': snowai_entry.snowai_notes,
                
                # Dataset info
                'snowai_dataset_name': snowai_entry.snowai_dataset_name,
                'snowai_dataset_description': snowai_entry.snowai_dataset_description,
                'snowai_dataset_size': snowai_entry.snowai_dataset_size,
                'snowai_dataset_features': snowai_entry.snowai_dataset_features,
                'snowai_dataset_source': snowai_entry.snowai_dataset_source,
                'snowai_financial_market_type': snowai_entry.snowai_financial_market_type,
                
                # All metrics
                'snowai_accuracy_score': snowai_entry.snowai_accuracy_score,
                'snowai_precision_score': snowai_entry.snowai_precision_score,
                'snowai_recall_score': snowai_entry.snowai_recall_score,
                'snowai_f1_score': snowai_entry.snowai_f1_score,
                'snowai_mae_score': snowai_entry.snowai_mae_score,
                'snowai_mse_score': snowai_entry.snowai_mse_score,
                'snowai_rmse_score': snowai_entry.snowai_rmse_score,
                'snowai_r2_score': snowai_entry.snowai_r2_score,
                'snowai_auc_score': snowai_entry.snowai_auc_score,
                'snowai_custom_metrics': snowai_entry.snowai_custom_metrics,
                
                # Training info
                'snowai_training_duration': snowai_entry.snowai_training_duration,
                'snowai_epochs_trained': snowai_entry.snowai_epochs_trained,
                'snowai_batch_size': snowai_entry.snowai_batch_size,
                'snowai_learning_rate': snowai_entry.snowai_learning_rate,
                'snowai_optimizer_used': snowai_entry.snowai_optimizer_used,
                
                # Financial metrics
                'snowai_profit_loss': snowai_entry.snowai_profit_loss,
                'snowai_sharpe_ratio': snowai_entry.snowai_sharpe_ratio,
                'snowai_max_drawdown': snowai_entry.snowai_max_drawdown,
                'snowai_win_rate': snowai_entry.snowai_win_rate,
                'snowai_roi_percentage': snowai_entry.snowai_roi_percentage,
            }
            return JsonResponse(snowai_entry_data)
            
        elif request.method == 'PUT':
            try:
                snowai_data = json.loads(request.body)
                
                # Update basic model information
                if 'snowai_model_name' in snowai_data:
                    snowai_entry.snowai_model_name = snowai_data['snowai_model_name']
                if 'snowai_model_type' in snowai_data:
                    snowai_entry.snowai_model_type = snowai_data['snowai_model_type']
                if 'snowai_description' in snowai_data:
                    snowai_entry.snowai_description = snowai_data['snowai_description']
                if 'snowai_status' in snowai_data:
                    snowai_entry.snowai_status = snowai_data['snowai_status']
                if 'snowai_notes' in snowai_data:
                    snowai_entry.snowai_notes = snowai_data['snowai_notes']
                
                # Update tags
                if 'snowai_tags' in snowai_data:
                    if isinstance(snowai_data['snowai_tags'], list):
                        snowai_entry.snowai_tags = ', '.join(snowai_data['snowai_tags'])
                    else:
                        snowai_entry.snowai_tags = snowai_data['snowai_tags']
                
                # Update code and implementation
                if 'snowai_code_used' in snowai_data:
                    snowai_entry.snowai_code_used = snowai_data['snowai_code_used']
                if 'snowai_colab_notebook_url' in snowai_data:
                    snowai_entry.snowai_colab_notebook_url = snowai_data['snowai_colab_notebook_url']
                if 'snowai_framework_used' in snowai_data:
                    snowai_entry.snowai_framework_used = snowai_data['snowai_framework_used']
                
                # Update dataset information
                if 'snowai_dataset_name' in snowai_data:
                    snowai_entry.snowai_dataset_name = snowai_data['snowai_dataset_name']
                if 'snowai_dataset_description' in snowai_data:
                    snowai_entry.snowai_dataset_description = snowai_data['snowai_dataset_description']
                if 'snowai_dataset_size' in snowai_data:
                    snowai_entry.snowai_dataset_size = snowai_data['snowai_dataset_size']
                if 'snowai_dataset_features' in snowai_data:
                    snowai_entry.snowai_dataset_features = snowai_data['snowai_dataset_features']
                if 'snowai_dataset_source' in snowai_data:
                    snowai_entry.snowai_dataset_source = snowai_data['snowai_dataset_source']
                if 'snowai_financial_market_type' in snowai_data:
                    snowai_entry.snowai_financial_market_type = snowai_data['snowai_financial_market_type']
                
                # Update performance metrics
                if 'snowai_accuracy_score' in snowai_data:
                    snowai_entry.snowai_accuracy_score = snowai_data['snowai_accuracy_score'] if snowai_data['snowai_accuracy_score'] else None
                if 'snowai_precision_score' in snowai_data:
                    snowai_entry.snowai_precision_score = snowai_data['snowai_precision_score'] if snowai_data['snowai_precision_score'] else None
                if 'snowai_recall_score' in snowai_data:
                    snowai_entry.snowai_recall_score = snowai_data['snowai_recall_score'] if snowai_data['snowai_recall_score'] else None
                if 'snowai_f1_score' in snowai_data:
                    snowai_entry.snowai_f1_score = snowai_data['snowai_f1_score'] if snowai_data['snowai_f1_score'] else None
                if 'snowai_mae_score' in snowai_data:
                    snowai_entry.snowai_mae_score = snowai_data['snowai_mae_score'] if snowai_data['snowai_mae_score'] else None
                if 'snowai_mse_score' in snowai_data:
                    snowai_entry.snowai_mse_score = snowai_data['snowai_mse_score'] if snowai_data['snowai_mse_score'] else None
                if 'snowai_rmse_score' in snowai_data:
                    snowai_entry.snowai_rmse_score = snowai_data['snowai_rmse_score'] if snowai_data['snowai_rmse_score'] else None
                if 'snowai_r2_score' in snowai_data:
                    snowai_entry.snowai_r2_score = snowai_data['snowai_r2_score'] if snowai_data['snowai_r2_score'] else None
                if 'snowai_auc_score' in snowai_data:
                    snowai_entry.snowai_auc_score = snowai_data['snowai_auc_score'] if snowai_data['snowai_auc_score'] else None
                if 'snowai_custom_metrics' in snowai_data:
                    snowai_entry.snowai_custom_metrics = snowai_data['snowai_custom_metrics']
                
                # Update training information
                if 'snowai_training_duration' in snowai_data:
                    snowai_entry.snowai_training_duration = snowai_data['snowai_training_duration'] if snowai_data['snowai_training_duration'] else None
                if 'snowai_epochs_trained' in snowai_data:
                    snowai_entry.snowai_epochs_trained = snowai_data['snowai_epochs_trained'] if snowai_data['snowai_epochs_trained'] else None
                if 'snowai_batch_size' in snowai_data:
                    snowai_entry.snowai_batch_size = snowai_data['snowai_batch_size'] if snowai_data['snowai_batch_size'] else None
                if 'snowai_learning_rate' in snowai_data:
                    snowai_entry.snowai_learning_rate = snowai_data['snowai_learning_rate'] if snowai_data['snowai_learning_rate'] else None
                if 'snowai_optimizer_used' in snowai_data:
                    snowai_entry.snowai_optimizer_used = snowai_data['snowai_optimizer_used']
                
                # Update financial metrics
                if 'snowai_profit_loss' in snowai_data:
                    snowai_entry.snowai_profit_loss = snowai_data['snowai_profit_loss'] if snowai_data['snowai_profit_loss'] else None
                if 'snowai_sharpe_ratio' in snowai_data:
                    snowai_entry.snowai_sharpe_ratio = snowai_data['snowai_sharpe_ratio'] if snowai_data['snowai_sharpe_ratio'] else None
                if 'snowai_max_drawdown' in snowai_data:
                    snowai_entry.snowai_max_drawdown = snowai_data['snowai_max_drawdown'] if snowai_data['snowai_max_drawdown'] else None
                if 'snowai_win_rate' in snowai_data:
                    snowai_entry.snowai_win_rate = snowai_data['snowai_win_rate'] if snowai_data['snowai_win_rate'] else None
                if 'snowai_roi_percentage' in snowai_data:
                    snowai_entry.snowai_roi_percentage = snowai_data['snowai_roi_percentage'] if snowai_data['snowai_roi_percentage'] else None
                
                # Save the updated entry
                snowai_entry.save()
                
                return JsonResponse({
                    'success': True,
                    'message': 'ML model entry updated successfully',
                    'id': snowai_entry.id
                })
                
            except json.JSONDecodeError:
                return JsonResponse({'error': 'Invalid JSON data'}, status=400)
            except ValueError as e:
                return JsonResponse({'error': f'Invalid data format: {str(e)}'}, status=400)
            
        elif request.method == 'DELETE':
            snowai_entry.delete()
            return JsonResponse({'success': True, 'message': 'Entry deleted successfully'})
            
    except SnowAIMLModelLogEntry.DoesNotExist:
        return JsonResponse({'error': 'Entry not found'}, status=404)
    except Exception as e:
        logger.error(f"Error in entry detail API: {str(e)}")
        return JsonResponse({'error': 'Internal server error'}, status=500)

        
@csrf_exempt
@require_http_methods(["GET"])
def snowai_research_logbook_api_analytics(request):
    """Get analytics and statistics for the research logbook"""
    try:
        # Basic counts
        snowai_total_entries = SnowAIMLModelLogEntry.objects.count()
        snowai_model_type_counts = SnowAIMLModelLogEntry.objects.values('snowai_model_type').annotate(count=Count('id'))
        snowai_status_counts = SnowAIMLModelLogEntry.objects.values('snowai_status').annotate(count=Count('id'))
        snowai_market_type_counts = SnowAIMLModelLogEntry.objects.values('snowai_financial_market_type').annotate(count=Count('id'))
        
        # Performance statistics
        snowai_accuracy_stats = SnowAIMLModelLogEntry.objects.filter(
            snowai_accuracy_score__isnull=False
        ).aggregate(
            avg=Avg('snowai_accuracy_score'),
            max=Max('snowai_accuracy_score'),
            min=Min('snowai_accuracy_score'),
            count=Count('snowai_accuracy_score')
        )
        
        snowai_r2_stats = SnowAIMLModelLogEntry.objects.filter(
            snowai_r2_score__isnull=False
        ).aggregate(
            avg=Avg('snowai_r2_score'),
            max=Max('snowai_r2_score'),
            min=Min('snowai_r2_score'),
            count=Count('snowai_r2_score')
        )
        
        # Financial performance stats
        snowai_roi_stats = SnowAIMLModelLogEntry.objects.filter(
            snowai_roi_percentage__isnull=False
        ).aggregate(
            avg=Avg('snowai_roi_percentage'),
            max=Max('snowai_roi_percentage'),
            min=Min('snowai_roi_percentage'),
            count=Count('snowai_roi_percentage')
        )
        
        # Recent activity
        snowai_recent_entries = SnowAIMLModelLogEntry.objects.order_by('-snowai_created_at')[:5]
        snowai_recent_data = []
        for entry in snowai_recent_entries:
            snowai_recent_data.append({
                'id': entry.id,
                'snowai_model_name': entry.snowai_model_name,
                'snowai_model_type': entry.snowai_model_type,
                'snowai_created_at': entry.snowai_created_at.isoformat(),
                'snowai_primary_metric': entry.snowai_get_primary_metric()
            })
        
        return JsonResponse({
            'snowai_total_entries': snowai_total_entries,
            'snowai_model_type_distribution': list(snowai_model_type_counts),
            'snowai_status_distribution': list(snowai_status_counts),
            'snowai_market_type_distribution': list(snowai_market_type_counts),
            'snowai_accuracy_statistics': snowai_accuracy_stats,
            'snowai_r2_statistics': snowai_r2_stats,
            'snowai_roi_statistics': snowai_roi_stats,
            'snowai_recent_entries': snowai_recent_data,
        })
        
    except Exception as e:
        logger.error(f"Error in analytics API: {str(e)}")
        return JsonResponse({'error': 'Internal server error'}, status=500)

@csrf_exempt  
@require_http_methods(["GET"])
def snowai_research_logbook_api_tags(request):
    """Get all unique tags used in the system"""
    try:
        snowai_all_entries = SnowAIMLModelLogEntry.objects.exclude(snowai_tags='').exclude(snowai_tags__isnull=True)
        snowai_all_tags = set()
        
        for entry in snowai_all_entries:
            snowai_all_tags.update(entry.snowai_tags_list)
        
        return JsonResponse({'snowai_tags': sorted(list(snowai_all_tags))})
        
    except Exception as e:
        logger.error(f"Error in tags API: {str(e)}")
        return JsonResponse({'error': 'Internal server error'}, status=500)
                

# Fixed Django Views - Remove the email override

@csrf_exempt
@require_http_methods(["GET"])
def check_fingerprint_status(request):
    """Check if fingerprint is registered in backend"""
    try:
        # Get email from request, fallback to your actual email
        email = request.GET.get('email', 'butterrobot83@gmail.com')
        # Remove this line that was overriding the email:
        # email = 'butterrobot83@gmail'
        
        domain = request.GET.get('domain', '')
        
        fingerprint_status, created = FingerprintStatus.objects.get_or_create(
            user_email=email,
            defaults={'is_registered': False, 'domain': domain}
        )
        
        # Add debugging info
        print(f"Checking fingerprint status for: {email}, Domain: {domain}, Registered: {fingerprint_status.is_registered}")
        
        return JsonResponse({
            'is_registered': fingerprint_status.is_registered,
            'domain': fingerprint_status.domain,
            'email_used': email,  # Add this for debugging
            'message': 'Fingerprint status retrieved successfully'
        })
    except Exception as e:
        print(f"Error checking fingerprint status: {str(e)}")
        return JsonResponse({'error': str(e)}, status=500)

@csrf_exempt
@require_http_methods(["POST"])
def register_fingerprint_backend(request):
    """Register fingerprint in backend after successful local registration"""
    try:
        data = json.loads(request.body)
        # Get email from request data, fallback to your actual email
        email = data.get('email', 'butterrobot83@gmail.com')
        # Remove this line that was overriding the email:
        # email = 'butterrobot83@gmail'
        
        domain = data.get('domain', '')
        
        fingerprint_status, created = FingerprintStatus.objects.get_or_create(
            user_email=email,
            defaults={'is_registered': True, 'domain': domain}
        )
        
        if not created:
            fingerprint_status.is_registered = True
            fingerprint_status.domain = domain
            fingerprint_status.save()
        
        print(f"Fingerprint registered for: {email}, Domain: {domain}, Created: {created}")
        
        return JsonResponse({
            'success': True,
            'is_registered': True,
            'email_used': email,  # Add this for debugging
            'message': 'Fingerprint registered successfully in backend'
        })
    except Exception as e:
        print(f"Error registering fingerprint: {str(e)}")
        return JsonResponse({'error': str(e)}, status=500)

@csrf_exempt
@require_http_methods(["POST"])
def reset_fingerprint_backend(request):
    """Reset fingerprint registration in backend"""
    try:
        data = json.loads(request.body)
        # Get email from request data, fallback to your actual email
        email = data.get('email', 'butterrobot83@gmail.com')
        # Remove this line that was overriding the email:
        # email = 'butterrobot83@gmail'
        
        try:
            fingerprint_status = FingerprintStatus.objects.get(user_email=email)
            fingerprint_status.is_registered = False
            fingerprint_status.domain = ''
            fingerprint_status.save()
            
            print(f"Fingerprint reset for: {email}")
            
            return JsonResponse({
                'success': True,
                'is_registered': False,
                'message': 'Fingerprint registration reset successfully'
            })
        except FingerprintStatus.DoesNotExist:
            print(f"No fingerprint status found for: {email}")
            return JsonResponse({'error': f'Fingerprint status not found for {email}'}, status=404)
            
    except Exception as e:
        print(f"Error resetting fingerprint: {str(e)}")
        return JsonResponse({'error': str(e)}, status=500)

# Add this new endpoint for debugging
@csrf_exempt
@require_http_methods(["GET"])
def debug_fingerprint_status(request):
    """Debug endpoint to see all fingerprint statuses"""
    try:
        all_statuses = FingerprintStatus.objects.all().values()
        return JsonResponse({
            'all_statuses': list(all_statuses),
            'count': len(all_statuses)
        })
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def snowai_trader_history_gpt_summary_endpoint(request):
    try:
        # Get trading data (not economic events)
        all_trades = AccountTrades.objects.all()
        accounts_data = Account.objects.all()
        
        if not all_trades.exists():
            return JsonResponse({
                'status': 'No trading data available',
                'summary': 'No trading history found to analyze.',
                'metrics': {}
            })
        
        # Calculate trading metrics
        total_trades = all_trades.count()
        profit_trades = all_trades.filter(outcome='Profit').count()
        loss_trades = all_trades.filter(outcome='Loss').count()
        win_rate = (profit_trades / total_trades * 100) if total_trades > 0 else 0
        
        total_pnl = all_trades.aggregate(Sum('amount'))['amount__sum'] or 0
        avg_trade_amount = all_trades.aggregate(Avg('amount'))['amount__avg'] or 0
        best_trade = all_trades.aggregate(Max('amount'))['amount__max'] or 0
        worst_trade = all_trades.aggregate(Min('amount'))['amount__min'] or 0
        
        # Strategy analysis
        strategy_performance = all_trades.values('strategy').annotate(
            total_trades=Count('id'),
            total_pnl=Sum('amount')
        ).order_by('-total_pnl')
        
        best_strategy = strategy_performance.first()['strategy'] if strategy_performance else 'N/A'
        worst_strategy = strategy_performance.last()['strategy'] if strategy_performance else 'N/A'
        
        # Asset analysis
        asset_counts = all_trades.values('asset').annotate(count=Count('id')).order_by('-count')
        most_traded_asset = asset_counts.first()['asset'] if asset_counts else 'N/A'
        
        # Get news data for context (if needed)
        major_assets = ['EURUSD', 'GBPUSD', 'USDJPY']
        try:
            news_data = fetch_news_data(major_assets, 'butterrobot83@gmail.com')
        except:
            news_data = {'message': []}
        
        # Create comprehensive prompt for GPT
        prompt = f"""
        Analyze this comprehensive trading performance data and provide a detailed, professional summary:

        TRADING PERFORMANCE METRICS:
        - Total Trades: {total_trades}
        - Win Rate: {win_rate:.2f}%
        - Total P&L: ${total_pnl:,.2f}
        - Average Trade Size: ${avg_trade_amount:,.2f}
        - Best Trade: ${best_trade:,.2f}
        - Worst Trade: ${worst_trade:,.2f}
        - Most Traded Asset: {most_traded_asset}
        - Best Performing Strategy: {best_strategy}
        - Worst Performing Strategy: {worst_strategy}

        DETAILED BREAKDOWN:
        - Profitable Trades: {profit_trades}
        - Losing Trades: {loss_trades}

        RECENT MARKET NEWS THEMES:
        {chr(10).join([f"- {item['asset']}: {item['title'][:100]}..." for item in news_data.get('message', [])[:5]])}

        Please provide:
        1. A comprehensive performance assessment
        2. Key strengths and weaknesses in the trading approach
        3. Risk management analysis
        4. Recommendations for improvement
        5. Strategic insights based on the data
        6. Asset allocation observations
        7. Future trading suggestions

        Format the response as a professional trading report with clear sections and actionable insights.
        """
        
        # Get AI summary
        ai_summary = chat_gpt(prompt)
        
        # Save to database
        summary_obj, created = SnowAITraderHistoryGPTSummary.objects.get_or_create(
            created_at__date=datetime.now().date(),
            defaults={
                'summary_text': ai_summary,
                'total_trades': total_trades,
                'win_rate': win_rate,
                'total_profit_loss': total_pnl,
                'best_performing_strategy': best_strategy,
                'worst_performing_strategy': worst_strategy,
                'most_traded_asset': most_traded_asset,
                'average_trade_amount': avg_trade_amount,
            }
        )
        
        if not created:
            # Update existing summary
            summary_obj.summary_text = ai_summary
            summary_obj.total_trades = total_trades
            summary_obj.win_rate = win_rate
            summary_obj.total_profit_loss = total_pnl
            summary_obj.best_performing_strategy = best_strategy
            summary_obj.worst_performing_strategy = worst_strategy
            summary_obj.most_traded_asset = most_traded_asset
            summary_obj.average_trade_amount = avg_trade_amount
            summary_obj.updated_at = datetime.now()
            summary_obj.save()
        
        return JsonResponse({
            'status': 'success',
            'summary': ai_summary,
            'metrics': {
                'total_trades': total_trades,
                'win_rate': f"{win_rate:.2f}%",
                'total_pnl': f"${total_pnl:,.2f}",
                'avg_trade_amount': f"${avg_trade_amount:,.2f}",
                'most_traded_asset': most_traded_asset,
                'best_strategy': best_strategy
            }
        })
        
    except Exception as e:
        print(f'Error in Trader History GPT Endpoint: {e}')
        return JsonResponse({'status': 'error', 'message': str(e)})


# @csrf_exempt
# @require_http_methods(["POST"])
# def snowai_macro_gpt_chat_endpoint(request):
#     try:
#         data = json.loads(request.body)
#         user_message = data.get('message', '')
        
#         if not user_message:
#             return JsonResponse({'status': 'error', 'message': 'No message provided'})
        
#         # Get recent macro context with more detailed information
#         recent_events = EconomicEvent.objects.filter(date_time__gte=datetime.now() - timedelta(days=7))
#         high_impact_recent = recent_events.filter(impact='high')
        
#         # Get some upcoming events too
#         upcoming_events = EconomicEvent.objects.filter(date_time__gt=datetime.now())[:5]
        
#         context_prompt = f"""
#         You are MacroGPT, an AI specialized in macro economic analysis, market trends, and economic event impact assessment.
        
#         Recent economic context (Last 7 days):
#         - Total events: {recent_events.count()}
#         - High impact events: {high_impact_recent.count()}
        
#         Recent high-impact events:
#         {chr(10).join([f"- {event.currency}: {event.event_name} ({event.impact} impact) - {event.date_time.strftime('%Y-%m-%d')}" for event in high_impact_recent[:5]])}
        
#         Upcoming events:
#         {chr(10).join([f"- {event.currency}: {event.event_name} - {event.date_time.strftime('%Y-%m-%d %H:%M')}" for event in upcoming_events])}
        
#         User question: {user_message}
        
#         Provide expert macro economic analysis and insights based on current market conditions and economic data. 
#         Be specific and actionable in your response. If the user asks about specific currencies or events, 
#         reference the available data context above.
#         """
        
#         ai_response = chat_gpt(context_prompt)
        
#         # Ensure we have a response
#         if not ai_response or ai_response.strip() == '':
#             ai_response = "I apologize, but I'm having trouble generating a response right now. Please try rephrasing your question about macro economic analysis."
        
#         SnowAIConversationHistory.objects.create(
#             gpt_system='MacroGPT',
#             user_message=user_message,
#             ai_response=ai_response
#         )
        
#         return JsonResponse({'status': 'success', 'response': ai_response})
        
#     except Exception as e:
#         print(f'Error in MacroGPT chat function: {e}')
#         return JsonResponse({'status': 'error', 'message': str(e)})


@csrf_exempt
@require_http_methods(["GET"])
def snowai_idea_gpt_summary_endpoint(request):
    try:
        all_ideas = IdeaModel.objects.all()
        trade_ideas = TradeIdea.objects.all()
        
        if not all_ideas.exists() and not trade_ideas.exists():
            return JsonResponse({
                'status': 'No ideas available',
                'summary': 'No ideas found in the system to analyze.',
                'metrics': {}
            })
        
        # Calculate metrics for regular ideas
        total_ideas = all_ideas.count()
        pending_ideas = all_ideas.filter(idea_tracker='Pending').count()
        in_progress_ideas = all_ideas.filter(idea_tracker='In Progress').count()
        completed_ideas = all_ideas.filter(idea_tracker='Completed').count()
        
        completion_rate = (completed_ideas / total_ideas * 100) if total_ideas > 0 else 0
        
        # Category analysis
        categories = all_ideas.values('idea_category').annotate(count=Count('id')).order_by('-count')
        most_common_category = categories.first()['idea_category'] if categories else 'N/A'
        
        # Trade ideas metrics
        total_trade_ideas = trade_ideas.count()
        pending_trade_ideas = trade_ideas.filter(trade_status='pending').count()
        executed_trade_ideas = trade_ideas.filter(trade_status='executed').count()
        
        # Get recent ideas for context
        recent_ideas = all_ideas.order_by('-created_at')[:10]
        recent_trade_ideas = trade_ideas.order_by('-date_created')[:5]
        
        oldest_pending = all_ideas.filter(idea_tracker='Pending').order_by('created_at').first()
        newest_idea = all_ideas.order_by('-created_at').first()
        
        prompt = f"""
        Analyze this comprehensive idea management data and provide detailed insights:

        GENERAL IDEAS ANALYSIS:
        - Total Ideas: {total_ideas}
        - Pending Ideas: {pending_ideas}
        - In Progress Ideas: {in_progress_ideas}
        - Completed Ideas: {completed_ideas}
        - Completion Rate: {completion_rate:.2f}%
        - Most Common Category: {most_common_category}

        TRADE IDEAS ANALYSIS:
        - Total Trade Ideas: {total_trade_ideas}
        - Pending Trade Ideas: {pending_trade_ideas}
        - Executed Trade Ideas: {executed_trade_ideas}

        RECENT IDEAS SAMPLE:
        {chr(10).join([f"- [{idea.idea_tracker}] {idea.idea_category}: {idea.idea_text[:100]}..." for idea in recent_ideas[:5]])}

        RECENT TRADE IDEAS:
        {chr(10).join([f"- [{trade.trade_status}] {trade.asset}: {trade.heading}" for trade in recent_trade_ideas])}

        Please provide:
        1. Comprehensive idea pipeline analysis
        2. Productivity and execution assessment
        3. Category-wise performance breakdown
        4. Bottleneck identification
        5. Recommendations for better idea management
        6. Trading idea conversion analysis
        7. Strategic prioritization suggestions
        8. Innovation and creativity assessment

        Format as a professional idea management report with actionable recommendations.
        """
        
        ai_summary = chat_gpt(prompt)
        
        summary_obj, created = SnowAIIdeaGPTSummary.objects.get_or_create(
            created_at__date=datetime.now().date(),
            defaults={
                'summary_text': ai_summary,
                'total_ideas': total_ideas,
                'pending_ideas': pending_ideas,
                'in_progress_ideas': in_progress_ideas,
                'completed_ideas': completed_ideas,
                'most_common_category': most_common_category,
                'completion_rate': completion_rate,
                'oldest_pending_idea': oldest_pending.idea_text[:200] if oldest_pending else 'N/A',
                'newest_idea': newest_idea.idea_text[:200] if newest_idea else 'N/A',
            }
        )
        
        if not created:
            summary_obj.summary_text = ai_summary
            summary_obj.total_ideas = total_ideas
            summary_obj.pending_ideas = pending_ideas
            summary_obj.in_progress_ideas = in_progress_ideas
            summary_obj.completed_ideas = completed_ideas
            summary_obj.completion_rate = completion_rate
            summary_obj.updated_at = datetime.now()
            summary_obj.save()
        
        return JsonResponse({
            'status': 'success',
            'summary': ai_summary,
            'metrics': {
                'total_ideas': total_ideas,
                'completion_rate': f"{completion_rate:.2f}%",
                'most_common_category': most_common_category,
                'pending_ideas': pending_ideas,
                'trade_ideas': total_trade_ideas
            }
        })
        
    except Exception as e:
        return JsonResponse({'status': 'error', 'message': str(e)})


# @csrf_exempt
# @require_http_methods(["POST"])
# def snowai_idea_gpt_chat_endpoint(request):
#     try:
#         data = json.loads(request.body)
#         user_message = data.get('message', '')
        
#         if not user_message:
#             return JsonResponse({'status': 'error', 'message': 'No message provided'})
        
#         recent_ideas = IdeaModel.objects.order_by('-created_at')[:10]
        
#         context_prompt = f"""
#         You are IdeaGPT, an AI specialized in idea management, creativity enhancement, and innovation strategy.
        
#         Recent ideas context:
#         - Total ideas in system: {IdeaModel.objects.count()}
#         - Recent ideas: {recent_ideas.count()}
        
#         Sample recent ideas:
#         {chr(10).join([f"- [{idea.idea_tracker}] {idea.idea_category}: {idea.idea_text[:100]}..." for idea in recent_ideas[:3]])}
        
#         User question: {user_message}
        
#         Provide creative and strategic insights for idea management, development, and execution.
#         """
        
#         ai_response = chat_gpt(context_prompt)
        
#         SnowAIConversationHistory.objects.create(
#             gpt_system='IdeaGPT',
#             user_message=user_message,
#             ai_response=ai_response
#         )
        
#         return JsonResponse({'status': 'success', 'response': ai_response})
        
#     except Exception as e:
#         return JsonResponse({'status': 'error', 'message': str(e)})


@csrf_exempt
@require_http_methods(["GET"])
def snowai_backtesting_gpt_summary_endpoint(request):
    try:
        all_backtests = BacktestModels.objects.all()
        all_results = BacktestResult.objects.all()
        
        if not all_backtests.exists():
            return JsonResponse({
                'status': 'No backtesting data available',
                'summary': 'No backtesting history found to analyze.',
                'metrics': {}
            })
        
        total_backtests = all_backtests.count()
        successful_backtests = all_backtests.filter(model_backtested=True).count()
        
        # Results analysis
        if all_results.exists():
            avg_sharpe = all_results.aggregate(Avg('sharpe_ratio'))['sharpe_ratio__avg'] or 0
            avg_annual_return = all_results.aggregate(Avg('annual_return'))['annual_return__avg'] or 0
            avg_max_drawdown = all_results.aggregate(Avg('max_drawdown'))['max_drawdown__avg'] or 0
            best_sharpe = all_results.aggregate(Max('sharpe_ratio'))['sharpe_ratio__max'] or 0
            worst_sharpe = all_results.aggregate(Min('sharpe_ratio'))['sharpe_ratio__min'] or 0
            
            best_result = all_results.filter(sharpe_ratio=best_sharpe).first()
            worst_result = all_results.filter(sharpe_ratio=worst_sharpe).first()
        else:
            avg_sharpe = avg_annual_return = avg_max_drawdown = 0
            best_result = worst_result = None
        
        # Dataset analysis
        datasets = all_backtests.values('chosen_dataset').annotate(count=Count('id')).order_by('-count')
        most_used_dataset = datasets.first()['chosen_dataset'] if datasets else 'N/A'
        
        # Recent backtests
        recent_backtests = all_backtests.order_by('-id')[:5]
        
        prompt = f"""
        Analyze this comprehensive backtesting performance data:

        BACKTESTING OVERVIEW:
        - Total Backtests: {total_backtests}
        - Successful Backtests: {successful_backtests}
        - Success Rate: {(successful_backtests/total_backtests*100) if total_backtests > 0 else 0:.2f}%
        - Most Used Dataset: {most_used_dataset}

        PERFORMANCE METRICS:
        - Average Sharpe Ratio: {avg_sharpe:.3f}
        - Average Annual Return: {avg_annual_return:.2f}%
        - Average Max Drawdown: {avg_max_drawdown:.2f}%
        - Best Sharpe Ratio: {best_sharpe:.3f}
        - Worst Sharpe Ratio: {worst_sharpe:.3f}

        RECENT BACKTESTS:
        {chr(10).join([f"- Dataset: {bt.chosen_dataset} | Period: {bt.dataset_start} to {bt.dataset_end} | Capital: ${bt.initial_capital:,.2f}" for bt in recent_backtests])}

        BEST PERFORMING STRATEGY:
        {f"Sharpe: {best_result.sharpe_ratio:.3f} | Annual Return: {best_result.annual_return:.2f}% | Drawdown: {best_result.max_drawdown:.2f}%" if best_result else "No results available"}

        WORST PERFORMING STRATEGY:
        {f"Sharpe: {worst_result.sharpe_ratio:.3f} | Annual Return: {worst_result.annual_return:.2f}% | Drawdown: {worst_result.max_drawdown:.2f}%" if worst_result else "No results available"}

        Please provide:
        1. Comprehensive backtesting performance assessment
        2. Strategy effectiveness analysis
        3. Risk-adjusted returns evaluation
        4. Dataset utilization insights
        5. Performance consistency analysis
        6. Recommendations for strategy improvement
        7. Risk management effectiveness
        8. Future backtesting suggestions

        Format as a professional quantitative analysis report.
        """
        
        ai_summary = chat_gpt(prompt)
        
        summary_obj, created = SnowAIBacktestingGPTSummary.objects.get_or_create(
            created_at__date=datetime.now().date(),
            defaults={
                'summary_text': ai_summary,
                'total_backtests': total_backtests,
                'successful_backtests': successful_backtests,
                'average_sharpe_ratio': avg_sharpe,
                'average_annual_return': avg_annual_return,
                'average_max_drawdown': avg_max_drawdown,
                'best_performing_strategy': f"Sharpe: {best_sharpe:.3f}" if best_result else 'N/A',
                'worst_performing_strategy': f"Sharpe: {worst_sharpe:.3f}" if worst_result else 'N/A',
                'most_used_dataset': most_used_dataset,
            }
        )
        
        if not created:
            summary_obj.summary_text = ai_summary
            summary_obj.total_backtests = total_backtests
            summary_obj.successful_backtests = successful_backtests
            summary_obj.average_sharpe_ratio = avg_sharpe
            summary_obj.updated_at = datetime.now()
            summary_obj.save()
        
        return JsonResponse({
            'status': 'success',
            'summary': ai_summary,
            'metrics': {
                'total_backtests': total_backtests,
                'success_rate': f"{(successful_backtests/total_backtests*100) if total_backtests > 0 else 0:.2f}%",
                'avg_sharpe_ratio': f"{avg_sharpe:.3f}",
                'avg_annual_return': f"{avg_annual_return:.2f}%",
                'most_used_dataset': most_used_dataset
            }
        })
        
    except Exception as e:
        return JsonResponse({'status': 'error', 'message': str(e)})


# @csrf_exempt
# @require_http_methods(["POST"])
# def snowai_backtesting_gpt_chat_endpoint(request):
#     try:
#         data = json.loads(request.body)
#         user_message = data.get('message', '')
        
#         if not user_message:
#             return JsonResponse({'status': 'error', 'message': 'No message provided'})
        
#         recent_results = BacktestResult.objects.order_by('-created_at')[:5]
        
#         context_prompt = f"""
#         You are BacktestingGPT, an AI specialized in quantitative strategy analysis, backtesting methodology, and trading system optimization.
        
#         Recent backtesting context:
#         - Total backtests: {BacktestModels.objects.count()}
#         - Total results: {BacktestResult.objects.count()}
        
#         Recent performance:
#         {chr(10).join([f"- Sharpe: {result.sharpe_ratio:.3f} | Return: {result.annual_return:.2f}% | Drawdown: {result.max_drawdown:.2f}%" for result in recent_results])}
        
#         User question: {user_message}
        
#         Provide expert quantitative analysis and backtesting insights based on the available strategy performance data.
#         """
        
#         ai_response = chat_gpt(context_prompt)
        
#         SnowAIConversationHistory.objects.create(
#             gpt_system='BacktestingGPT',
#             user_message=user_message,
#             ai_response=ai_response
#         )
        
#         return JsonResponse({'status': 'success', 'response': ai_response})
        
#     except Exception as e:
#         return JsonResponse({'status': 'error', 'message': str(e)})


@csrf_exempt
@require_http_methods(["GET"])
def snowai_paper_gpt_summary_endpoint(request):
    try:
        all_papers = PaperGPT.objects.all()
        
        if not all_papers.exists():
            return JsonResponse({
                'status': 'No research papers available',
                'summary': 'No research papers found in the system to analyze.',
                'metrics': {}
            })
        
        total_papers = all_papers.count()
        
        # Calculate total file size safely
        total_file_size = 0
        for paper in all_papers:
            if paper.file_size:
                total_file_size += paper.file_size
        total_file_size_mb = total_file_size / (1024 * 1024)  # Convert to MB
        
        # Category analysis
        categories = all_papers.exclude(category__isnull=True).exclude(category='').values('category').annotate(count=Count('id')).order_by('-count')
        most_common_category = categories.first()['category'] if categories else 'Uncategorized'
        
        # Length analysis (approximate based on extracted text)
        papers_with_text = all_papers.exclude(extracted_text__isnull=True).exclude(extracted_text='')
        avg_paper_length = 0
        if papers_with_text.exists():
            total_length = sum([len(paper.extracted_text) for paper in papers_with_text])
            avg_paper_length = total_length / papers_with_text.count()
        
        # Recent uploads
        recent_papers = all_papers.order_by('-upload_date')[:5]
        latest_upload = recent_papers.first()
        
        # Get AI summaries for analysis
        papers_with_summaries = all_papers.exclude(ai_summary__isnull=True).exclude(ai_summary='')
        paper_summaries = []
        if papers_with_summaries.exists():
            paper_summaries = [paper.ai_summary[:200] + "..." for paper in papers_with_summaries[:10]]
        
        # Get personal notes
        papers_with_notes = all_papers.exclude(personal_notes__isnull=True).exclude(personal_notes='')
        personal_notes = []
        if papers_with_notes.exists():
            personal_notes = [paper.personal_notes[:100] + "..." for paper in papers_with_notes[:5]]
        
        prompt = f"""
        Analyze this comprehensive research paper collection and provide insights:

        PAPER COLLECTION OVERVIEW:
        - Total Papers: {total_papers}
        - Total File Size: {total_file_size_mb:.2f} MB
        - Most Common Category: {most_common_category}
        - Average Paper Length: ~{avg_paper_length:.0f} characters

        RECENT UPLOADS:
        {chr(10).join([f"- {paper.title} | Category: {paper.category or 'N/A'} | Size: {(paper.file_size/(1024*1024)):.1f}MB" for paper in recent_papers if paper.file_size])}

        EXISTING AI SUMMARIES SAMPLE:
        {chr(10).join([f"- {summary}" for summary in paper_summaries[:5]])}

        PERSONAL NOTES SAMPLE:
        {chr(10).join([f"- {note}" for note in personal_notes])}

        CATEGORY BREAKDOWN:
        {chr(10).join([f"- {cat['category']}: {cat['count']} papers" for cat in categories[:5]])}

        Please provide:
        1. Comprehensive research collection assessment
        2. Knowledge domain analysis
        3. Research gap identification
        4. Cross-paper insight synthesis
        5. Future research recommendations
        6. Practical application opportunities
        7. Knowledge management suggestions
        8. Research methodology insights
        9. Literature review conclusions
        10. Strategic research directions

        Format as a comprehensive research portfolio analysis with actionable recommendations.
        """
        
        ai_summary = chat_gpt(prompt)
        
        # Generate research recommendations
        recommendations_prompt = f"""
        Based on the {total_papers} research papers in categories like {most_common_category}, provide specific future research applications and recommendations:

        1. Identify 3-5 key research themes
        2. Suggest practical applications for trading/finance
        3. Recommend next research directions
        4. Identify knowledge gaps that need filling

        Keep recommendations specific and actionable.
        """
        
        research_recommendations = chat_gpt(recommendations_prompt)
        
        summary_obj, created = SnowAIPaperGPTSummary.objects.get_or_create(
            created_at__date=datetime.now().date(),
            defaults={
                'summary_text': ai_summary,
                'total_papers': total_papers,
                'most_common_category': most_common_category,
                'total_file_size_mb': total_file_size_mb,
                'average_paper_length': avg_paper_length,
                'latest_upload': latest_upload.title if latest_upload else 'N/A',
                'research_recommendations': research_recommendations,
                'key_insights': ', '.join([summary[:50] for summary in paper_summaries[:3]]),
            }
        )
        
        if not created:
            summary_obj.summary_text = ai_summary
            summary_obj.total_papers = total_papers
            summary_obj.research_recommendations = research_recommendations
            summary_obj.updated_at = datetime.now()
            summary_obj.save()
        
        return JsonResponse({
            'status': 'success',
            'summary': ai_summary,
            'metrics': {
                'total_papers': total_papers,
                'total_size_mb': f"{total_file_size_mb:.2f} MB",
                'most_common_category': most_common_category,
                'categories_count': len(categories),
                'avg_length': f"{avg_paper_length:.0f} chars"
            }
        })
        
    except Exception as e:
        print(f'Error in paper_gpt function: {e}')
        return JsonResponse({'status': 'error', 'message': str(e)})

# @csrf_exempt
# @require_http_methods(["POST"])
# def snowai_paper_gpt_chat_endpoint(request):
#     try:
#         data = json.loads(request.body)
#         user_message = data.get('message', '')
        
#         if not user_message:
#             return JsonResponse({'status': 'error', 'message': 'No message provided'})
        
#         recent_papers = PaperGPT.objects.order_by('-upload_date')[:5]
        
#         context_prompt = f"""
#         You are PaperGPT, an AI specialized in research paper analysis, academic literature synthesis, and research methodology.
        
#         Research paper context:
#         - Total papers in collection: {PaperGPT.objects.count()}
#         - Recent papers: {recent_papers.count()}
        
#         Sample recent papers:
#         {chr(10).join([f"- {paper.title} | Category: {paper.category or 'N/A'}" for paper in recent_papers])}
        
#         User question: {user_message}
        
#         Provide expert academic and research insights based on the available paper collection and research methodology expertise.
#         """
        
#         ai_response = chat_gpt(context_prompt)
        
#         SnowAIConversationHistory.objects.create(
#             gpt_system='PaperGPT',
#             user_message=user_message,
#             ai_response=ai_response
#         )
        
#         return JsonResponse({'status': 'success', 'response': ai_response})
        
#     except Exception as e:
#         return JsonResponse({'status': 'error', 'message': str(e)})


@csrf_exempt
@require_http_methods(["GET"])
def snowai_research_gpt_summary_endpoint(request):
    try:
        # Combine all research sources
        all_papers = PaperGPT.objects.all()
        ml_models = SnowAIMLModelLogEntry.objects.all()
        backtests = BacktestModels.objects.all()

        if not any([all_papers.exists(), ml_models.exists(), backtests.exists()]):
            return JsonResponse({
                'status': 'No research data available',
                'summary': 'No research data found across papers, ML models, or backtests.',
                'metrics': {}
            })

        total_papers = all_papers.count()
        total_ml_models = ml_models.count()
        total_backtests = backtests.count()
        total_research_entries = total_papers + total_ml_models + total_backtests

        # ML Model analysis
        model_types = ml_models.values('snowai_model_type').annotate(count=Count('id')).order_by('-count')
        financial_markets = ml_models.values('snowai_financial_market_type').annotate(count=Count('id')).order_by('-count')

        # Paper categories
        paper_categories = all_papers.exclude(category__isnull=True).values('category').annotate(count=Count('id')).order_by('-count')

        # Recent research activity
        recent_papers = all_papers.order_by('-upload_date')[:3]
        recent_models = ml_models.order_by('-snowai_created_at')[:3]

        # Performance metrics from ML models
        avg_accuracy = ml_models.exclude(snowai_accuracy_score__isnull=True).aggregate(Avg('snowai_accuracy_score'))['snowai_accuracy_score__avg'] or 0
        avg_sharpe = ml_models.exclude(snowai_sharpe_ratio__isnull=True).aggregate(Avg('snowai_sharpe_ratio'))['snowai_sharpe_ratio__avg'] or 0

        # Prompt for GPT summary
        prompt = f"""
        Analyze this comprehensive research ecosystem and provide strategic insights:

        RESEARCH PORTFOLIO OVERVIEW:
        - Total Research Entries: {total_research_entries}
        - Research Papers: {total_papers}
        - ML Models: {total_ml_models}
        - Backtesting Strategies: {total_backtests}

        ML MODEL RESEARCH:
        - Most Common Model Type: {model_types[0]['snowai_model_type'] if model_types else 'N/A'}
        - Primary Financial Market: {financial_markets[0]['snowai_financial_market_type'] if financial_markets else 'N/A'}
        - Average Model Accuracy: {avg_accuracy:.3f}
        - Average Sharpe Ratio: {avg_sharpe:.3f}

        PAPER RESEARCH:
        - Primary Research Category: {paper_categories[0]['category'] if paper_categories else 'N/A'}
        - Category Distribution: {len(paper_categories)} different categories

        RECENT RESEARCH ACTIVITY:
        Papers:
        {chr(10).join([f"- {paper.title}" for paper in recent_papers])}

        Models:
        {chr(10).join([f"- {model.snowai_model_name} ({model.snowai_model_type})" for model in recent_models])}

        MODEL TYPE DISTRIBUTION:
        {chr(10).join([f"- {mt['snowai_model_type']}: {mt['count']} models" for mt in model_types[:5]])}

        FINANCIAL MARKET FOCUS:
        {chr(10).join([f"- {fm['snowai_financial_market_type']}: {fm['count']} models" for fm in financial_markets[:5]])}

        Please provide:
        1. Comprehensive research ecosystem analysis
        2. Cross-disciplinary knowledge synthesis
        3. Research methodology assessment
        4. Knowledge gap identification and prioritization
        5. Future research direction recommendations
        6. Practical application opportunities
        7. Research ROI analysis
        8. Strategic research roadmap
        9. Innovation potential assessment
        10. Academic-industry bridge recommendations

        Format as a strategic research portfolio review with actionable insights.
        """

        ai_summary = chat_gpt(prompt)

        # Generate specific research directions
        directions_prompt = f"""
        Based on {total_research_entries} research entries including {total_ml_models} ML models and {total_papers} papers,
        provide 5 specific future research directions that bridge theory and practical trading applications.
        Focus on unexplored combinations and high-impact opportunities.
        """

        future_directions = chat_gpt(directions_prompt)

        # You can optionally include future_directions in the response or save it

        return JsonResponse({
            'status': 'success',
            'summary': ai_summary,
            'future_directions': future_directions,
            'metrics': {
                'total_papers': total_papers,
                'total_ml_models': total_ml_models,
                'total_backtests': total_backtests,
                'avg_accuracy': round(avg_accuracy, 3),
                'avg_sharpe': round(avg_sharpe, 3)
            }
        })

    except Exception as e:
        return JsonResponse({'status': 'error', 'message': str(e)})


@csrf_exempt
@require_http_methods(["GET"])
def snowai_macro_gpt_summary_endpoint(request):
    try:
        # Get economic events from the last month (NOT trading data)
        one_month_ago = datetime.now() - timedelta(days=30)
        recent_events = EconomicEvent.objects.filter(date_time__gte=one_month_ago)
        
        if not recent_events.exists():
            return JsonResponse({
                'status': 'No recent economic data available',
                'summary': 'No economic events found in the last month to analyze.',
                'metrics': {}
            })
        
        # Calculate economic event metrics
        total_events = recent_events.count()
        high_impact_events = recent_events.filter(impact='high').count()
        medium_impact_events = recent_events.filter(impact='medium').count()
        low_impact_events = recent_events.filter(impact='low').count()
        
        # Currency analysis
        currency_counts = recent_events.values('currency').annotate(count=Count('id')).order_by('-count')
        most_active_currency = currency_counts.first()['currency'] if currency_counts else 'N/A'
        
        # Get news data for major assets
        major_assets = ['EURUSD', 'GBPUSD', 'USDJPY']
        try:
            news_data = fetch_news_data(major_assets, 'butterrobot83@gmail.com')
        except:
            news_data = {'message': []}
        
        # Upcoming events
        upcoming_events = EconomicEvent.objects.filter(date_time__gt=datetime.now())[:10]
        
        # Create comprehensive prompt
        prompt = f"""
        Analyze this comprehensive macro economic data and provide a detailed market analysis:

        ECONOMIC EVENTS ANALYSIS (Last 30 Days):
        - Total Economic Events: {total_events}
        - High Impact Events: {high_impact_events}
        - Medium Impact Events: {medium_impact_events}  
        - Low Impact Events: {low_impact_events}
        - Most Active Currency: {most_active_currency}
        
        RECENT HIGH IMPACT EVENTS:
        {chr(10).join([f"- {event.currency}: {event.event_name} ({event.date_time.strftime('%Y-%m-%d')})" for event in recent_events.filter(impact='high')[:10]])}
        
        UPCOMING EVENTS PREVIEW:
        {chr(10).join([f"- {event.currency}: {event.event_name} ({event.date_time.strftime('%Y-%m-%d %H:%M')})" for event in upcoming_events])}
        
        NEWS THEMES FROM MAJOR ASSETS:
        {chr(10).join([f"- {item['asset']}: {item['title'][:100]}..." for item in news_data.get('message', [])[:10]])}

        Please provide:
        1. Comprehensive macro economic assessment
        2. Key market themes and trends
        3. Currency strength analysis
        4. Risk assessment for upcoming events
        5. Trading opportunities and recommendations
        6. Market sentiment analysis
        7. Geopolitical impact assessment
        8. Central bank policy implications

        Format as a professional macro economic briefing with actionable market insights.
        """
        
        ai_summary = chat_gpt(prompt)
        
        # Save to database
        summary_obj, created = SnowAIMacroGPTSummary.objects.get_or_create(
            created_at__date=datetime.now().date(),
            defaults={
                'summary_text': ai_summary,
                'total_economic_events': total_events,
                'high_impact_events_count': high_impact_events,
                'most_active_currency': most_active_currency,
                'key_market_themes': ', '.join([item['title'][:50] for item in news_data.get('message', [])[:5]]),
                'upcoming_events_preview': ', '.join([f"{event.currency}: {event.event_name}" for event in upcoming_events[:5]]),
                'market_sentiment': 'Mixed' if high_impact_events > 5 else 'Stable',
            }
        )
        
        if not created:
            summary_obj.summary_text = ai_summary
            summary_obj.total_economic_events = total_events
            summary_obj.high_impact_events_count = high_impact_events
            summary_obj.most_active_currency = most_active_currency
            summary_obj.updated_at = datetime.now()
            summary_obj.save()
        
        return JsonResponse({
            'status': 'success',
            'summary': ai_summary,
            'metrics': {
                'total_events': total_events,
                'high_impact_events': high_impact_events,
                'most_active_currency': most_active_currency,
                'upcoming_events': len(upcoming_events),
                'news_items': len(news_data.get('message', []))
            }
        })
        
    except Exception as e:
        print(f'Error in Macro GPT Endpoint: {e}')
        return JsonResponse({'status': 'error', 'message': str(e)})


# Add these new endpoints to your Django views.py
from django.core.paginator import Paginator

# Endpoint to get conversation history for a specific GPT
@csrf_exempt 
@require_http_methods(["GET"])
def get_conversation_history(request, gpt_system):
    try:
        # Get conversation history for the specific GPT system
        conversations = SnowAIConversationHistory.objects.filter(
            gpt_system=gpt_system
        ).order_by('timestamp')[:50]  # Get last 50 conversations
        
        # Convert to list of dictionaries
        conversation_data = []
        for conv in conversations:
            conversation_data.append({
                'gpt_system': conv.gpt_system,
                'user_message': conv.user_message,
                'ai_response': conv.ai_response,
                'timestamp': conv.timestamp.isoformat()
            })
        
        return JsonResponse({
            'status': 'success',
            'conversation_history': conversation_data,
            'total_messages': len(conversation_data)
        })
        
    except Exception as e:
        return JsonResponse({'status': 'error', 'message': str(e)})

# Endpoint to clear conversation history for a specific GPT
@csrf_exempt
@require_http_methods(["POST"])
def clear_conversation_history(request, gpt_system):
    try:
        # Delete all conversation history for the specific GPT system
        deleted_count, _ = SnowAIConversationHistory.objects.filter(
            gpt_system=gpt_system
        ).delete()
        
        return JsonResponse({
            'status': 'success',
            'message': f'Cleared {deleted_count} messages for {gpt_system}',
            'deleted_count': deleted_count
        })
        
    except Exception as e:
        return JsonResponse({'status': 'error', 'message': str(e)})

# Updated chat endpoints with conversation memory context

@csrf_exempt
@require_http_methods(["POST"])
def snowai_paper_gpt_chat_endpoint(request):
    try:
        data = json.loads(request.body)
        user_message = data.get('message', '')
        
        if not user_message:
            return JsonResponse({'status': 'error', 'message': 'No message provided'})
        
        # Get recent conversation history for context (last 10 exchanges)
        recent_conversations = SnowAIConversationHistory.objects.filter(
            gpt_system='PaperGPT'
        ).order_by('-timestamp')[:10]
        
        conversation_context = ""
        if recent_conversations:
            conversation_context = "\n\nRecent conversation history:\n"
            for conv in reversed(recent_conversations):
                conversation_context += f"User: {conv.user_message}\n"
                conversation_context += f"Assistant: {conv.ai_response}\n\n"
        
        # Get paper data with limited fields to avoid large payloads
        recent_papers = PaperGPT.objects.order_by('-upload_date')[:10]
        total_papers = PaperGPT.objects.count()
        
        # Create lightweight paper summaries (avoid large text fields)
        papers_summary = []
        for paper in recent_papers:
            paper_info = {
                'title': paper.title,
                'category': paper.category,
                'upload_date': paper.upload_date.isoformat(),
                'file_name': paper.file_name,
                'summary_preview': paper.ai_summary[:200] + "..." if paper.ai_summary and len(paper.ai_summary) > 200 else paper.ai_summary,
                'has_notes': bool(paper.personal_notes),
                'text_length': len(paper.extracted_text) if paper.extracted_text else 0
            }
            papers_summary.append(paper_info)
        
        context_prompt = f"""
        You are PaperGPT, an AI assistant who specializes in research paper analysis, academic literature synthesis, and research methodology. 
        You maintain conversation continuity and remember our previous discussions.
        
        {conversation_context}
        
        Available research papers data (use only when relevant to the conversation):
        Total papers in collection: {total_papers}
        
        Recent papers summary:
        {json.dumps(papers_summary, indent=2)}
        
        Current user message: {user_message}
        
        Instructions:
        - Maintain conversation continuity by referencing previous discussions when relevant
        - Have a natural, conversational response that builds on our chat history
        - Only provide detailed paper analysis or summaries if the user specifically asks for research insights, paper analysis, or academic information
        - For casual conversation (greetings, thanks, general questions), respond naturally without forcing paper-related content
        - Be helpful and friendly while staying true to your research expertise
        - Reference the research data when it's actually relevant to what the user is asking
        - You have access to paper metadata and summaries; if full text is needed, acknowledge the limitation
        """
        
        ai_response = chat_gpt(context_prompt)
        
        # Ensure we have a valid response
        if not ai_response or ai_response.strip() == '':
            ai_response = "I apologize, but I'm having trouble generating a response right now. Please try rephrasing your question."
        
        SnowAIConversationHistory.objects.create(
            gpt_system='PaperGPT',
            user_message=user_message,
            ai_response=ai_response
        )
        
        return JsonResponse({'status': 'success', 'response': ai_response})
        
    except Exception as e:
        print(f'Error in PaperGPT chat function: {e}')
        import traceback
        traceback.print_exc()
        return JsonResponse({'status': 'error', 'message': str(e)})


@csrf_exempt
@require_http_methods(["POST"])
def snowai_backtesting_gpt_chat_endpoint(request):
    try:
        data = json.loads(request.body)
        user_message = data.get('message', '')
        
        if not user_message:
            return JsonResponse({'status': 'error', 'message': 'No message provided'})
        
        # Get recent conversation history for context
        recent_conversations = SnowAIConversationHistory.objects.filter(
            gpt_system='BacktestingGPT'
        ).order_by('-timestamp')[:10]
        
        conversation_context = ""
        if recent_conversations:
            conversation_context = "\n\nRecent conversation history:\n"
            for conv in reversed(recent_conversations):
                conversation_context += f"User: {conv.user_message}\n"
                conversation_context += f"Assistant: {conv.ai_response}\n\n"
        
        # Get backtest data with essential fields only
        recent_results = BacktestResult.objects.order_by('-created_at')[:10]
        backtest_models = BacktestModels.objects.all()[:10]
        
        total_results = BacktestResult.objects.count()
        total_models = BacktestModels.objects.count()
        
        # Create lightweight summaries (avoid JSON field which might be large)
        results_summary = []
        for result in recent_results:
            result_info = {
                'start': result.start.isoformat(),
                'end': result.end.isoformat(),
                'duration': result.duration,
                'return_percent': result.return_percent,
                'annual_return': result.annual_return,
                'sharpe_ratio': result.sharpe_ratio,
                'max_drawdown': result.max_drawdown,
                'num_trades': result.num_trades,
                'win_rate': result.win_rate,
                'equity_final': result.equity_final,
                'created_at': result.created_at.isoformat()
            }
            results_summary.append(result_info)
        
        # Serialize models data with essential fields including generated code
        models_summary = []
        for model in backtest_models:
            model_info = {
                'chosen_dataset': model.chosen_dataset,
                'dataset_start': model.dataset_start,
                'dataset_end': model.dataset_end,
                'initial_capital': model.initial_capital,
                'model_backtested': model.model_backtested,
                'generated_code': model.generated_code[:2000] + "..." if model.generated_code and len(model.generated_code) > 2000 else model.generated_code
            }
            models_summary.append(model_info)
        
        context_prompt = f"""
        You are BacktestingGPT, an AI assistant who specializes in quantitative strategy analysis, backtesting methodology, and trading system optimization.
        You maintain conversation continuity and remember our previous discussions.
        
        {conversation_context}
        
        Available backtesting data (use only when relevant to the conversation):
        Total backtests: {total_models}
        Total results: {total_results}
        
        Recent backtest results summary:
        {json.dumps(results_summary, indent=2)}
        
        Backtest models summary:
        {json.dumps(models_summary, indent=2)}
        
        Current user message: {user_message}
        
        Instructions:
        - Maintain conversation continuity by referencing previous discussions when relevant
        - Have a natural, conversational response that builds on our chat history
        - Only provide detailed backtesting analysis or performance summaries if the user specifically asks about trading strategies, backtesting, or quantitative analysis
        - For casual conversation (greetings, thanks, general questions), respond naturally without forcing backtesting-related content
        - Be helpful and friendly while staying true to your quantitative expertise
        - Reference the backtesting data when it's actually relevant to what the user is asking
        - You have access to backtest results and model configuration summaries
        """
        
        ai_response = chat_gpt(context_prompt)
        
        # Ensure we have a valid response
        if not ai_response or ai_response.strip() == '':
            ai_response = "I apologize, but I'm having trouble generating a response right now. Please try rephrasing your question."
        
        SnowAIConversationHistory.objects.create(
            gpt_system='BacktestingGPT',
            user_message=user_message,
            ai_response=ai_response
        )
        
        return JsonResponse({'status': 'success', 'response': ai_response})
        
    except Exception as e:
        print(f'Error in BacktestingGPT chat function: {e}')
        import traceback
        traceback.print_exc()
        return JsonResponse({'status': 'error', 'message': str(e)})

@csrf_exempt
@require_http_methods(["POST"])
def snowai_research_gpt_chat_endpoint(request):
    try:
        data = json.loads(request.body)
        user_message = data.get('message', '')
        
        if not user_message:
            return JsonResponse({'status': 'error', 'message': 'No message provided'})
        
        # Get recent conversation history for context
        recent_conversations = SnowAIConversationHistory.objects.filter(
            gpt_system='ResearchGPT'
        ).order_by('-timestamp')[:10]
        
        conversation_context = ""
        if recent_conversations:
            conversation_context = "\n\nRecent conversation history:\n"
            for conv in reversed(recent_conversations):
                conversation_context += f"User: {conv.user_message}\n"
                conversation_context += f"Assistant: {conv.ai_response}\n\n"
        
        # Get actual research data
        ml_models = SnowAIMLModelLogEntry.objects.all()[:10]
        
        # Serialize ML models data
        ml_models_data = []
        for model in ml_models:
            model_dict = {}
            for field in model._meta.fields:
                field_value = getattr(model, field.name)
                if hasattr(field_value, 'isoformat'):
                    model_dict[field.name] = field_value.isoformat()
                else:
                    model_dict[field.name] = str(field_value) if field_value is not None else None
            ml_models_data.append(model_dict)
        
        context_prompt = f"""
        You are ResearchGPT, an AI assistant who specializes in comprehensive research analysis, cross-disciplinary synthesis, and strategic research planning.
        You maintain conversation continuity and remember our previous discussions.
        
        {conversation_context}
        
        Available research ecosystem data (use only when relevant to the conversation):
        Total ML models: {SnowAIMLModelLogEntry.objects.count()}
        
        ML Models data:
        {json.dumps(ml_models_data, indent=2)}
        
        Current user message: {user_message}
        
        Instructions:
        - Maintain conversation continuity by referencing previous discussions when relevant
        - Have a natural, conversational response that builds on our chat history
        - Only provide detailed research analysis or comprehensive summaries if the user specifically asks about research insights, cross-disciplinary analysis, or strategic planning
        - Be helpful and friendly while staying true to your research expertise
        - Reference the research ecosystem data when it's actually relevant to what the user is asking
        - You have access to full ML model data and research context
        """
        
        ai_response = chat_gpt(context_prompt)
        
        SnowAIConversationHistory.objects.create(
            gpt_system='ResearchGPT',
            user_message=user_message,
            ai_response=ai_response
        )
        
        return JsonResponse({'status': 'success', 'response': ai_response})
        
    except Exception as e:
        return JsonResponse({'status': 'error', 'message': str(e)})


@csrf_exempt
@require_http_methods(["POST"])
def snowai_trader_history_gpt_chat_endpoint(request):
    try:
        data = json.loads(request.body)
        user_message = data.get('message', '')
        
        if not user_message:
            return JsonResponse({'status': 'error', 'message': 'No message provided'})
        
        # Get recent conversation history for context
        recent_conversations = SnowAIConversationHistory.objects.filter(
            gpt_system='TraderHistoryGPT'
        ).order_by('-timestamp')[:10]
        
        conversation_context = ""
        if recent_conversations:
            conversation_context = "\n\nRecent conversation history:\n"
            for conv in reversed(recent_conversations):
                conversation_context += f"User: {conv.user_message}\n"
                conversation_context += f"Assistant: {conv.ai_response}\n\n"
        
        # Get actual trading data
        recent_trades = AccountTrades.objects.all()[:50]
        
        # Serialize trades data
        trades_data = []
        for trade in recent_trades:
            trade_dict = {}
            for field in trade._meta.fields:
                field_value = getattr(trade, field.name)
                if hasattr(field_value, 'isoformat'):
                    trade_dict[field.name] = field_value.isoformat()
                else:
                    trade_dict[field.name] = str(field_value) if field_value is not None else None
            trades_data.append(trade_dict)
        
        context_prompt = f"""
        You are TraderHistoryGPT, an AI assistant who specializes in analyzing trading performance and providing trading insights.
        You maintain conversation continuity and remember our previous discussions.
        
        {conversation_context}
        
        Available trading data (use only when relevant to the conversation):
        Total trades in system: {AccountTrades.objects.count()}
        
        Recent trades data:
        {json.dumps(trades_data, indent=2)}
        
        Current user message: {user_message}
        
        Instructions:
        - Maintain conversation continuity by referencing previous discussions when relevant
        - Have a natural, conversational response that builds on our chat history
        - Only provide detailed trading analysis or performance summaries if the user specifically asks about trading performance, metrics, or trading-related questions
        - For casual conversation (greetings, thanks, general questions), respond naturally without forcing trading-related content
        - Be helpful and friendly while staying true to your trading expertise
        - Reference the trading data when it's actually relevant to what the user is asking
        - You have access to full trade data including all fields and can calculate any metrics from this data
        - If the user asks about specific metrics, calculate them from the available trade data
        """
        
        ai_response = chat_gpt(context_prompt)
        
        SnowAIConversationHistory.objects.create(
            gpt_system='TraderHistoryGPT',
            user_message=user_message,
            ai_response=ai_response
        )
        
        return JsonResponse({'status': 'success', 'response': ai_response})
        
    except Exception as e:
        return JsonResponse({'status': 'error', 'message': str(e)})


@csrf_exempt
@require_http_methods(["POST"])
def snowai_idea_gpt_chat_endpoint(request):
    try:
        data = json.loads(request.body)
        user_message = data.get('message', '')
        
        if not user_message:
            return JsonResponse({'status': 'error', 'message': 'No message provided'})
        
        # Get recent conversation history for context
        recent_conversations = SnowAIConversationHistory.objects.filter(
            gpt_system='IdeaGPT'
        ).order_by('-timestamp')[:10]
        
        conversation_context = ""
        if recent_conversations:
            conversation_context = "\n\nRecent conversation history:\n"
            for conv in reversed(recent_conversations):
                conversation_context += f"User: {conv.user_message}\n"
                conversation_context += f"Assistant: {conv.ai_response}\n\n"
        
        # Get actual ideas data
        recent_ideas = IdeaModel.objects.order_by('-created_at')[:20]
        
        # Serialize ideas data
        ideas_data = []
        for idea in recent_ideas:
            idea_dict = {}
            for field in idea._meta.fields:
                field_value = getattr(idea, field.name)
                if hasattr(field_value, 'isoformat'):
                    idea_dict[field.name] = field_value.isoformat()
                else:
                    idea_dict[field.name] = str(field_value) if field_value is not None else None
            ideas_data.append(idea_dict)
        
        context_prompt = f"""
        You are IdeaGPT, an AI assistant who specializes in idea management, creativity enhancement, and innovation strategy.
        You maintain conversation continuity and remember our previous discussions.
        
        {conversation_context}
        
        Available ideas data (use only when relevant to the conversation):
        Total ideas in system: {IdeaModel.objects.count()}
        
        Recent ideas data:
        {json.dumps(ideas_data, indent=2)}
        
        Current user message: {user_message}
        
        Instructions:
        - Maintain conversation continuity by referencing previous discussions when relevant
        - Have a natural, conversational response that builds on our chat history
        - Only provide detailed idea analysis or creativity insights if the user specifically asks about ideas, creativity, innovation, or brainstorming
        - For casual conversation (greetings, thanks, general questions), respond naturally without forcing idea-related content
        - Be helpful and friendly while staying true to your creativity and innovation expertise
        - Reference the ideas data when it's actually relevant to what the user is asking
        - You have access to full idea data including all fields and content
        """
        
        ai_response = chat_gpt(context_prompt)
        
        SnowAIConversationHistory.objects.create(
            gpt_system='IdeaGPT',
            user_message=user_message,
            ai_response=ai_response
        )
        
        return JsonResponse({'status': 'success', 'response': ai_response})
        
    except Exception as e:
        return JsonResponse({'status': 'error', 'message': str(e)})


@csrf_exempt
@require_http_methods(["POST"])
def snowai_macro_gpt_chat_endpoint(request):
    try:
        data = json.loads(request.body)
        user_message = data.get('message', '')
        
        if not user_message:
            return JsonResponse({'status': 'error', 'message': 'No message provided'})
        
        # Get recent conversation history for context
        recent_conversations = SnowAIConversationHistory.objects.filter(
            gpt_system='MacroGPT'
        ).order_by('-timestamp')[:10]
        
        conversation_context = ""
        if recent_conversations:
            conversation_context = "\n\nRecent conversation history:\n"
            for conv in reversed(recent_conversations):
                conversation_context += f"User: {conv.user_message}\n"
                conversation_context += f"Assistant: {conv.ai_response}\n\n"
        
        # Get actual economic data - PAST EVENTS FROM LAST 30 DAYS
        past_events = EconomicEvent.objects.filter(
            date_time__gte=datetime.now() - timedelta(days=30),
            date_time__lt=datetime.now()
        ).order_by('-date_time')
        
        # Serialize past events data
        past_events_data = []
        for event in past_events:
            event_dict = {}
            for field in event._meta.fields:
                field_value = getattr(event, field.name)
                if hasattr(field_value, 'isoformat'):
                    event_dict[field.name] = field_value.isoformat()
                else:
                    event_dict[field.name] = str(field_value) if field_value is not None else None
            past_events_data.append(event_dict)
        
        context_prompt = f"""
        You are MacroGPT, an AI assistant who specializes in macro economic analysis, market trends, and economic event impact assessment.
        You maintain conversation continuity and remember our previous discussions.
        
        {conversation_context}
        
        Available economic data (use only when relevant to the conversation):
        
        Past economic events (Last 30 days):
        {json.dumps(past_events_data, indent=2)}
        
        Current user message: {user_message}
        
        Instructions:
        - Maintain conversation continuity by referencing previous discussions when relevant
        - Have a natural, conversational response that builds on our chat history
        - Only provide detailed macro economic analysis or market insights if the user specifically asks about economic events, market trends, or macro analysis
        - For casual conversation (greetings, thanks, general questions), respond naturally without forcing economic content
        - Be helpful and friendly while staying true to your macro economic expertise
        - Reference the economic data when it's actually relevant to what the user is asking
        - You have access to past economic event data including all fields and details
        - If the user asks about specific currencies or past events, reference the available data context
        """
        
        ai_response = chat_gpt(context_prompt)
        
        # Ensure we have a response
        if not ai_response or ai_response.strip() == '':
            ai_response = "I apologize, but I'm having trouble generating a response right now. Please try rephrasing your question."
        
        SnowAIConversationHistory.objects.create(
            gpt_system='MacroGPT',
            user_message=user_message,
            ai_response=ai_response
        )
        
        return JsonResponse({'status': 'success', 'response': ai_response})
        
    except Exception as e:
        print(f'Error in MacroGPT chat function: {e}')
        return JsonResponse({'status': 'error', 'message': str(e)})
        

def init_scheduler():
    """Initialize the background scheduler"""
    global scheduler
    if scheduler is None:
        scheduler = BackgroundScheduler()
        
        # Add job to run all GPT summaries every 24 hours at 2 AM
        scheduler.add_job(
            func=generate_all_gpt_summaries,
            trigger="cron",
            hour=2,  # Run at 2 AM
            minute=0,
            id='gpt_summaries_job',
            replace_existing=True
        )
        
        # Add job to run every hour for testing (remove this in production)
        # scheduler.add_job(
        #     func=generate_all_gpt_summaries,
        #     trigger="interval",
        #     hours=1,
        #     id='gpt_summaries_hourly_test',
        #     replace_existing=True
        # )
        
        scheduler.start()
        logger.info("Scheduler started successfully")
        
        # Shutdown scheduler when the application exits
        # atexit.register(lambda: scheduler.shutdown() if scheduler else None)
    
    return scheduler

def generate_all_gpt_summaries():
    """Function to generate all GPT summaries - called by scheduler"""
    logger.info("Starting scheduled GPT summary generation...")
    
    summary_functions = [
        ('TraderHistoryGPT', generate_trader_history_summary),
        ('MacroGPT', generate_macro_gpt_summary), 
        ('IdeaGPT', generate_idea_gpt_summary),
        ('BacktestingGPT', generate_backtesting_gpt_summary),
        ('PaperGPT', generate_paper_gpt_summary),
        ('ResearchGPT', generate_research_gpt_summary),
    ]
    
    results = {}
    for gpt_name, func in summary_functions:
        try:
            logger.info(f"Generating {gpt_name} summary...")
            result = func()
            results[gpt_name] = 'success' if result else 'failed'
            logger.info(f"{gpt_name} summary generation completed")
        except Exception as e:
            logger.error(f"Error generating {gpt_name} summary: {str(e)}")
            results[gpt_name] = f'error: {str(e)}'
    
    logger.info(f"Scheduled summary generation completed. Results: {results}")
    return results


# NEW ENDPOINT: Get existing summary without generating
@csrf_exempt
@require_http_methods(["GET"])
def get_existing_summary(request, gpt_type):
    """Retrieve existing summary from database without generating new one"""
    try:
        gpt_type_map = {
            'TraderHistoryGPT': SnowAITraderHistoryGPTSummary,
            'MacroGPT': SnowAIMacroGPTSummary,
            'IdeaGPT': SnowAIIdeaGPTSummary, 
            'BacktestingGPT': SnowAIBacktestingGPTSummary,
            'PaperGPT': SnowAIPaperGPTSummary,
            'ResearchGPT': SnowAIResearchGPTSummary,
        }
        
        model_class = gpt_type_map.get(gpt_type)
        if not model_class:
            return JsonResponse({'status': 'error', 'message': 'Invalid GPT type'})
        
        # Get the most recent summary
        try:
            summary_obj = model_class.objects.latest('created_at')
            
            # Build metrics based on available fields
            metrics = {}
            
            # Common fields across different models
            if hasattr(summary_obj, 'total_trades'):
                metrics['total_trades'] = summary_obj.total_trades
            if hasattr(summary_obj, 'win_rate'): 
                metrics['win_rate'] = f"{summary_obj.win_rate:.2f}%"
            if hasattr(summary_obj, 'total_profit_loss'):
                metrics['total_pnl'] = f"${summary_obj.total_profit_loss:,.2f}"
            if hasattr(summary_obj, 'most_traded_asset'):
                metrics['most_traded_asset'] = summary_obj.most_traded_asset
            if hasattr(summary_obj, 'best_performing_strategy'):
                metrics['best_strategy'] = summary_obj.best_performing_strategy
            if hasattr(summary_obj, 'total_ideas'):
                metrics['total_ideas'] = summary_obj.total_ideas
            if hasattr(summary_obj, 'completion_rate'):
                metrics['completion_rate'] = f"{summary_obj.completion_rate:.2f}%"
            if hasattr(summary_obj, 'total_backtests'):
                metrics['total_backtests'] = summary_obj.total_backtests
            if hasattr(summary_obj, 'average_sharpe_ratio'):
                metrics['avg_sharpe_ratio'] = f"{summary_obj.average_sharpe_ratio:.3f}"
            if hasattr(summary_obj, 'total_papers'):
                metrics['total_papers'] = summary_obj.total_papers
            if hasattr(summary_obj, 'most_common_category'):
                metrics['most_common_category'] = summary_obj.most_common_category
            if hasattr(summary_obj, 'total_economic_events'):
                metrics['total_events'] = summary_obj.total_economic_events
            if hasattr(summary_obj, 'high_impact_events_count'):
                metrics['high_impact_events'] = summary_obj.high_impact_events_count
            if hasattr(summary_obj, 'most_active_currency'):
                metrics['most_active_currency'] = summary_obj.most_active_currency
            
            return JsonResponse({
                'status': 'success',
                'summary': summary_obj.summary_text,
                'metrics': metrics,
                'last_updated': summary_obj.updated_at.isoformat() if hasattr(summary_obj, 'updated_at') and summary_obj.updated_at else summary_obj.created_at.isoformat()
            })
            
        except model_class.DoesNotExist:
            return JsonResponse({
                'status': 'No summary available',
                'summary': None,
                'metrics': {},
                'last_updated': None
            })
            
    except Exception as e:
        logger.error(f'Error fetching existing summary for {gpt_type}: {str(e)}')
        return JsonResponse({'status': 'error', 'message': str(e)})


# SEPARATED SUMMARY GENERATION FUNCTIONS (for scheduler use)
def generate_trader_history_summary():
    """Generate TraderHistoryGPT summary - for scheduler use"""
    try:
        # Get trading data
        all_trades = AccountTrades.objects.all()
        
        if not all_trades.exists():
            return False
        
        # Calculate trading metrics
        total_trades = all_trades.count()
        profit_trades = all_trades.filter(outcome='Win').count()
        loss_trades = all_trades.filter(outcome='Loss').count()
        win_rate = (profit_trades / total_trades * 100) if total_trades > 0 else 0
        
        total_pnl = all_trades.aggregate(Sum('amount'))['amount__sum'] or 0
        avg_trade_amount = all_trades.aggregate(Avg('amount'))['amount__avg'] or 0
        best_trade = all_trades.aggregate(Max('amount'))['amount__max'] or 0
        worst_trade = all_trades.aggregate(Min('amount'))['amount__min'] or 0
        
        # Strategy analysis
        strategy_performance = all_trades.values('strategy').annotate(
            total_trades=Count('id'),
            total_pnl=Sum('amount')
        ).order_by('-total_pnl')
        
        best_strategy = strategy_performance.first()['strategy'] if strategy_performance else 'N/A'
        worst_strategy = strategy_performance.last()['strategy'] if strategy_performance else 'N/A'
        
        # Asset analysis
        asset_counts = all_trades.values('asset').annotate(count=Count('id')).order_by('-count')
        most_traded_asset = asset_counts.first()['asset'] if asset_counts else 'N/A'
        
        # Get news data for context
        major_assets = ['EURUSD', 'GBPUSD', 'USDJPY']
        try:
            news_data = fetch_news_data(major_assets, 'butterrobot83@gmail.com')
        except:
            news_data = {'message': []}
        
        # Create comprehensive prompt for GPT
        prompt = f"""
        Analyze this comprehensive trading performance data and provide a detailed, professional summary:

        TRADING PERFORMANCE METRICS:
        - Total Trades: {total_trades}
        - Win Rate: {win_rate:.2f}%
        - Total P&L: ${total_pnl:,.2f}
        - Average Trade Size: ${avg_trade_amount:,.2f}
        - Best Trade: ${best_trade:,.2f}
        - Worst Trade: ${worst_trade:,.2f}
        - Most Traded Asset: {most_traded_asset}
        - Best Performing Strategy: {best_strategy}
        - Worst Performing Strategy: {worst_strategy}

        DETAILED BREAKDOWN:
        - Profitable Trades: {profit_trades}
        - Losing Trades: {loss_trades}

        RECENT MARKET NEWS THEMES:
        {chr(10).join([f"- {item['asset']}: {item['title'][:100]}..." for item in news_data.get('message', [])[:5]])}

        Please provide:
        1. A comprehensive performance assessment
        2. Key strengths and weaknesses in the trading approach
        3. Risk management analysis
        4. Recommendations for improvement
        5. Strategic insights based on the data
        6. Asset allocation observations
        7. Future trading suggestions

        Format the response as a professional trading report with clear sections and actionable insights.
        """
        
        # Get AI summary
        ai_summary = chat_gpt(prompt)
        
        # Save to database
        summary_obj, created = SnowAITraderHistoryGPTSummary.objects.get_or_create(
            created_at__date=datetime.now().date(),
            defaults={
                'summary_text': ai_summary,
                'total_trades': total_trades,
                'win_rate': win_rate,
                'total_profit_loss': total_pnl,
                'best_performing_strategy': best_strategy,
                'worst_performing_strategy': worst_strategy,
                'most_traded_asset': most_traded_asset,
                'average_trade_amount': avg_trade_amount,
            }
        )
        
        if not created:
            # Update existing summary
            summary_obj.summary_text = ai_summary
            summary_obj.total_trades = total_trades
            summary_obj.win_rate = win_rate
            summary_obj.total_profit_loss = total_pnl
            summary_obj.best_performing_strategy = best_strategy
            summary_obj.worst_performing_strategy = worst_strategy
            summary_obj.most_traded_asset = most_traded_asset
            summary_obj.average_trade_amount = avg_trade_amount
            summary_obj.updated_at = datetime.now()
            summary_obj.save()
        
        return True
        
    except Exception as e:
        logger.error(f'Error in generate_trader_history_summary: {e}')
        return False


def generate_macro_gpt_summary():
    """Generate MacroGPT summary - for scheduler use"""
    try:
        one_month_ago = datetime.now() - timedelta(days=30)
        recent_events = EconomicEvent.objects.filter(date_time__gte=one_month_ago)
        
        if not recent_events.exists():
            return False
        
        # Calculate economic event metrics
        total_events = recent_events.count()
        high_impact_events = recent_events.filter(impact='high').count()
        medium_impact_events = recent_events.filter(impact='medium').count()
        low_impact_events = recent_events.filter(impact='low').count()
        
        # Currency analysis
        currency_counts = recent_events.values('currency').annotate(count=Count('id')).order_by('-count')
        most_active_currency = currency_counts.first()['currency'] if currency_counts else 'N/A'
        
        # Get news data for major assets
        major_assets = ['EURUSD', 'GBPUSD', 'USDJPY']
        try:
            news_data = fetch_news_data(major_assets, 'butterrobot83@gmail.com')
        except:
            news_data = {'message': []}
        
        # Upcoming events
        upcoming_events = EconomicEvent.objects.filter(date_time__gt=datetime.now())[:10]
        
        # Create comprehensive prompt
        prompt = f"""
        Analyze this comprehensive macro economic data and provide a detailed market analysis:

        ECONOMIC EVENTS ANALYSIS (Last 30 Days):
        - Total Economic Events: {total_events}
        - High Impact Events: {high_impact_events}
        - Medium Impact Events: {medium_impact_events}  
        - Low Impact Events: {low_impact_events}
        - Most Active Currency: {most_active_currency}
        
        RECENT HIGH IMPACT EVENTS:
        {chr(10).join([f"- {event.currency}: {event.event_name} ({event.date_time.strftime('%Y-%m-%d')})" for event in recent_events.filter(impact='high')[:10]])}
        
        UPCOMING EVENTS PREVIEW:
        {chr(10).join([f"- {event.currency}: {event.event_name} ({event.date_time.strftime('%Y-%m-%d %H:%M')})" for event in upcoming_events])}
        
        NEWS THEMES FROM MAJOR ASSETS:
        {chr(10).join([f"- {item['asset']}: {item['title'][:100]}..." for item in news_data.get('message', [])[:10]])}

        Please provide:
        1. Comprehensive macro economic assessment
        2. Key market themes and trends
        3. Currency strength analysis
        4. Risk assessment for upcoming events
        5. Trading opportunities and recommendations
        6. Market sentiment analysis
        7. Geopolitical impact assessment
        8. Central bank policy implications

        Format as a professional macro economic briefing with actionable market insights.
        """
        
        ai_summary = chat_gpt(prompt)
        
        # Save to database
        summary_obj, created = SnowAIMacroGPTSummary.objects.get_or_create(
            created_at__date=datetime.now().date(),
            defaults={
                'summary_text': ai_summary,
                'total_economic_events': total_events,
                'high_impact_events_count': high_impact_events,
                'most_active_currency': most_active_currency,
                'key_market_themes': ', '.join([item['title'][:50] for item in news_data.get('message', [])[:5]]),
                'upcoming_events_preview': ', '.join([f"{event.currency}: {event.event_name}" for event in upcoming_events[:5]]),
                'market_sentiment': 'Mixed' if high_impact_events > 5 else 'Stable',
            }
        )
        
        if not created:
            summary_obj.summary_text = ai_summary
            summary_obj.total_economic_events = total_events
            summary_obj.high_impact_events_count = high_impact_events
            summary_obj.most_active_currency = most_active_currency
            summary_obj.updated_at = datetime.now()
            summary_obj.save()
        
        return True
        
    except Exception as e:
        logger.error(f'Error in generate_macro_gpt_summary: {e}')
        return False


def generate_idea_gpt_summary():
    """Generate IdeaGPT summary - for scheduler use"""
    try:
        all_ideas = IdeaModel.objects.all()
        trade_ideas = TradeIdea.objects.all()
        
        if not all_ideas.exists() and not trade_ideas.exists():
            return False
        
        # Calculate metrics for regular ideas
        total_ideas = all_ideas.count()
        pending_ideas = all_ideas.filter(idea_tracker='Pending').count()
        in_progress_ideas = all_ideas.filter(idea_tracker='In Progress').count()
        completed_ideas = all_ideas.filter(idea_tracker='Completed').count()
        
        completion_rate = (completed_ideas / total_ideas * 100) if total_ideas > 0 else 0
        
        # Category analysis
        categories = all_ideas.values('idea_category').annotate(count=Count('id')).order_by('-count')
        most_common_category = categories.first()['idea_category'] if categories else 'N/A'
        
        # Trade ideas metrics
        total_trade_ideas = trade_ideas.count()
        pending_trade_ideas = trade_ideas.filter(trade_status='pending').count()
        executed_trade_ideas = trade_ideas.filter(trade_status='executed').count()
        
        # Get recent ideas for context
        recent_ideas = all_ideas.order_by('-created_at')[:10]
        recent_trade_ideas = trade_ideas.order_by('-date_created')[:5]
        
        oldest_pending = all_ideas.filter(idea_tracker='Pending').order_by('created_at').first()
        newest_idea = all_ideas.order_by('-created_at').first()
        
        prompt = f"""
        Analyze this comprehensive idea management data and provide detailed insights:

        GENERAL IDEAS ANALYSIS:
        - Total Ideas: {total_ideas}
        - Pending Ideas: {pending_ideas}
        - In Progress Ideas: {in_progress_ideas}
        - Completed Ideas: {completed_ideas}
        - Completion Rate: {completion_rate:.2f}%
        - Most Common Category: {most_common_category}

        TRADE IDEAS ANALYSIS:
        - Total Trade Ideas: {total_trade_ideas}
        - Pending Trade Ideas: {pending_trade_ideas}
        - Executed Trade Ideas: {executed_trade_ideas}

        RECENT IDEAS SAMPLE:
        {chr(10).join([f"- [{idea.idea_tracker}] {idea.idea_category}: {idea.idea_text[:100]}..." for idea in recent_ideas[:5]])}

        RECENT TRADE IDEAS:
        {chr(10).join([f"- [{trade.trade_status}] {trade.asset}: {trade.heading}" for trade in recent_trade_ideas])}

        Please provide:
        1. Comprehensive idea pipeline analysis
        2. Productivity and execution assessment
        3. Category-wise performance breakdown
        4. Bottleneck identification
        5. Recommendations for better idea management
        6. Trading idea conversion analysis
        7. Strategic prioritization suggestions
        8. Innovation and creativity assessment

        Format as a professional idea management report with actionable recommendations.
        """
        
        ai_summary = chat_gpt(prompt)
        
        summary_obj, created = SnowAIIdeaGPTSummary.objects.get_or_create(
            created_at__date=datetime.now().date(),
            defaults={
                'summary_text': ai_summary,
                'total_ideas': total_ideas,
                'pending_ideas': pending_ideas,
                'in_progress_ideas': in_progress_ideas,
                'completed_ideas': completed_ideas,
                'most_common_category': most_common_category,
                'completion_rate': completion_rate,
                'oldest_pending_idea': oldest_pending.idea_text[:200] if oldest_pending else 'N/A',
                'newest_idea': newest_idea.idea_text[:200] if newest_idea else 'N/A',
            }
        )
        
        if not created:
            summary_obj.summary_text = ai_summary
            summary_obj.total_ideas = total_ideas
            summary_obj.pending_ideas = pending_ideas
            summary_obj.in_progress_ideas = in_progress_ideas
            summary_obj.completed_ideas = completed_ideas
            summary_obj.completion_rate = completion_rate
            summary_obj.updated_at = datetime.now()
            summary_obj.save()
        
        return True
        
    except Exception as e:
        logger.error(f'Error in generate_idea_gpt_summary: {e}')
        return False


def generate_backtesting_gpt_summary():
    """Generate BacktestingGPT summary - for scheduler use"""
    try:
        all_backtests = BacktestModels.objects.all()
        all_results = BacktestResult.objects.all()
        
        if not all_backtests.exists():
            return False
        
        total_backtests = all_backtests.count()
        successful_backtests = all_backtests.filter(model_backtested=True).count()
        
        # Results analysis
        if all_results.exists():
            avg_sharpe = all_results.aggregate(Avg('sharpe_ratio'))['sharpe_ratio__avg'] or 0
            avg_annual_return = all_results.aggregate(Avg('annual_return'))['annual_return__avg'] or 0
            avg_max_drawdown = all_results.aggregate(Avg('max_drawdown'))['max_drawdown__avg'] or 0
            best_sharpe = all_results.aggregate(Max('sharpe_ratio'))['sharpe_ratio__max'] or 0
            worst_sharpe = all_results.aggregate(Min('sharpe_ratio'))['sharpe_ratio__min'] or 0
            
            best_result = all_results.filter(sharpe_ratio=best_sharpe).first()
            worst_result = all_results.filter(sharpe_ratio=worst_sharpe).first()
        else:
            avg_sharpe = avg_annual_return = avg_max_drawdown = 0
            best_result = worst_result = None
        
        # Dataset analysis
        datasets = all_backtests.values('chosen_dataset').annotate(count=Count('id')).order_by('-count')
        most_used_dataset = datasets.first()['chosen_dataset'] if datasets else 'N/A'
        
        # Recent backtests
        recent_backtests = all_backtests.order_by('-id')[:5]
        
        prompt = f"""
        Analyze this comprehensive backtesting performance data:

        BACKTESTING OVERVIEW:
        - Total Backtests: {total_backtests}
        - Successful Backtests: {successful_backtests}
        - Success Rate: {(successful_backtests/total_backtests*100) if total_backtests > 0 else 0:.2f}%
        - Most Used Dataset: {most_used_dataset}

        PERFORMANCE METRICS:
        - Average Sharpe Ratio: {avg_sharpe:.3f}
        - Average Annual Return: {avg_annual_return:.2f}%
        - Average Max Drawdown: {avg_max_drawdown:.2f}%
        - Best Sharpe Ratio: {best_sharpe:.3f}
        - Worst Sharpe Ratio: {worst_sharpe:.3f}

        RECENT BACKTESTS:
        {chr(10).join([f"- Dataset: {bt.chosen_dataset} | Period: {bt.dataset_start} to {bt.dataset_end} | Capital: ${bt.initial_capital:,.2f}" for bt in recent_backtests])}

        BEST PERFORMING STRATEGY:
        {f"Sharpe: {best_result.sharpe_ratio:.3f} | Annual Return: {best_result.annual_return:.2f}% | Drawdown: {best_result.max_drawdown:.2f}%" if best_result else "No results available"}

        WORST PERFORMING STRATEGY:
        {f"Sharpe: {worst_result.sharpe_ratio:.3f} | Annual Return: {worst_result.annual_return:.2f}% | Drawdown: {worst_result.max_drawdown:.2f}%" if worst_result else "No results available"}

        Please provide:
        1. Comprehensive backtesting performance assessment
        2. Strategy effectiveness analysis
        3. Risk-adjusted returns evaluation
        4. Dataset utilization insights
        5. Performance consistency analysis
        6. Recommendations for strategy improvement
        7. Risk management effectiveness
        8. Future backtesting suggestions

        Format as a professional quantitative analysis report.
        """
        
        ai_summary = chat_gpt(prompt)
        
        summary_obj, created = SnowAIBacktestingGPTSummary.objects.get_or_create(
            created_at__date=datetime.now().date(),
            defaults={
                'summary_text': ai_summary,
                'total_backtests': total_backtests,
                'successful_backtests': successful_backtests,
                'average_sharpe_ratio': avg_sharpe,
                'average_annual_return': avg_annual_return,
                'average_max_drawdown': avg_max_drawdown,
                'best_performing_strategy': f"Sharpe: {best_sharpe:.3f}" if best_result else 'N/A',
                'worst_performing_strategy': f"Sharpe: {worst_sharpe:.3f}" if worst_result else 'N/A',
                'most_used_dataset': most_used_dataset,
            }
        )
        
        if not created:
            summary_obj.summary_text = ai_summary
            summary_obj.total_backtests = total_backtests
            summary_obj.successful_backtests = successful_backtests
            summary_obj.average_sharpe_ratio = avg_sharpe
            summary_obj.updated_at = datetime.now()
            summary_obj.save()
        
        return True
        
    except Exception as e:
        logger.error(f'Error in generate_backtesting_gpt_summary: {e}')
        return False


def generate_paper_gpt_summary():
    """Generate PaperGPT summary - for scheduler use"""
    try:
        all_papers = PaperGPT.objects.all()
        
        if not all_papers.exists():
            return False
        
        total_papers = all_papers.count()
        
        # Calculate total file size safely
        total_file_size = 0
        for paper in all_papers:
            if paper.file_size:
                total_file_size += paper.file_size
        total_file_size_mb = total_file_size / (1024 * 1024)  # Convert to MB
        
        # Category analysis
        categories = all_papers.exclude(category__isnull=True).exclude(category='').values('category').annotate(count=Count('id')).order_by('-count')
        most_common_category = categories.first()['category'] if categories else 'Uncategorized'
        
        # Length analysis
        papers_with_text = all_papers.exclude(extracted_text__isnull=True).exclude(extracted_text='')
        avg_paper_length = 0
        if papers_with_text.exists():
            total_length = sum([len(paper.extracted_text) for paper in papers_with_text])
            avg_paper_length = total_length / papers_with_text.count()
        
        # Recent uploads
        recent_papers = all_papers.order_by('-upload_date')[:5]
        latest_upload = recent_papers.first()
        
        # Get AI summaries for analysis
        papers_with_summaries = all_papers.exclude(ai_summary__isnull=True).exclude(ai_summary='')
        paper_summaries = []
        if papers_with_summaries.exists():
            paper_summaries = [paper.ai_summary[:200] + "..." for paper in papers_with_summaries[:10]]
        
        # Get personal notes
        papers_with_notes = all_papers.exclude(personal_notes__isnull=True).exclude(personal_notes='')
        personal_notes = []
        if papers_with_notes.exists():
            personal_notes = [paper.personal_notes[:100] + "..." for paper in papers_with_notes[:5]]
        
        prompt = f"""
        Analyze this comprehensive research paper collection and provide insights:

        PAPER COLLECTION OVERVIEW:
        - Total Papers: {total_papers}
        - Total File Size: {total_file_size_mb:.2f} MB
        - Most Common Category: {most_common_category}
        - Average Paper Length: ~{avg_paper_length:.0f} characters

        RECENT UPLOADS:
        {chr(10).join([f"- {paper.title} | Category: {paper.category or 'N/A'} | Size: {(paper.file_size/(1024*1024)):.1f}MB" for paper in recent_papers if paper.file_size])}

        EXISTING AI SUMMARIES SAMPLE:
        {chr(10).join([f"- {summary}" for summary in paper_summaries[:5]])}

        PERSONAL NOTES SAMPLE:
        {chr(10).join([f"- {note}" for note in personal_notes])}

        CATEGORY BREAKDOWN:
        {chr(10).join([f"- {cat['category']}: {cat['count']} papers" for cat in categories[:5]])}

        Please provide:
        1. Comprehensive research collection assessment
        2. Knowledge domain analysis
        3. Research gap identification
        4. Cross-paper insight synthesis
        5. Future research recommendations
        6. Practical application opportunities
        7. Knowledge management suggestions
        8. Research methodology insights
        9. Literature review conclusions
        10. Strategic research directions

        Format as a comprehensive research portfolio analysis with actionable recommendations.
        """
        
        ai_summary = chat_gpt(prompt)
        
        # Generate research recommendations
        recommendations_prompt = f"""
        Based on the {total_papers} research papers in categories like {most_common_category}, provide specific future research applications and recommendations:

        1. Identify 3-5 key research themes
        2. Suggest practical applications for trading/finance
        3. Recommend next research directions
        4. Identify knowledge gaps that need filling

        Keep recommendations specific and actionable.
        """
        
        research_recommendations = chat_gpt(recommendations_prompt)
        
        summary_obj, created = SnowAIPaperGPTSummary.objects.get_or_create(
            created_at__date=datetime.now().date(),
            defaults={
                'summary_text': ai_summary,
                'total_papers': total_papers,
                'most_common_category': most_common_category,
                'total_file_size_mb': total_file_size_mb,
                'average_paper_length': avg_paper_length,
                'latest_upload': latest_upload.title if latest_upload else 'N/A',
                'research_recommendations': research_recommendations,
                'key_insights': ', '.join([summary[:50] for summary in paper_summaries[:3]]),
            }
        )
        
        if not created:
            summary_obj.summary_text = ai_summary
            summary_obj.total_papers = total_papers
            summary_obj.research_recommendations = research_recommendations
            summary_obj.updated_at = datetime.now()
            summary_obj.save()
        
        return True
        
    except Exception as e:
        logger.error(f'Error in generate_paper_gpt_summary: {e}')
        return False


def generate_research_gpt_summary():
    """Generate ResearchGPT summary - focuses only on ML models research"""
    try:
        # Only analyze ML models (no papers or backtests)
        ml_models = SnowAIMLModelLogEntry.objects.all()

        if not ml_models.exists():
            return False

        total_ml_models = ml_models.count()

        # ML Model analysis
        model_types = ml_models.values('snowai_model_type').annotate(count=Count('id')).order_by('-count')
        financial_markets = ml_models.values('snowai_financial_market_type').annotate(count=Count('id')).order_by('-count')

        # Recent ML model activity
        recent_models = ml_models.order_by('-snowai_created_at')[:5]

        # Performance metrics from ML models
        avg_accuracy = ml_models.exclude(snowai_accuracy_score__isnull=True).aggregate(Avg('snowai_accuracy_score'))['snowai_accuracy_score__avg'] or 0
        avg_sharpe = ml_models.exclude(snowai_sharpe_ratio__isnull=True).aggregate(Avg('snowai_sharpe_ratio'))['snowai_sharpe_ratio__avg'] or 0

        # Model performance distribution
        high_performing_models = ml_models.filter(snowai_accuracy_score__gte=0.7).count()
        profitable_models = ml_models.filter(snowai_sharpe_ratio__gte=1.0).count()

        # Training data analysis
        avg_training_samples = ml_models.exclude(snowai_dataset_size__isnull=True).aggregate(Avg('snowai_dataset_size'))['snowai_dataset_size__avg'] or 0

        # Prompt for GPT summary focusing on ML research
        prompt = f"""
        Analyze this machine learning research ecosystem and provide strategic ML insights:

        ML MODEL RESEARCH OVERVIEW:
        - Total ML Models: {total_ml_models}
        - High-Performing Models (>70% accuracy): {high_performing_models}
        - Profitable Models (Sharpe >1.0): {profitable_models}
        - Average Model Accuracy: {avg_accuracy}
        - Average Sharpe Ratio: {avg_sharpe}
        - Average Training Samples: {avg_training_samples}

        MODEL TYPE DISTRIBUTION:
        {chr(10).join([f"- {mt['snowai_model_type']}: {mt['count']} models" for mt in model_types[:10]])}

        FINANCIAL MARKET FOCUS:
        {chr(10).join([f"- {fm['snowai_financial_market_type']}: {fm['count']} models" for fm in financial_markets[:10]])}

        RECENT ML MODEL DEVELOPMENT:
        {chr(10).join([f"- {model.snowai_model_name} ({model.snowai_model_type}) - Accuracy: {model.snowai_accuracy_score or 'N/A'}" for model in recent_models])}

        Please provide:
        1. ML model performance analysis and trends
        2. Algorithm effectiveness assessment across different markets
        3. Model architecture optimization insights
        4. Feature engineering opportunities identified
        5. Cross-market model transferability analysis
        6. Overfitting and generalization patterns
        7. Training data quality and quantity recommendations
        8. Model ensemble and combination strategies
        9. Risk management through ML model diversification
        10. Next-generation ML research directions

        Focus specifically on machine learning research insights, model development patterns, and algorithmic trading applications.
        """

        ai_summary = chat_gpt(prompt)

        # Generate ML-specific knowledge gaps
        gaps_prompt = f"""
        Based on {total_ml_models} ML models with average accuracy of {avg_accuracy}, 
        identify 3-5 specific knowledge gaps in our ML research that could significantly improve model performance.
        Focus on algorithmic gaps, data gaps, and methodology gaps.
        """

        knowledge_gaps = chat_gpt(gaps_prompt)

        # Generate ML research directions
        directions_prompt = f"""
        Based on current ML model portfolio analysis, suggest 5 specific future ML research directions 
        that could improve trading performance. Consider model architectures, feature engineering, 
        ensemble methods, and novel ML approaches for financial markets.
        """

        future_directions = chat_gpt(directions_prompt)

        # Generate cross-model insights
        insights_prompt = f"""
        Analyze patterns across {total_ml_models} ML models to identify 3-5 key insights about 
        what makes models successful vs unsuccessful in financial markets. Focus on 
        transferable learnings and best practices.
        """

        cross_insights = chat_gpt(insights_prompt)

        # Generate practical applications
        applications_prompt = f"""
        Based on ML model analysis, suggest 3-5 practical applications or improvements 
        that could be immediately implemented to enhance trading performance.
        """

        practical_apps = chat_gpt(applications_prompt)

        # Generate methodology suggestions
        methodology_prompt = f"""
        Recommend 3-5 ML research methodology improvements based on current model portfolio.
        Focus on training approaches, validation techniques, and evaluation metrics.
        """

        methodology_suggestions = chat_gpt(methodology_prompt)

        # SIMPLE FIX: Use update_or_create instead of get_or_create
        # and get today's date at the start of the day
        today = datetime.now().date()
        today_start = datetime.combine(today, datetime.min.time())
        today_end = datetime.combine(today, datetime.max.time())

        summary_obj, created = SnowAIResearchGPTSummary.objects.update_or_create(
            created_at__range=[today_start, today_end],
            defaults={
                'summary_text': ai_summary,
                'total_research_entries': total_ml_models,
                'total_papers_analyzed': 0,
                'knowledge_gaps_identified': knowledge_gaps,
                'future_research_directions': future_directions,
                'cross_paper_insights': cross_insights,
                'practical_applications': practical_apps,
                'research_methodology_suggestions': methodology_suggestions,
            }
        )

        logger.info(f'ResearchGPT summary {"created" if created else "updated"} successfully for {total_ml_models} ML models')
        return True

    except Exception as e:
        logger.error(f'Error in generate_research_gpt_summary: {str(e)}')
        return False
        
        
# OPTIONAL: Manual trigger endpoint for testing
@csrf_exempt
# @require_http_methods(["POST"])
def manual_trigger_summaries(request):
    """Manually trigger summary generation - useful for testing"""
    try:
        results = generate_all_gpt_summaries()
        return JsonResponse({
            'status': 'success', 
            'message': 'Summary generation triggered',
            'results': results
        })
    except Exception as e:
        return JsonResponse({'status': 'error', 'message': str(e)})


@csrf_exempt
@require_http_methods(["GET"])
def fetch_custom_global_interest_rates_data_v2024(request):
    """
    Fetch interest rates data from local database instead of external API
    Returns data in the same format as the original API for frontend compatibility
    """
    try:
        # Define the mapping of currencies to their interest rate event names
        interest_rate_mappings = {
            'USD': 'Federal Funds Rate',
            'EUR': 'Main Refinancing Rate', 
            'GBP': 'Official Bank Rate',
            'JPY': 'BOJ Policy Rate',
            'AUD': 'Cash Rate',
            'CAD': 'Overnight Rate',
            'CHF': 'SNB Policy Rate',
            'CNY': '1-y Loan Prime Rate',  # Using 1-year as primary, you can change this
        }
        
        # Define central bank names for display
        central_bank_names = {
            'USD': 'Federal Reserve (USA)',
            'EUR': 'European Central Bank',
            'GBP': 'Bank of England',
            'JPY': 'Bank of Japan',
            'AUD': 'Reserve Bank of Australia', 
            'CAD': 'Bank of Canada',
            'CHF': 'Swiss National Bank',
            'CNY': 'People\'s Bank of China',
        }
        
        central_bank_rates = []
        
        for currency, event_name in interest_rate_mappings.items():
            try:
                # Get the most recent interest rate data for each currency
                latest_event = EconomicEvent.objects.filter(
                    currency=currency,
                    event_name=event_name,
                    actual__isnull=False
                ).exclude(
                    actual=''
                ).order_by('-date_time').first()
                
                if latest_event:
                    # Clean and convert the actual rate to float
                    actual_rate = latest_event.actual.strip()
                    
                    # Handle percentage signs and convert to float
                    if '%' in actual_rate:
                        rate_value = float(actual_rate.replace('%', ''))
                    else:
                        rate_value = float(actual_rate)
                    
                    central_bank_rates.append({
                        'central_bank': central_bank_names.get(currency, f'{currency} Central Bank'),
                        'currency': currency,
                        'rate_pct': rate_value,
                        'event_name': event_name,
                        'last_updated': latest_event.date_time.isoformat()
                    })
                else:
                    # If no data found, add with 0 rate or skip
                    central_bank_rates.append({
                        'central_bank': central_bank_names.get(currency, f'{currency} Central Bank'),
                        'currency': currency,
                        'rate_pct': 0.0,
                        'event_name': event_name,
                        'last_updated': None
                    })
                    
            except (ValueError, TypeError) as e:
                # Handle conversion errors
                print(f"Error processing {currency} rate: {e}")
                central_bank_rates.append({
                    'central_bank': central_bank_names.get(currency, f'{currency} Central Bank'),
                    'currency': currency,
                    'rate_pct': 0.0,
                    'event_name': event_name,
                    'last_updated': None
                })
        
        # Sort by rate for better visualization
        central_bank_rates.sort(key=lambda x: x['rate_pct'], reverse=True)
        
        # Format response to match the original API structure
        response_data = {
            'Interest Rates': json.dumps({
                'central_bank_rates': central_bank_rates,
                'data_source': 'local_database',
                'total_banks': len(central_bank_rates)
            })
        }
        
        return JsonResponse(response_data)
        
    except Exception as e:
        return JsonResponse({
            'error': 'Failed to fetch interest rates data',
            'message': str(e)
        }, status=500)


# Alternative view if you want to include both 1-year and 5-year CNY rates
@csrf_exempt 
@require_http_methods(["GET"])
def fetch_custom_global_interest_rates_extended_cny_v2024(request):
    """
    Extended version that includes both CNY 1-year and 5-year rates
    """
    try:
        # Extended mapping including both CNY rates
        interest_rate_mappings = {
            'USD': 'Federal Funds Rate',
            'EUR': 'Main Refinancing Rate',
            'GBP': 'Official Bank Rate', 
            'JPY': 'BOJ Policy Rate',
            'AUD': 'Cash Rate',
            'CAD': 'Overnight Rate',
            'CHF': 'SNB Policy Rate',
            'CNY_1Y': '1-y Loan Prime Rate',
            'CNY_5Y': '5-y Loan Prime Rate',
        }
        
        central_bank_names = {
            'USD': 'Federal Reserve (USA)',
            'EUR': 'European Central Bank',
            'GBP': 'Bank of England',
            'JPY': 'Bank of Japan', 
            'AUD': 'Reserve Bank of Australia',
            'CAD': 'Bank of Canada',
            'CHF': 'Swiss National Bank',
            'CNY_1Y': 'People\'s Bank of China (1Y)',
            'CNY_5Y': 'People\'s Bank of China (5Y)',
        }
        
        central_bank_rates = []
        
        for currency_key, event_name in interest_rate_mappings.items():
            try:
                # Extract base currency (handle CNY_1Y, CNY_5Y cases)
                base_currency = currency_key.split('_')[0] if '_' in currency_key else currency_key
                
                latest_event = EconomicEvent.objects.filter(
                    currency=base_currency,
                    event_name=event_name,
                    actual__isnull=False
                ).exclude(
                    actual=''
                ).order_by('-date_time').first()
                
                if latest_event:
                    actual_rate = latest_event.actual.strip()
                    
                    if '%' in actual_rate:
                        rate_value = float(actual_rate.replace('%', ''))
                    else:
                        rate_value = float(actual_rate)
                    
                    central_bank_rates.append({
                        'central_bank': central_bank_names.get(currency_key, f'{base_currency} Central Bank'),
                        'currency': base_currency,
                        'rate_pct': rate_value,
                        'event_name': event_name,
                        'last_updated': latest_event.date_time.isoformat()
                    })
                else:
                    central_bank_rates.append({
                        'central_bank': central_bank_names.get(currency_key, f'{base_currency} Central Bank'),
                        'currency': base_currency, 
                        'rate_pct': 0.0,
                        'event_name': event_name,
                        'last_updated': None
                    })
                    
            except (ValueError, TypeError) as e:
                print(f"Error processing {currency_key} rate: {e}")
                base_currency = currency_key.split('_')[0] if '_' in currency_key else currency_key
                central_bank_rates.append({
                    'central_bank': central_bank_names.get(currency_key, f'{base_currency} Central Bank'),
                    'currency': base_currency,
                    'rate_pct': 0.0,
                    'event_name': event_name,
                    'last_updated': None
                })
        
        central_bank_rates.sort(key=lambda x: x['rate_pct'], reverse=True)
        
        response_data = {
            'Interest Rates': json.dumps({
                'central_bank_rates': central_bank_rates,
                'data_source': 'local_database',
                'total_banks': len(central_bank_rates)
            })
        }
        
        return JsonResponse(response_data)
        
    except Exception as e:
        return JsonResponse({
            'error': 'Failed to fetch interest rates data',
            'message': str(e)
        }, status=500)


# Country to currency mapping
COUNTRY_CURRENCY_MAPPING = {
    'United States': 'USD',
    'USA': 'USD',
    'US': 'USD',
    'Canada': 'CAD',
    'United Kingdom': 'GBP',
    'UK': 'GBP',
    'Britain': 'GBP',
    'European Union': 'EUR',
    'Germany': 'EUR',
    'France': 'EUR',
    'Italy': 'EUR',
    'Spain': 'EUR',
    'Netherlands': 'EUR',
    'Belgium': 'EUR',
    'Austria': 'EUR',
    'Portugal': 'EUR',
    'Ireland': 'EUR',
    'Greece': 'EUR',
    'Finland': 'EUR',
    'Japan': 'JPY',
    'Australia': 'AUD',
    'Switzerland': 'CHF',
    'China': 'CNY',
    'Brazil': 'BRL',
    'Mexico': 'MXN',
    'South Africa': 'ZAR',
    'India': 'INR',
    'Russia': 'RUB',
    'South Korea': 'KRW',
    'Sweden': 'SEK',
    'Norway': 'NOK',
    'Denmark': 'DKK',
    'Poland': 'PLN',
    'Czech Republic': 'CZK',
    'Hungary': 'HUF',
    'Turkey': 'TRY',
    'Singapore': 'SGD',
    'Hong Kong': 'HKD',
    'New Zealand': 'NZD',
    'Thailand': 'THB',
    'Malaysia': 'MYR',
    'Indonesia': 'IDR',
    'Philippines': 'PHP',
    'Taiwan': 'TWD',
    'Israel': 'ILS',
    'Chile': 'CLP',
    'Colombia': 'COP',
    'Peru': 'PEN',
    'Argentina': 'ARS',
    'Egypt': 'EGP',
    'Saudi Arabia': 'SAR',
    'UAE': 'AED',
    'Kuwait': 'KWD',
    'Qatar': 'QAR'
}

def get_currency_from_country(country_name):
    """Convert country name to currency code"""
    # Try direct match first
    if country_name in COUNTRY_CURRENCY_MAPPING:
        return COUNTRY_CURRENCY_MAPPING[country_name]
    
    # Try partial matches (case insensitive)
    country_lower = country_name.lower()
    for country, currency in COUNTRY_CURRENCY_MAPPING.items():
        if country.lower() in country_lower or country_lower in country.lower():
            return currency
    
    return None

def chat_gpt_economic_analysis(economic_data, country_name, currency):
    """Generate AI analysis of economic data"""
    
    # Prepare the data for AI analysis
    events_summary = []
    for event in economic_data:
        events_summary.append({
            'date': event.date_time.strftime('%Y-%m-%d'),
            'event': event.event_name,
            'impact': event.impact,
            'actual': event.actual,
            'forecast': event.forecast,
            'previous': event.previous
        })
    
    prompt = f"""
    Analyze the following economic data for {country_name} ({currency}) over the last 30 days and provide a comprehensive economic summary.
    
    Economic Events:
    {json.dumps(events_summary, indent=2)}
    
    Please provide your analysis in the following JSON format (no markdown, just pure JSON):
    {{
        "country": "{country_name}",
        "currency": "{currency}",
        "analysis_period": "Last 30 days",
        "overall_sentiment": "positive/negative/neutral",
        "key_highlights": [
            "highlight 1",
            "highlight 2",
            "highlight 3"
        ],
        "economic_outlook": "Brief outlook summary",
        "major_events": [
            {{
                "event_name": "Event name",
                "impact_level": "high/medium/low",
                "summary": "Brief event summary"
            }}
        ],
        "risk_factors": [
            "risk factor 1",
            "risk factor 2"
        ],
        "opportunities": [
            "opportunity 1",
            "opportunity 2"
        ],
        "summary": "Overall economic summary in 2-3 sentences"
    }}
    
    Ensure the response is valid JSON only, no additional text or formatting.
    """
    
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        
        ai_response = response.choices[0].message['content'].strip()
        
        # Try to parse as JSON to validate
        try:
            json.loads(ai_response)
            return ai_response
        except json.JSONDecodeError:
            # If parsing fails, return a structured error response
            return json.dumps({
                "country": country_name,
                "currency": currency,
                "analysis_period": "Last 30 days",
                "overall_sentiment": "neutral",
                "key_highlights": ["Analysis could not be completed due to formatting issues"],
                "economic_outlook": "Unable to generate outlook",
                "major_events": [],
                "risk_factors": ["Analysis incomplete"],
                "opportunities": [],
                "summary": "Economic analysis could not be completed due to technical issues."
            })
            
    except Exception as e:
        return json.dumps({
            "country": country_name,
            "currency": currency,
            "analysis_period": "Last 30 days",
            "overall_sentiment": "neutral",
            "key_highlights": [f"Error generating analysis: {str(e)}"],
            "economic_outlook": "Analysis unavailable",
            "major_events": [],
            "risk_factors": ["Analysis service unavailable"],
            "opportunities": [],
            "summary": "Economic analysis is currently unavailable due to technical issues."
        })

@csrf_exempt
@require_http_methods(["POST"])
def get_country_economic_data(request):
    """Get economic data and AI analysis for a specific country"""
    try:
        data = json.loads(request.body)
        country_name = data.get('country_name', '').strip()
        
        if not country_name:
            return JsonResponse({
                'success': False,
                'error': 'Country name is required'
            }, status=400)
        
        # Get currency code from country name
        currency_code = get_currency_from_country(country_name)
        
        if not currency_code:
            return JsonResponse({
                'success': True,
                'has_data': False,
                'message': f'No economic data available for {country_name}',
                'country': country_name,
                'currency': None,
                'ai_analysis': json.dumps({
                    "country": country_name,
                    "currency": "N/A",
                    "analysis_period": "Last 30 days",
                    "overall_sentiment": "neutral",
                    "key_highlights": [f"No economic data tracking available for {country_name}"],
                    "economic_outlook": "Economic data not tracked for this region",
                    "major_events": [],
                    "risk_factors": ["Limited economic data availability"],
                    "opportunities": ["Economic data tracking could be expanded"],
                    "summary": f"Currently, we do not track economic indicators for {country_name}. Economic analysis is not available for this region."
                })
            })
        
        # Query economic events for the last 30 days
        thirty_days_ago = timezone.now() - timedelta(days=30)
        economic_events = EconomicEvent.objects.filter(
            currency=currency_code,
            date_time__gte=thirty_days_ago
        ).order_by('-date_time')
        
        if not economic_events.exists():
            return JsonResponse({
                'success': True,
                'has_data': False,
                'message': f'No recent economic data found for {country_name} ({currency_code})',
                'country': country_name,
                'currency': currency_code,
                'ai_analysis': json.dumps({
                    "country": country_name,
                    "currency": currency_code,
                    "analysis_period": "Last 30 days",
                    "overall_sentiment": "neutral",
                    "key_highlights": [f"No economic events recorded for {currency_code} in the last 30 days"],
                    "economic_outlook": "No recent data available for analysis",
                    "major_events": [],
                    "risk_factors": ["Limited recent economic data"],
                    "opportunities": ["Economic monitoring could be enhanced"],
                    "summary": f"No economic events have been recorded for {country_name} ({currency_code}) in the past 30 days."
                })
            })
        
        # Generate AI analysis
        ai_analysis = chat_gpt_economic_analysis(economic_events, country_name, currency_code)
        
        return JsonResponse({
            'success': True,
            'has_data': True,
            'message': f'Economic data found for {country_name} ({currency_code})',
            'country': country_name,
            'currency': currency_code,
            'events_count': economic_events.count(),
            'ai_analysis': ai_analysis
        })
        
    except json.JSONDecodeError:
        return JsonResponse({
            'success': False,
            'error': 'Invalid JSON data'
        }, status=400)
        
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': f'An error occurred: {str(e)}'
        }, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def trigger_manual_gpt_discussion(request):
    """Manually trigger a GPT discussion"""
    try:
        result = initiate_gpt_discussion(trigger_type='manual')
        return JsonResponse({
            'status': 'success', 
            'message': 'GPT discussion initiated successfully',
            'discussion_id': result.get('discussion_id'),
            'total_messages': result.get('total_messages', 0)
        })
    except Exception as e:
        return JsonResponse({'status': 'error', 'message': str(e)})

@csrf_exempt
@require_http_methods(["GET"])
def get_current_gpt_discussion(request):
    """Get the current GPT discussion and all messages"""
    try:
        # Get the current discussion
        discussion = GPTDiscussion.objects.filter(discussion_id='current_discussion').first()
        
        if not discussion:
            return JsonResponse({
                'status': 'success',
                'discussion': None,
                'messages': []
            })
        
        # Get all messages for this discussion
        messages = GPTDiscussionMessage.objects.filter(discussion=discussion).order_by('timestamp')
        
        messages_data = []
        for msg in messages:
            messages_data.append({
                'gpt_system': msg.gpt_system,
                'message': msg.message,
                'timestamp': msg.timestamp.isoformat(),
                'turn_number': msg.turn_number
            })
        
        discussion_data = {
            'discussion_id': discussion.discussion_id,
            'started_at': discussion.started_at.isoformat(),
            'completed_at': discussion.completed_at.isoformat() if discussion.completed_at else None,
            'is_active': discussion.is_active,
            'total_messages': discussion.total_messages,
            'central_gpt_summary': discussion.central_gpt_summary,
            'discussion_metrics': discussion.discussion_metrics,
            'trigger_type': discussion.trigger_type
        }
        
        return JsonResponse({
            'status': 'success',
            'discussion': discussion_data,
            'messages': messages_data
        })
        
    except Exception as e:
        return JsonResponse({'status': 'error', 'message': str(e)})
        

def initiate_gpt_discussion(trigger_type='scheduled'):
    """Main function to orchestrate GPT discussion with model data access"""
    try:
        # Delete any existing discussion
        GPTDiscussion.objects.filter(discussion_id='current_discussion').delete()
        
        # Create new discussion
        discussion = GPTDiscussion.objects.create(
            discussion_id='current_discussion',
            trigger_type=trigger_type,
            is_active=True
        )
        
        # Define GPT systems and their characteristics
        gpt_systems = {
            'TraderHistoryGPT': {'specialty': 'trading performance analysis', 'tone': 'analytical'},
            'MacroGPT': {'specialty': 'macro economic trends', 'tone': 'strategic'},
            'IdeaGPT': {'specialty': 'creative innovation', 'tone': 'enthusiastic'},
            'BacktestingGPT': {'specialty': 'quantitative strategy validation', 'tone': 'methodical'},
            'PaperGPT': {'specialty': 'research synthesis', 'tone': 'academic'},
            'ResearchGPT': {'specialty': 'comprehensive analysis', 'tone': 'insightful'}
        }
        
        # Generate discussion topic using IdeaGPT with its data context
        idea_data = get_idea_gpt_data()
        topic_prompt = f"""
        You are IdeaGPT, the creative innovation specialist for SnowAI. You're initiating a discussion 
        between all the specialized AI systems about SnowAI improvements and insights.
        
        Your current ideas data context:
        {idea_data}
        
        Generate a focused, creative discussion topic about SnowAI that each specialist can contribute 
        to meaningfully. The topic should be:
        - Concise (1-2 sentences)
        - Innovative and thought-provoking
        - Related to: system performance, user value, future improvements, or strategic insights
        - Something that would spark creative solutions based on current system data
        
        Just return the topic, nothing else.
        """
        
        discussion_topic = chat_gpt(topic_prompt).strip()
        
        # Store the topic as first message from IdeaGPT
        GPTDiscussionMessage.objects.create(
            discussion=discussion,
            gpt_system='IdeaGPT',
            message=f"Discussion Topic: {discussion_topic}",
            turn_number=0
        )
        
        # Conduct discussion rounds (3 rounds, each GPT speaks once per round)
        total_messages = 1  # Starting with topic message
        gpt_list = list(gpt_systems.keys())
        
        for round_num in range(1, 4):  # 3 rounds
            random.shuffle(gpt_list)  # Randomize speaking order each round
            
            for gpt_system in gpt_list:
                # Get previous messages for context
                previous_messages = list(GPTDiscussionMessage.objects.filter(
                    discussion=discussion
                ).order_by('timestamp'))
                
                context = f"Discussion Topic: {discussion_topic}\n\nPrevious messages:\n"
                # Get last 10 messages for context
                recent_messages = previous_messages[-10:] if len(previous_messages) > 10 else previous_messages
                for msg in recent_messages:
                    if msg.gpt_system != 'CentralGPT' or msg.turn_number == 0:
                        context += f"{msg.gpt_system}: {msg.message}\n"
                
                # Get system-specific data context
                system_data_context = get_system_data_context(gpt_system)
                
                # Generate response for this GPT with its data context
                gpt_prompt = f"""
                You are {gpt_system}, specializing in {gpt_systems[gpt_system]['specialty']}.
                You have a {gpt_systems[gpt_system]['tone']} personality.
                
                Your current system data context:
                {system_data_context}
                
                Discussion context:
                {context}
                
                Current round: {round_num}/3
                
                Instructions:
                - Contribute your unique perspective on SnowAI based on your specialty and current data
                - Use insights from your actual system data to inform your response
                - Keep your response concise (2-3 sentences max)
                - Reference or build upon previous comments when relevant
                - Stay focused on the discussion topic
                - Be conversational and collaborative
                - Don't repeat what others have already said
                - Ground your insights in the actual data you have access to
                
                Your response:
                """
                
                gpt_response = chat_gpt(gpt_prompt).strip()
                
                # Store the message
                GPTDiscussionMessage.objects.create(
                    discussion=discussion,
                    gpt_system=gpt_system,
                    message=gpt_response,
                    turn_number=round_num
                )
                
                total_messages += 1
        
        # Generate final summary using CentralGPT with all system data
        all_messages = GPTDiscussionMessage.objects.filter(
            discussion=discussion
        ).exclude(gpt_system='CentralGPT').order_by('timestamp')
        
        conversation_text = ""
        for msg in all_messages:
            conversation_text += f"{msg.gpt_system}: {msg.message}\n"
        
        # Get comprehensive system overview for CentralGPT
        comprehensive_data = get_comprehensive_system_data()
        
        summary_prompt = f"""
        You are CentralGPT analyzing a discussion between SnowAI's specialized systems.
        
        Discussion Topic: {discussion_topic}
        
        Full Conversation:
        {conversation_text}
        
        Complete System Data Overview:
        {comprehensive_data}
        
        Generate a comprehensive summary that includes:
        1. Key insights and themes that emerged from the data-informed discussion
        2. Areas of consensus among the systems based on their actual data
        3. Unique perspectives each system brought from their specialized data
        4. Actionable recommendations for SnowAI based on real system metrics
        5. Data-driven observations about system performance and opportunities
        
        Keep it structured and insightful, grounding recommendations in actual system data.
        """
        
        final_summary = chat_gpt(summary_prompt).strip()
        
        # Calculate metrics
        word_counts = {}
        sentiment_diversity = len(set([gpt_systems[gpt]['tone'] for gpt in gpt_systems.keys()]))
        
        for gpt in gpt_systems.keys():
            gpt_messages = all_messages.filter(gpt_system=gpt)
            total_words = sum(len(msg.message.split()) for msg in gpt_messages)
            word_counts[gpt] = total_words
        
        discussion_metrics = {
            'topic': discussion_topic,
            'rounds_completed': 3,
            'word_counts': word_counts,
            'total_words': sum(word_counts.values()),
            'participating_systems': len(gpt_systems),
            'sentiment_diversity_score': sentiment_diversity,
            'discussion_duration_minutes': round((timezone.now() - discussion.started_at).total_seconds() / 60, 2)
        }
        
        # Update discussion with final data
        discussion.completed_at = timezone.now()
        discussion.is_active = False
        discussion.total_messages = total_messages
        discussion.central_gpt_summary = final_summary
        discussion.discussion_metrics = discussion_metrics
        discussion.save()
        
        return {
            'status': 'success',
            'discussion_id': discussion.discussion_id,
            'total_messages': total_messages,
            'summary': final_summary[:200] + '...' if len(final_summary) > 200 else final_summary
        }
        
    except Exception as e:
        print(f'Error in GPT discussion: {e}')
        import traceback
        traceback.print_exc()
        return {'status': 'error', 'message': str(e)}


def get_system_data_context(gpt_system):
    """Get specific data context for each GPT system"""
    try:
        if gpt_system == 'TraderHistoryGPT':
            return get_trader_history_gpt_data()
        elif gpt_system == 'MacroGPT':
            return get_macro_gpt_data()
        elif gpt_system == 'IdeaGPT':
            return get_idea_gpt_data()
        elif gpt_system == 'BacktestingGPT':
            return get_backtesting_gpt_data()
        elif gpt_system == 'PaperGPT':
            return get_paper_gpt_data()
        elif gpt_system == 'ResearchGPT':
            return get_research_gpt_data()
        else:
            return "No specific data context available."
    except Exception as e:
        print(f"Error getting data context for {gpt_system}: {e}")
        return f"Error accessing {gpt_system} data context."


def get_trader_history_gpt_data():
    """Get trading data for TraderHistoryGPT"""
    try:
        recent_trades = AccountTrades.objects.all()[:30]
        total_trades = AccountTrades.objects.count()
        
        trades_data = []
        for trade in recent_trades:
            trade_dict = {}
            for field in trade._meta.fields:
                field_value = getattr(trade, field.name)
                if hasattr(field_value, 'isoformat'):
                    trade_dict[field.name] = field_value.isoformat()
                else:
                    trade_dict[field.name] = str(field_value) if field_value is not None else None
            trades_data.append(trade_dict)
        
        return f"""Trading Performance Data:
Total trades in system: {total_trades}

Recent trades data:
{json.dumps(trades_data, indent=2)}
"""
    except Exception as e:
        return f"Error accessing trading data: {e}"


def get_macro_gpt_data():
    """Get economic data for MacroGPT"""
    try:
        past_events = EconomicEvent.objects.filter(
            date_time__gte=datetime.now() - timedelta(days=30),
            date_time__lt=datetime.now()
        ).order_by('-date_time')[:20]
        
        events_data = []
        for event in past_events:
            event_dict = {}
            for field in event._meta.fields:
                field_value = getattr(event, field.name)
                if hasattr(field_value, 'isoformat'):
                    event_dict[field.name] = field_value.isoformat()
                else:
                    event_dict[field.name] = str(field_value) if field_value is not None else None
            events_data.append(event_dict)
        
        return f"""Economic Events Data:
Total events tracked: {EconomicEvent.objects.count()}

Past economic events (Last 30 days):
{json.dumps(events_data, indent=2)}
"""
    except Exception as e:
        return f"Error accessing economic data: {e}"


def get_idea_gpt_data():
    """Get ideas data for IdeaGPT"""
    try:
        recent_ideas = IdeaModel.objects.order_by('-created_at')[:15]
        total_ideas = IdeaModel.objects.count()
        
        ideas_data = []
        for idea in recent_ideas:
            idea_dict = {}
            for field in idea._meta.fields:
                field_value = getattr(idea, field.name)
                if hasattr(field_value, 'isoformat'):
                    idea_dict[field.name] = field_value.isoformat()
                else:
                    idea_dict[field.name] = str(field_value) if field_value is not None else None
            ideas_data.append(idea_dict)
        
        return f"""Ideas Management Data:
Total ideas in system: {total_ideas}

Recent ideas data:
{json.dumps(ideas_data, indent=2)}
"""
    except Exception as e:
        return f"Error accessing ideas data: {e}"


def get_backtesting_gpt_data():
    """Get backtesting data for BacktestingGPT"""
    try:
        recent_results = BacktestResult.objects.order_by('-created_at')[:10]
        backtest_models = BacktestModels.objects.all()[:10]
        
        results_data = []
        for result in recent_results:
            result_info = {
                'start': result.start.isoformat(),
                'end': result.end.isoformat(),
                'duration': result.duration,
                'return_percent': result.return_percent,
                'annual_return': result.annual_return,
                'sharpe_ratio': result.sharpe_ratio,
                'max_drawdown': result.max_drawdown,
                'num_trades': result.num_trades,
                'win_rate': result.win_rate,
                'equity_final': result.equity_final,
                'created_at': result.created_at.isoformat()
            }
            results_data.append(result_info)
        
        models_data = []
        for model in backtest_models:
            model_info = {
                'chosen_dataset': model.chosen_dataset,
                'dataset_start': model.dataset_start,
                'dataset_end': model.dataset_end,
                'initial_capital': model.initial_capital,
                'model_backtested': model.model_backtested,
                'generated_code': model.generated_code[:1000] + "..." if model.generated_code and len(model.generated_code) > 1000 else model.generated_code
            }
            models_data.append(model_info)
        
        return f"""Backtesting Performance Data:
Total backtest results: {BacktestResult.objects.count()}
Total backtest models: {BacktestModels.objects.count()}

Recent backtest results:
{json.dumps(results_data, indent=2)}

Backtest models data:
{json.dumps(models_data, indent=2)}
"""
    except Exception as e:
        return f"Error accessing backtesting data: {e}"


def get_paper_gpt_data():
    """Get research papers data for PaperGPT"""
    try:
        recent_papers = PaperGPT.objects.order_by('-upload_date')[:10]
        total_papers = PaperGPT.objects.count()
        
        papers_data = []
        for paper in recent_papers:
            paper_info = {
                'title': paper.title,
                'category': paper.category,
                'upload_date': paper.upload_date.isoformat(),
                'file_name': paper.file_name,
                'ai_summary': paper.ai_summary[:300] + "..." if paper.ai_summary and len(paper.ai_summary) > 300 else paper.ai_summary,
                'has_notes': bool(paper.personal_notes),
                'text_length': len(paper.extracted_text) if paper.extracted_text else 0
            }
            papers_data.append(paper_info)
        
        return f"""Research Papers Data:
Total papers in collection: {total_papers}

Recent papers data:
{json.dumps(papers_data, indent=2)}
"""
    except Exception as e:
        return f"Error accessing papers data: {e}"


def get_research_gpt_data():
    """Get ML research data for ResearchGPT"""
    try:
        ml_models = SnowAIMLModelLogEntry.objects.all()[:10]
        total_models = SnowAIMLModelLogEntry.objects.count()
        
        ml_models_data = []
        for model in ml_models:
            model_dict = {}
            for field in model._meta.fields:
                field_value = getattr(model, field.name)
                if hasattr(field_value, 'isoformat'):
                    model_dict[field.name] = field_value.isoformat()
                else:
                    model_dict[field.name] = str(field_value) if field_value is not None else None
            ml_models_data.append(model_dict)
        
        return f"""Research & ML Data:
Total ML models tracked: {total_models}

ML Models data:
{json.dumps(ml_models_data, indent=2)}
"""
    except Exception as e:
        return f"Error accessing research data: {e}"


def get_comprehensive_system_data():
    """Get overview of all system data for CentralGPT"""
    try:
        # Get actual data summaries from each system
        trading_data = get_trader_history_gpt_data()
        macro_data = get_macro_gpt_data()
        ideas_data = get_idea_gpt_data()
        backtesting_data = get_backtesting_gpt_data()
        papers_data = get_paper_gpt_data()
        research_data = get_research_gpt_data()
        
        overview = {
            'system_metrics': {
                'total_trades': AccountTrades.objects.count(),
                'total_economic_events': EconomicEvent.objects.count(),
                'total_ideas': IdeaModel.objects.count(),
                'total_backtest_results': BacktestResult.objects.count(),
                'total_backtest_models': BacktestModels.objects.count(),
                'total_papers': PaperGPT.objects.count(),
                'total_ml_models': SnowAIMLModelLogEntry.objects.count(),
                'recent_activity': {
                    'recent_trades': AccountTrades.objects.filter(
                        created_at__gte=datetime.now() - timedelta(days=7)
                    ).count() if hasattr(AccountTrades._meta.get_field('created_at'), 'name') else 'N/A',
                    'recent_events': EconomicEvent.objects.filter(
                        date_time__gte=datetime.now() - timedelta(days=7)
                    ).count(),
                    'recent_ideas': IdeaModel.objects.filter(
                        created_at__gte=datetime.now() - timedelta(days=7)
                    ).count(),
                    'recent_backtests': BacktestResult.objects.filter(
                        created_at__gte=datetime.now() - timedelta(days=7)
                    ).count()
                }
            }
        }
        
        return f"""Comprehensive System Data Overview:

System Metrics Summary:
{json.dumps(overview, indent=2)}

TraderHistoryGPT Data Context:
{trading_data}

MacroGPT Data Context:
{macro_data}

IdeaGPT Data Context:
{ideas_data}

BacktestingGPT Data Context:
{backtesting_data}

PaperGPT Data Context:
{papers_data}

ResearchGPT Data Context:
{research_data}
"""
    except Exception as e:
        return f"Error accessing comprehensive system data: {e}"



import pytz
import statistics
@csrf_exempt
@require_http_methods(["GET"])
def fetch_snowai_accounts_for_deep_analysis(request):
    """Fetch all account names for selection"""
    try:
        accounts = Account.objects.all().values('id', 'account_name', 'main_assets', 'initial_capital')
        accounts_list = list(accounts)
        
        return JsonResponse({
            'success': True,
            'accounts': accounts_list
        })
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        })

@csrf_exempt
@require_http_methods(["GET"])
def calculate_deep_account_performance_metrics(request, account_id):
    """Calculate comprehensive diagnostics for a specific account - FIXED VERSION"""
    try:
        # Get South African timezone
        sa_tz = pytz.timezone('Africa/Johannesburg')
        current_time = timezone.now().astimezone(sa_tz)
        
        # Get account and all trades for this specific account
        account = Account.objects.get(id=account_id)
        all_trades = AccountTrades.objects.filter(account=account)
        
        # Basic Performance Metrics
        total_trades = all_trades.count()
        profitable_trades = all_trades.filter(outcome='Win').count()
        losing_trades = all_trades.filter(outcome='Loss').count()
        breakeven_trades = all_trades.filter(outcome='Break Even').count()
        
        win_rate = (profitable_trades / total_trades * 100) if total_trades > 0 else 0
        
        # Calculate total P&L
        total_profit = sum(abs(trade.amount) for trade in all_trades.filter(outcome='Win'))
        total_loss = sum(abs(trade.amount) for trade in all_trades.filter(outcome='Loss'))
        net_pnl = total_profit - total_loss
        
        # Average trade metrics
        avg_win = total_profit / profitable_trades if profitable_trades > 0 else 0
        avg_loss = total_loss / losing_trades if losing_trades > 0 else 0
        
        # Risk-Reward Ratio
        risk_reward_ratio = avg_win / avg_loss if avg_loss > 0 else 0
        
        # Profit Factor
        profit_factor = total_profit / total_loss if total_loss > 0 else float('inf')
        
        # FIXED: Time-based Analysis
        current_day = current_time.strftime('%A')
        current_hour = current_time.hour
        current_session = get_trading_session_advanced(current_hour)
        
        # FIXED: Get performance for current day - make sure we're filtering by exact account
        day_performance = get_current_day_performance(account, current_day)
        
        # FIXED: Get performance for current session - make sure we're filtering by exact account
        session_performance = get_current_session_performance(account, current_session)
        
        # FIXED: Get combined time performance (current day AND current session)
        combined_time_performance = get_combined_time_performance(account, current_day, current_session)
        
        # Calculate the most accurate time-based probability
        time_based_win_probability = calculate_refined_time_probability(
            day_performance, 
            session_performance, 
            combined_time_performance, 
            win_rate
        )
        
        # Asset Performance Analysis
        asset_performance = analyze_asset_performance(all_trades)
        
        # Strategy Performance Analysis
        strategy_performance = analyze_strategy_performance(all_trades)
        
        # Emotional Bias Analysis
        emotional_analysis = analyze_emotional_patterns(all_trades)
        
        # Streaks Analysis
        streaks_data = analyze_winning_losing_streaks(all_trades)
        
        # Monthly Performance Trend
        monthly_performance = calculate_monthly_performance_trend(all_trades)
        
        # Drawdown Analysis
        drawdown_analysis = calculate_drawdown_metrics(all_trades, account.initial_capital)
        
        # Risk Metrics
        risk_metrics = calculate_advanced_risk_metrics(all_trades, account.initial_capital)
        
        return JsonResponse({
            'success': True,
            'account_info': {
                'name': account.account_name,
                'main_assets': account.main_assets,
                'initial_capital': account.initial_capital
            },
            'basic_metrics': {
                'total_trades': total_trades,
                'profitable_trades': profitable_trades,
                'losing_trades': losing_trades,
                'breakeven_trades': breakeven_trades,
                'win_rate': round(win_rate, 2),
                'net_pnl': round(net_pnl, 2),
                'avg_win': round(avg_win, 2),
                'avg_loss': round(avg_loss, 2),
                'risk_reward_ratio': round(risk_reward_ratio, 2),
                'profit_factor': round(profit_factor, 2)
            },
            'time_analysis': {
                'current_day': current_day,
                'current_session': current_session,
                'current_time': current_time.strftime('%H:%M %Z'),
                'time_based_win_probability': round(time_based_win_probability, 2),
                'day_performance': day_performance,
                'session_performance': session_performance,
                'combined_time_performance': combined_time_performance,  # Added for debugging
                'debug_info': {
                    'account_id': account_id,
                    'total_account_trades': total_trades,
                    'current_day_trades': day_performance['trade_count'],
                    'current_session_trades': session_performance['trade_count'],
                    'combined_time_trades': combined_time_performance['trade_count']
                }
            },
            'asset_performance': asset_performance,
            'strategy_performance': strategy_performance,
            'emotional_analysis': emotional_analysis,
            'streaks_data': streaks_data,
            'monthly_performance': monthly_performance,
            'drawdown_analysis': drawdown_analysis,
            'risk_metrics': risk_metrics
        })
        
    except Account.DoesNotExist:
        return JsonResponse({
            'success': False,
            'error': 'Account not found'
        })
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        })


@csrf_exempt
@require_http_methods(["POST"])
def generate_ai_enhanced_account_diagnostics(request, account_id):
    """Generate AI-powered diagnostic insights using GPT-4o-mini"""
    try:
        data = json.loads(request.body)
        performance_data = data.get('performance_data', {})
        
        # Set up OpenAI API key
        openai.api_key = os.getenv('OPENAI_API_KEY')
        
        # Create comprehensive prompt for AI analysis
        prompt = create_ai_diagnostic_prompt(account_id, performance_data)
        
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert trading performance analyst with deep expertise in risk management, behavioral finance, and trading psychology. Provide actionable insights and recommendations."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            max_tokens=1500,
            temperature=0.7
        )
        
        ai_insights = response.choices[0].message.content
        
        return JsonResponse({
            'success': True,
            'ai_insights': ai_insights,
            'generated_at': timezone.now().isoformat()
        })
        
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        })

# Helper Functions

def get_trading_session_advanced(hour):
    """Determine trading session based on hour (SA time) - FIXED"""
    if 9 <= hour < 17:
        return "London"
    elif 15 <= hour < 23:
        return "NY"
    elif 2 <= hour < 10:
        return "Asian"
    else:
        return "Off-Hours"


def get_current_day_performance(account, current_day):
    """Get performance for current day of week - FIXED to use specific account"""
    # Filter trades for this specific account and current day
    day_trades = AccountTrades.objects.filter(
        account=account,
        day_of_week_entered__iexact=current_day  # Case insensitive match
    )
    
    if not day_trades.exists():
        return {
            'win_rate': 0, 
            'trade_count': 0, 
            'confidence': 'No Data',
            'wins': 0,
            'losses': 0
        }
    
    total = day_trades.count()
    wins = day_trades.filter(outcome='Win').count()
    losses = day_trades.filter(outcome='Loss').count()
    win_rate = (wins / total * 100) if total > 0 else 0
    
    # Confidence based on sample size
    if total >= 20:
        confidence = 'High'
    elif total >= 10:
        confidence = 'Medium'
    elif total >= 5:
        confidence = 'Low'
    else:
        confidence = 'Very Low'
    
    return {
        'win_rate': round(win_rate, 2),
        'trade_count': total,
        'wins': wins,
        'losses': losses,
        'confidence': confidence
    }

def get_current_session_performance(account, current_session):
    """Get performance for current trading session - FIXED to use specific account"""
    # Filter trades for this specific account and current session
    session_trades = AccountTrades.objects.filter(
        account=account,
        trading_session_entered__iexact=current_session  # Case insensitive match
    )
    
    if not session_trades.exists():
        return {
            'win_rate': 0, 
            'trade_count': 0, 
            'confidence': 'No Data',
            'wins': 0,
            'losses': 0
        }
    
    total = session_trades.count()
    wins = session_trades.filter(outcome='Win').count()
    losses = session_trades.filter(outcome='Loss').count()
    win_rate = (wins / total * 100) if total > 0 else 0
    
    # Confidence based on sample size
    if total >= 25:
        confidence = 'High'
    elif total >= 15:
        confidence = 'Medium'
    elif total >= 8:
        confidence = 'Low'
    else:
        confidence = 'Very Low'
    
    return {
        'win_rate': round(win_rate, 2),
        'trade_count': total,
        'wins': wins,
        'losses': losses,
        'confidence': confidence
    }


def get_combined_time_performance(account, current_day, current_session):
    """Get performance for current day AND current session combined - NEW FUNCTION"""
    # Filter trades for this specific account, current day AND current session
    combined_trades = AccountTrades.objects.filter(
        account=account,
        day_of_week_entered__iexact=current_day,
        trading_session_entered__iexact=current_session
    )
    
    if not combined_trades.exists():
        return {
            'win_rate': 0, 
            'trade_count': 0, 
            'confidence': 'No Data',
            'wins': 0,
            'losses': 0
        }
    
    total = combined_trades.count()
    wins = combined_trades.filter(outcome='Win').count()
    losses = combined_trades.filter(outcome='Loss').count()
    win_rate = (wins / total * 100) if total > 0 else 0
    
    # Confidence based on sample size for combined data
    if total >= 15:
        confidence = 'High'
    elif total >= 8:
        confidence = 'Medium'
    elif total >= 4:
        confidence = 'Low'
    else:
        confidence = 'Very Low'
    
    return {
        'win_rate': round(win_rate, 2),
        'trade_count': total,
        'wins': wins,
        'losses': losses,
        'confidence': confidence
    }


def calculate_refined_time_probability(day_perf, session_perf, combined_perf, overall_win_rate):
    """Calculate the most accurate time-based probability - IMPROVED ALGORITHM"""
    
    # Priority system: Combined time data is most specific and valuable
    if combined_perf['trade_count'] >= 5:
        # We have enough combined data (same day + same session)
        primary_rate = combined_perf['win_rate']
        confidence_weight = 0.8  # High weight for combined data
        
        # Supplement with overall win rate
        final_probability = (primary_rate * confidence_weight) + (overall_win_rate * (1 - confidence_weight))
        
    elif day_perf['trade_count'] >= 5 and session_perf['trade_count'] >= 5:
        # We have good data for both day and session separately
        day_weight = min(day_perf['trade_count'] / 20, 0.4)  # Max 40% weight
        session_weight = min(session_perf['trade_count'] / 25, 0.4)  # Max 40% weight
        overall_weight = 1 - day_weight - session_weight
        
        final_probability = (
            day_perf['win_rate'] * day_weight +
            session_perf['win_rate'] * session_weight +
            overall_win_rate * overall_weight
        )
        
    elif day_perf['trade_count'] >= 5:
        # Only day data is reliable
        day_weight = min(day_perf['trade_count'] / 20, 0.6)  # Max 60% weight
        final_probability = (day_perf['win_rate'] * day_weight) + (overall_win_rate * (1 - day_weight))
        
    elif session_perf['trade_count'] >= 5:
        # Only session data is reliable
        session_weight = min(session_perf['trade_count'] / 25, 0.6)  # Max 60% weight
        final_probability = (session_perf['win_rate'] * session_weight) + (overall_win_rate * (1 - session_weight))
        
    else:
        # Not enough specific time data, use overall with slight adjustments
        day_adjustment = (day_perf['win_rate'] - overall_win_rate) * 0.1 if day_perf['trade_count'] > 0 else 0
        session_adjustment = (session_perf['win_rate'] - overall_win_rate) * 0.1 if session_perf['trade_count'] > 0 else 0
        
        final_probability = overall_win_rate + day_adjustment + session_adjustment
    
    # Ensure probability stays within reasonable bounds
    return max(0, min(100, final_probability))



def calculate_day_performance_probability(day_trades):
    """Calculate win probability for specific day"""
    if not day_trades.exists():
        return {'win_rate': 0, 'trade_count': 0, 'confidence': 'Low'}
    
    total = day_trades.count()
    # Fixed: Use 'Win' instead of 'Profit'
    wins = day_trades.filter(outcome='Win').count()
    win_rate = (wins / total * 100) if total > 0 else 0
    
    confidence = 'High' if total >= 10 else 'Medium' if total >= 5 else 'Low'
    
    return {
        'win_rate': round(win_rate, 2),
        'trade_count': total,
        'confidence': confidence
    }

def calculate_session_performance_probability(session_trades):
    """Calculate win probability for specific session"""
    if not session_trades.exists():
        return {'win_rate': 0, 'trade_count': 0, 'confidence': 'Low'}
    
    total = session_trades.count()
    # Fixed: Use 'Win' instead of 'Profit'
    wins = session_trades.filter(outcome='Win').count()
    win_rate = (wins / total * 100) if total > 0 else 0
    
    confidence = 'High' if total >= 15 else 'Medium' if total >= 8 else 'Low'
    
    return {
        'win_rate': round(win_rate, 2),
        'trade_count': total,
        'confidence': confidence
    }

def calculate_time_based_win_probability(day_perf, session_perf, overall_win_rate):
    """Combine day and session performance with overall win rate"""
    weights = {
        'day': 0.3,
        'session': 0.3,
        'overall': 0.4
    }
    
    day_rate = day_perf['win_rate'] if day_perf['trade_count'] > 0 else overall_win_rate
    session_rate = session_perf['win_rate'] if session_perf['trade_count'] > 0 else overall_win_rate
    
    weighted_probability = (
        day_rate * weights['day'] +
        session_rate * weights['session'] +
        overall_win_rate * weights['overall']
    )
    
    return weighted_probability

def analyze_asset_performance(trades):
    """Analyze performance by asset"""
    asset_stats = defaultdict(lambda: {'wins': 0, 'losses': 0, 'total_pnl': 0})
    
    for trade in trades:
        # Fixed: Properly calculate P&L based on outcome
        if trade.outcome == 'Win':
            asset_stats[trade.asset]['total_pnl'] += abs(trade.amount)
            asset_stats[trade.asset]['wins'] += 1
        elif trade.outcome == 'Loss':
            asset_stats[trade.asset]['total_pnl'] -= abs(trade.amount)
            asset_stats[trade.asset]['losses'] += 1
        # Break Even trades don't affect P&L but count as trades
    
    result = []
    for asset, stats in asset_stats.items():
        total_trades = stats['wins'] + stats['losses']
        win_rate = (stats['wins'] / total_trades * 100) if total_trades > 0 else 0
        
        result.append({
            'asset': asset,
            'total_trades': total_trades,
            'win_rate': round(win_rate, 2),
            'total_pnl': round(stats['total_pnl'], 2),
            'wins': stats['wins'],
            'losses': stats['losses']
        })
    
    return sorted(result, key=lambda x: x['total_pnl'], reverse=True)

def analyze_strategy_performance(trades):
    """Analyze performance by strategy"""
    strategy_stats = defaultdict(lambda: {'wins': 0, 'losses': 0, 'total_pnl': 0})
    
    for trade in trades:
        # Fixed: Properly calculate P&L based on outcome
        if trade.outcome == 'Win':
            strategy_stats[trade.strategy]['total_pnl'] += abs(trade.amount)
            strategy_stats[trade.strategy]['wins'] += 1
        elif trade.outcome == 'Loss':
            strategy_stats[trade.strategy]['total_pnl'] -= abs(trade.amount)
            strategy_stats[trade.strategy]['losses'] += 1
    
    result = []
    for strategy, stats in strategy_stats.items():
        total_trades = stats['wins'] + stats['losses']
        win_rate = (stats['wins'] / total_trades * 100) if total_trades > 0 else 0
        
        result.append({
            'strategy': strategy,
            'total_trades': total_trades,
            'win_rate': round(win_rate, 2),
            'total_pnl': round(stats['total_pnl'], 2),
            'wins': stats['wins'],
            'losses': stats['losses']
        })
    
    return sorted(result, key=lambda x: x['win_rate'], reverse=True)

def analyze_emotional_patterns(trades):
    """Analyze emotional bias patterns"""
    trades_with_emotion = trades.exclude(emotional_bias__isnull=True).exclude(emotional_bias='')
    
    if not trades_with_emotion.exists():
        return {'has_data': False, 'message': 'No emotional bias data available'}
    
    emotion_performance = defaultdict(lambda: {'count': 0, 'wins': 0, 'total_pnl': 0})
    
    for trade in trades_with_emotion:
        emotion = trade.emotional_bias.lower()
        emotion_performance[emotion]['count'] += 1
        
        # Fixed: Properly calculate P&L and wins
        if trade.outcome == 'Win':
            emotion_performance[emotion]['total_pnl'] += abs(trade.amount)
            emotion_performance[emotion]['wins'] += 1
        elif trade.outcome == 'Loss':
            emotion_performance[emotion]['total_pnl'] -= abs(trade.amount)
    
    result = []
    for emotion, stats in emotion_performance.items():
        win_rate = (stats['wins'] / stats['count'] * 100) if stats['count'] > 0 else 0
        result.append({
            'emotion': emotion.title(),
            'trade_count': stats['count'],
            'win_rate': round(win_rate, 2),
            'avg_pnl': round(stats['total_pnl'] / stats['count'], 2)
        })
    
    return {
        'has_data': True,
        'patterns': sorted(result, key=lambda x: x['trade_count'], reverse=True)
    }

def analyze_winning_losing_streaks(trades):
    """Analyze winning and losing streaks"""
    if not trades.exists():
        return {'max_winning_streak': 0, 'max_losing_streak': 0, 'current_streak': 0}
    
    # Order trades by date
    ordered_trades = trades.order_by('date_entered')
    
    max_winning_streak = 0
    max_losing_streak = 0
    current_streak = 0
    current_streak_type = None
    
    temp_winning = 0
    temp_losing = 0
    
    for trade in ordered_trades:
        # Fixed: Use 'Win' instead of 'Profit'
        if trade.outcome == 'Win':
            temp_winning += 1
            temp_losing = 0
            max_winning_streak = max(max_winning_streak, temp_winning)
            
            if current_streak_type == 'win':
                current_streak += 1
            else:
                current_streak = 1
                current_streak_type = 'win'
        elif trade.outcome == 'Loss':
            temp_losing += 1
            temp_winning = 0
            max_losing_streak = max(max_losing_streak, temp_losing)
            
            if current_streak_type == 'loss':
                current_streak += 1
            else:
                current_streak = 1
                current_streak_type = 'loss'
        else:  # Break Even - breaks both streaks
            temp_winning = 0
            temp_losing = 0
            current_streak = 0
            current_streak_type = 'none'
    
    return {
        'max_winning_streak': max_winning_streak,
        'max_losing_streak': max_losing_streak,
        'current_streak': current_streak,
        'current_streak_type': current_streak_type or 'none'
    }

def calculate_monthly_performance_trend(trades):
    """Calculate monthly performance trend"""
    monthly_data = defaultdict(lambda: {'pnl': 0, 'trades': 0, 'wins': 0})
    
    for trade in trades:
        if trade.date_entered:
            month_key = trade.date_entered.strftime('%Y-%m')
            monthly_data[month_key]['trades'] += 1
            
            # Fixed: Properly calculate monthly P&L
            if trade.outcome == 'Win':
                monthly_data[month_key]['pnl'] += abs(trade.amount)
                monthly_data[month_key]['wins'] += 1
            elif trade.outcome == 'Loss':
                monthly_data[month_key]['pnl'] -= abs(trade.amount)
    
    result = []
    for month, data in sorted(monthly_data.items()):
        win_rate = (data['wins'] / data['trades'] * 100) if data['trades'] > 0 else 0
        result.append({
            'month': month,
            'pnl': round(data['pnl'], 2),
            'trades': data['trades'],
            'win_rate': round(win_rate, 2)
        })
    
    return result

def calculate_drawdown_metrics(trades, initial_capital):
    """Calculate drawdown metrics"""
    if not trades.exists():
        return {'max_drawdown': 0, 'current_drawdown': 0, 'drawdown_periods': []}
    
    # Order trades by date and calculate running balance
    ordered_trades = trades.order_by('date_entered')
    running_balance = initial_capital
    peak_balance = initial_capital
    max_drawdown = 0
    drawdowns = []
    
    for trade in ordered_trades:
        # Fixed: Properly calculate running balance
        if trade.outcome == 'Win':
            running_balance += abs(trade.amount)
        elif trade.outcome == 'Loss':
            running_balance -= abs(trade.amount)
        # Break Even trades don't change balance
        
        if running_balance > peak_balance:
            peak_balance = running_balance
        
        drawdown_pct = ((peak_balance - running_balance) / peak_balance * 100) if peak_balance > 0 else 0
        max_drawdown = max(max_drawdown, drawdown_pct)
        
        if drawdown_pct > 5:  # Track significant drawdowns
            drawdowns.append({
                'date': trade.date_entered.strftime('%Y-%m-%d') if trade.date_entered else 'Unknown',
                'drawdown_pct': round(drawdown_pct, 2),
                'balance': round(running_balance, 2)
            })
    
    current_drawdown = ((peak_balance - running_balance) / peak_balance * 100) if peak_balance > 0 else 0
    
    return {
        'max_drawdown': round(max_drawdown, 2),
        'current_drawdown': round(current_drawdown, 2),
        'current_balance': round(running_balance, 2),
        'peak_balance': round(peak_balance, 2),
        'significant_drawdowns': drawdowns[-5:]  # Last 5 significant drawdowns
    }

def calculate_advanced_risk_metrics(trades, initial_capital):
    """Calculate advanced risk metrics"""
    if not trades.exists():
        return {}
    
    # Fixed: Calculate returns properly based on outcome
    returns = []
    for trade in trades:
        if trade.outcome == 'Win':
            returns.append(abs(trade.amount) / initial_capital * 100)
        elif trade.outcome == 'Loss':
            returns.append(-abs(trade.amount) / initial_capital * 100)
        # Break Even trades have 0 return - skip or add 0
    
    # Calculate Sharpe-like ratio (simplified)
    avg_return = statistics.mean(returns) if returns else 0
    return_std = statistics.stdev(returns) if len(returns) > 1 else 0
    sharpe_ratio = avg_return / return_std if return_std > 0 else 0
    
    # Calculate Value at Risk (VaR) - 95% confidence
    sorted_returns = sorted(returns)
    var_95 = sorted_returns[int(0.05 * len(sorted_returns))] if len(sorted_returns) > 20 else min(returns) if returns else 0
    
    # Calculate Maximum Consecutive Losses
    max_consecutive_losses = 0
    consecutive_losses = 0
    
    for trade in trades.order_by('date_entered'):
        if trade.outcome == 'Loss':
            consecutive_losses += 1
            max_consecutive_losses = max(max_consecutive_losses, consecutive_losses)
        else:
            consecutive_losses = 0
    
    return {
        'sharpe_ratio': round(sharpe_ratio, 3),
        'var_95': round(var_95, 2),
        'volatility': round(return_std, 2),
        'max_consecutive_losses': max_consecutive_losses,
        'avg_return_pct': round(avg_return, 2)
    }

def create_ai_diagnostic_prompt(account_id, performance_data):
    """Create comprehensive prompt for AI analysis"""
    prompt = f"""
    Please analyze the following trading account performance data and provide expert insights:

    ACCOUNT OVERVIEW:
    - Account: {performance_data.get('account_info', {}).get('name', 'Unknown')}
    - Initial Capital: ${performance_data.get('account_info', {}).get('initial_capital', 0):,.2f}
    - Main Assets: {performance_data.get('account_info', {}).get('main_assets', 'Unknown')}

    PERFORMANCE METRICS:
    - Total Trades: {performance_data.get('basic_metrics', {}).get('total_trades', 0)}
    - Win Rate: {performance_data.get('basic_metrics', {}).get('win_rate', 0)}%
    - Net P&L: ${performance_data.get('basic_metrics', {}).get('net_pnl', 0):,.2f}
    - Risk/Reward Ratio: {performance_data.get('basic_metrics', {}).get('risk_reward_ratio', 0)}
    - Profit Factor: {performance_data.get('basic_metrics', {}).get('profit_factor', 0)}

    CURRENT TIME ANALYSIS:
    - Current Day: {performance_data.get('time_analysis', {}).get('current_day', 'Unknown')}
    - Current Session: {performance_data.get('time_analysis', {}).get('current_session', 'Unknown')}
    - Time-based Win Probability: {performance_data.get('time_analysis', {}).get('time_based_win_probability', 0)}%

    RISK METRICS:
    - Max Drawdown: {performance_data.get('drawdown_analysis', {}).get('max_drawdown', 0)}%
    - Sharpe Ratio: {performance_data.get('risk_metrics', {}).get('sharpe_ratio', 0)}
    - Volatility: {performance_data.get('risk_metrics', {}).get('volatility', 0)}%

    Based on this data, please provide:
    1. Overall performance assessment
    2. Key strengths and weaknesses
    3. Risk management evaluation
    4. Specific actionable recommendations
    5. Psychological/behavioral insights
    6. Market timing analysis
    7. Strategic improvements for better performance

    Keep the analysis concise but comprehensive, focusing on actionable insights.
    """
    
    return prompt

import json
import uuid
import re
import subprocess
import tempfile
import os
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.core.paginator import Paginator
from django.db.models import Q

# Try to import yt-dlp
try:
    import yt_dlp
    YT_DLP_AVAILABLE = True
except ImportError:
    YT_DLP_AVAILABLE = False
    yt_dlp = None

def extract_youtube_video_id_from_url(youtube_url):
    """Extract video ID from various YouTube URL formats"""
    patterns = [
        r'(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([^&\n?#]+)',
        r'youtube\.com\/watch\?.*v=([^&\n?#]+)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, youtube_url)
        if match:
            return match.group(1)
    return None


def create_cookie_jar_from_dict(cookies_dict):
    """Create a temporary cookie jar file from a dictionary of cookies"""
    import tempfile
    import http.cookiejar
    import urllib.parse
    
    # Create a temporary file for cookies
    cookie_jar = http.cookiejar.MozillaCookieJar()
    
    # Add cookies to jar
    for name, value in cookies_dict.items():
        cookie = http.cookiejar.Cookie(
            version=0,
            name=name,
            value=value,
            port=None,
            port_specified=False,
            domain='.youtube.com',
            domain_specified=True,
            domain_initial_dot=True,
            path='/',
            path_specified=True,
            secure=True,
            expires=None,
            discard=True,
            comment=None,
            comment_url=None,
            rest={}
        )
        cookie_jar.set_cookie(cookie)
    
    # Save to temporary file
    temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt')
    cookie_jar.save(temp_file.name, ignore_discard=True, ignore_expires=True)
    temp_file.close()
    
    return temp_file.name


def get_default_youtube_cookies():
    """Return updated default YouTube cookies for bot detection bypass"""
    return {
        "GPS": "1",
        "YSC": "FHn3n08s0P8",
        "VISITOR_INFO1_LIVE": "31Mb8-Hdb-0",
        "VISITOR_PRIVACY_METADATA": "CgJaQRIEGgAgMg%3D%3D",
        "__Secure-1PSIDTS": "sidts-CjUBwQ9iIzpwYQrC7xJOzg-aYyKKiUztj7mPKgbTOOt6bV9soOYJeZKtreTueu2Te6EmBTQ6dhAA",
        "__Secure-3PSIDTS": "sidts-CjUBwQ9iIzpwYQrC7xJOzg-aYyKKiUztj7mPKgbTOOt6bV9soOYJeZKtreTueu2Te6EmBTQ6dhAA",
        "__Secure-3PAPISID": "7aph5Sq9un6IOJTG/AvNR8-Tar5Osbfu0Y",
        "__Secure-3PSID": "g.a0002whVj5rgUCOba7GdCCIe0M1_QjggulPFhyemJoDIUgOZF-JBtfk4VjnnDYoFB3wngyYH0QACgYKAX4SARESFQHGX2Mi-zPpLfUbqvKuTrJGo-QGGhoVAUF8yKqBkSkURZaFJYwrVSrdRDlZ0076",
        "LOGIN_INFO": "AFmmF2swRQIgEeG6is4-vdZGXq3D9EUne1RvXw-xSW9UrfWWwFFB7ZYCIQC8UaG5ApZDUNf7wfmHS40hUHvUvYHDrohMkoHUtC56Dg:QUQ3MjNmeUVOU1pUbkVXbXhHUS1ENThyLTFtUkRmamNwN0ltT2FvcHNKR2s5R1NvekZnV3dXMHNIRWQ1bDVtNXU4RGRZM05RUmx6b2o1MFAySVo4Q0NrQS03Zkl4OWpZOExqY1ZIQ0VUd2JwN212NF9xRTJPdmlFTFdkR280blZVWEVJT0VwX3dIQXh0RG9rUll5QlZIYTZkQ2hWX2w5UFZ3",
        "__Secure-ROLLOUT_TOKEN": "CK3m5Kimx6iLVxDw4O_7oMuQAxj83Mn9oMuQAw%3D%3D",
        "PREF": "tz=Africa.Johannesburg&f5=30000",
        "__Secure-3PSIDCC": "AKEyXzVFmK21pfqNLYvrwUp-jlKjxCxXNHuQtvJK71iZlDnezNLxpmZHXS9Mz5lXb6RZOBvHzA"
    }


def process_cookies_data(cookies_data):
    """Process and validate cookies data with fallback to default cookies"""
    processed_cookies = None
    
    if cookies_data:
        try:
            if isinstance(cookies_data, str):
                # Try parsing as JSON first
                try:
                    processed_cookies = json.loads(cookies_data)
                    print("Parsed user-provided cookies as JSON")
                except json.JSONDecodeError:
                    # Try parsing as cookie string format
                    cookies_dict = {}
                    cookie_pairs = cookies_data.split(';')
                    
                    for pair in cookie_pairs:
                        parts = pair.split('=', 1)  # Split only on first =
                        if len(parts) == 2:
                            name, value = parts[0].strip(), parts[1].strip()
                            if name and value:
                                cookies_dict[name] = value
                    
                    if cookies_dict:
                        processed_cookies = cookies_dict
                        print(f"Parsed user-provided cookies from string format: {len(cookies_dict)} cookies")
                    
            elif isinstance(cookies_data, dict):
                processed_cookies = cookies_data
                print(f"Using user-provided cookies dict: {len(processed_cookies)} cookies")
                
        except Exception as cookie_error:
            print(f"Error processing user cookies: {cookie_error}")
    
    # If no valid user cookies, use defaults
    if not processed_cookies:
        processed_cookies = get_default_youtube_cookies()
        print(f"Using default fallback cookies: {len(processed_cookies)} cookies")
    
    return processed_cookies



def parse_subtitle_content(subtitle_content):
    """Parse VTT or SRT subtitle content to extract clean text with deduplication"""
    lines = subtitle_content.split('\n')
    text_lines = []
    
    for line in lines:
        line = line.strip()
        
        # Skip empty lines
        if not line:
            continue
            
        # Skip VTT header
        if line.startswith('WEBVTT') or line.startswith('NOTE'):
            continue
            
        # Skip SRT sequence numbers (pure digits)
        if line.isdigit():
            continue
            
        # Skip timestamp lines (contain --> or time patterns)
        if '-->' in line or re.match(r'^\d{2}:\d{2}:\d{2}', line):
            continue
            
        # Skip VTT style/position tags
        if line.startswith('<') or line.startswith('&'):
            continue
            
        # Clean HTML tags and entities
        line = re.sub(r'<[^>]+>', '', line)  # Remove HTML tags
        line = re.sub(r'&[a-zA-Z]+;', '', line)  # Remove HTML entities
        
        # Remove VTT positioning tags
        line = re.sub(r'\{[^}]*\}', '', line)
        
        # If line still has content, add it
        if line and not line.startswith('[') and not line.startswith('('):
            text_lines.append(line)
    
    # Join all text and clean up
    full_text = ' '.join(text_lines)
    
    # Remove extra whitespace
    full_text = re.sub(r'\s+', ' ', full_text)
    
    # Apply deduplication
    deduplicated_text = deduplicate_transcript_text(full_text)
    
    return deduplicated_text


def deduplicate_transcript_text(text):
    """Remove duplicate sentences and phrases from transcript text"""
    import difflib
    
    # Split into sentences using multiple delimiters
    sentences = re.split(r'[.!?]+\s+', text)
    
    # Clean up sentences
    clean_sentences = []
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) > 3:  # Only keep sentences with more than 3 characters
            clean_sentences.append(sentence)
    
    if not clean_sentences:
        return text
    
    # Remove exact duplicates first
    deduplicated = []
    seen_sentences = set()
    
    for sentence in clean_sentences:
        sentence_lower = sentence.lower().strip()
        if sentence_lower not in seen_sentences:
            seen_sentences.add(sentence_lower)
            deduplicated.append(sentence)
    
    # Remove near-duplicate sentences (with similarity threshold)
    final_sentences = []
    similarity_threshold = 0.2
    
    for i, sentence in enumerate(deduplicated):
        is_duplicate = False
        
        # Check against already added sentences
        for existing_sentence in final_sentences:
            similarity = difflib.SequenceMatcher(None, sentence.lower(), existing_sentence.lower()).ratio()
            
            if similarity > similarity_threshold:
                is_duplicate = True
                break
        
        if not is_duplicate:
            final_sentences.append(sentence)
    
    # Remove repeated phrases within the remaining text
    final_text = '. '.join(final_sentences)
    
    # Remove repeated phrases (3+ words that appear consecutively)
    final_text = remove_repeated_phrases(final_text)
    
    # Clean up final spacing and punctuation
    final_text = re.sub(r'\s+', ' ', final_text)
    final_text = re.sub(r'\s*\.\s*\.', '.', final_text)  # Remove double periods
    final_text = final_text.strip()
    
    # Add final period if missing
    if final_text and not final_text.endswith(('.', '!', '?')):
        final_text += '.'
    
    return final_text


def remove_repeated_phrases(text):
    """Remove repeated phrases of 3 or more words"""
    words = text.split()
    
    if len(words) < 6:  # Need at least 6 words to have a repeated 3-word phrase
        return text
    
    # Check for repeated phrases of different lengths
    for phrase_length in range(3, min(15, len(words) // 2)):  # Check phrases up to 15 words
        cleaned_words = remove_phrases_of_length(words, phrase_length)
        if len(cleaned_words) < len(words):
            words = cleaned_words
    
    return ' '.join(words)


def remove_phrases_of_length(words, phrase_length):
    """Remove repeated phrases of a specific length"""
    if len(words) < phrase_length * 2:
        return words
    
    cleaned_words = []
    i = 0
    
    while i < len(words):
        if i + phrase_length * 2 <= len(words):
            # Get current phrase and next phrase
            current_phrase = words[i:i + phrase_length]
            next_phrase = words[i + phrase_length:i + phrase_length * 2]
            
            # Check if they're the same (case-insensitive)
            if [word.lower() for word in current_phrase] == [word.lower() for word in next_phrase]:
                # Skip the duplicate phrase
                cleaned_words.extend(current_phrase)
                i += phrase_length * 2  # Skip both phrases, we kept one
                continue
        
        # No duplicate found, add current word
        cleaned_words.append(words[i])
        i += 1
    
    return cleaned_words


def analyze_transcript_quality(text):
    """Analyze transcript quality and provide statistics"""
    words = text.split()
    sentences = re.split(r'[.!?]+', text)
    
    # Calculate statistics
    total_words = len(words)
    total_sentences = len([s for s in sentences if s.strip()])
    
    # Find potential repeated phrases
    repeated_phrases = find_repeated_phrases(text)
    
    # Calculate readability score (simple version)
    avg_sentence_length = total_words / max(total_sentences, 1)
    
    quality_score = 100
    if repeated_phrases > 10:
        quality_score -= 20
    if avg_sentence_length > 40:  # Very long sentences might indicate parsing issues
        quality_score -= 10
    if avg_sentence_length < 5:   # Very short sentences might indicate fragmentation
        quality_score -= 10
    
    return {
        'total_words': total_words,
        'total_sentences': total_sentences,
        'average_sentence_length': round(avg_sentence_length, 1),
        'repeated_phrases_detected': repeated_phrases,
        'quality_score': max(quality_score, 0)
    }


def find_repeated_phrases(text, min_phrase_length=3):
    """Count repeated phrases in text"""
    words = text.lower().split()
    phrase_counts = {}
    repeated_count = 0
    
    # Check for repeated phrases
    for phrase_length in range(min_phrase_length, min(10, len(words) // 3)):
        for i in range(len(words) - phrase_length + 1):
            phrase = ' '.join(words[i:i + phrase_length])
            phrase_counts[phrase] = phrase_counts.get(phrase, 0) + 1
    
    # Count phrases that appear more than once
    for phrase, count in phrase_counts.items():
        if count > 1:
            repeated_count += count - 1  # Subtract 1 because original occurrence is not a repeat
    
    return repeated_count


from django.utils import timezone
from datetime import datetime

def safe_truncate_field(value, max_length):
    """Safely truncate a field to fit database constraints"""
    if not value:
        return value
    if len(str(value)) > max_length:
        return str(value)[:max_length-3] + "..."
    return str(value)

def create_transcription_method_name(source_type, language, strategy, use_cookies=True):
    """Create a concise transcription method name that fits database constraints"""
    # Create shorter method names to fit in 50 characters
    strategy_map = {
        'modern_with_auth': 'modern',
        'android_authenticated': 'android', 
        'ios_authenticated': 'ios',
        'tv_authenticated': 'tv',
        'fallback_no_cookies': 'fallback'
    }
    
    short_strategy = strategy_map.get(strategy, strategy[:10])
    cookie_suffix = '_auth' if use_cookies else '_noauth'
    
    # Format: ytdlp_auto_en_tv_auth (max ~20 chars)
    method_name = f"ytdlp_{source_type[:4]}_{language[:2]}_{short_strategy}{cookie_suffix}"
    
    # Ensure it fits in 50 characters
    return safe_truncate_field(method_name, 50)

@csrf_exempt
@require_http_methods(["POST"])
def snowai_extract_youtube_transcript_from_url(request):
    """Extract transcript from YouTube URL using yt-dlp with default cookies and enhanced fallback strategies"""
    try:
        data = json.loads(request.body)
        youtube_url = data.get('youtube_url', '').strip()
        cookies_data = data.get('cookies')  # This can be None and will use defaults
        
        if not youtube_url:
            return JsonResponse({'error': 'YouTube URL is required'}, status=400)
        
        video_id = extract_youtube_video_id_from_url(youtube_url)
        if not video_id:
            return JsonResponse({'error': 'Invalid YouTube URL format'}, status=400)
        
        # Check if transcript already exists
        existing_transcript = SnowAIVideoTranscriptRecord.objects.filter(
            youtube_video_id=video_id
        ).first()
        
        if existing_transcript:
            return JsonResponse({
                'message': 'Transcript already exists',
                'transcript_id': existing_transcript.transcript_uuid,
                'existing': True,
                'word_count': existing_transcript.word_count
            })
        
        print(f"Starting transcript extraction for video ID: {video_id}")
        if cookies_data:
            print("Using user-provided cookies for extraction")
        else:
            print("Using default cookies for extraction")
        
        # Extract transcript using yt-dlp with default cookies
        try:
            transcript_result = extract_transcript_with_ytdlp(youtube_url, video_id, cookies_data)
            
            if not transcript_result['text']:
                return JsonResponse({
                    'error': 'No transcript text could be extracted from this video. The video may not have captions or subtitles available.',
                    'debug_info': {
                        'video_id': video_id,
                        'extraction_method': transcript_result.get('method', 'unknown'),
                        'yt_dlp_available': YT_DLP_AVAILABLE,
                        'used_default_cookies': not bool(cookies_data)
                    }
                }, status=400)
            
            full_text = transcript_result['text']
            video_metadata = transcript_result['metadata']
            
            print(f"Transcript extracted: {len(full_text)} characters, method: {transcript_result['method']}")
            
            # Handle video upload date with proper timezone handling
            video_upload_date = None
            if video_metadata.get('upload_date'):
                try:
                    # Parse the upload date string (format: YYYYMMDD)
                    upload_date_str = video_metadata['upload_date']
                    if len(upload_date_str) == 8:  # YYYYMMDD format
                        parsed_date = datetime.strptime(upload_date_str, '%Y%m%d')
                        # Make it timezone-aware
                        video_upload_date = timezone.make_aware(parsed_date, timezone.get_current_timezone())
                except Exception as date_error:
                    print(f"Error parsing upload date: {date_error}")
                    video_upload_date = None
            
            # Safely truncate fields to match your database schema exactly
            video_title = safe_truncate_field(video_metadata.get('title', data.get('video_title', '')), 300)
            speaker_name = safe_truncate_field(data.get('speaker_name', ''), 200)
            country_code = safe_truncate_field(data.get('country_code', ''), 10)
            country_name = safe_truncate_field(data.get('country_name', ''), 100)
            category = safe_truncate_field(data.get('category', 'central_bank'), 100)
            transcription_method = safe_truncate_field(transcript_result.get('method', 'ytdlp_auto'), 50)
            transcript_language = safe_truncate_field(transcript_result.get('language', 'en'), 10)
            processing_status = safe_truncate_field('completed', 30)
            
            print(f"Creating transcript record...")
            print(f"- Title: {video_title[:50]}... (length: {len(video_title)})")
            print(f"- Method: {transcription_method} (length: {len(transcription_method)})")
            print(f"- Language: {transcript_language} (length: {len(transcript_language)})")
            print(f"- Category: {category} (length: {len(category)})")
            
            # Create transcript record with proper field length handling
            transcript_record = SnowAIVideoTranscriptRecord.objects.create(
                transcript_uuid=str(uuid.uuid4()),
                youtube_video_id=video_id,
                youtube_url=youtube_url,
                video_title=video_title,
                full_transcript_text=full_text,
                video_duration_seconds=video_metadata.get('duration'),
                video_upload_date=video_upload_date,
                primary_speaker_name=speaker_name,
                speaker_country_code=country_code,
                speaker_country_name=country_name,
                content_category=category,
                transcription_method=transcription_method,
                transcript_language=transcript_language,
                processing_status=processing_status
            )
            
            print(f"Transcript record created successfully with ID: {transcript_record.transcript_uuid}")
            
            return JsonResponse({
                'message': 'Transcript extracted and saved successfully',
                'transcript_id': transcript_record.transcript_uuid,
                'word_count': transcript_record.word_count,
                'duration': video_metadata.get('duration'),
                'title': video_title,
                'language': transcript_result['language'],
                'extraction_method': transcript_result['method'],
                'used_default_cookies': not bool(cookies_data),
                'text_preview': full_text[:200] + '...' if len(full_text) > 200 else full_text
            })
            
        except Exception as transcript_error:
            error_str = str(transcript_error).lower()
            print(f"Transcript extraction error: {error_str}")
            print(f"Full error details: {transcript_error}")
            
            # Print full traceback for debugging
            import traceback
            print(f"Full traceback: {traceback.format_exc()}")
            
            # Check if it's a database error
            if 'value too long' in error_str:
                return JsonResponse({
                    'error': 'Database field length error - some metadata fields are too long for the database schema.',
                    'debug_info': {
                        'video_id': video_id,
                        'error_type': 'database_constraint_error',
                        'full_error': str(transcript_error)[:200],
                        'suggestion': 'Check database field lengths for title, speaker name, etc.'
                    }
                }, status=400)
            
            # Provide more specific error messages
            if 'sign in to confirm' in error_str or 'not a bot' in error_str:
                return JsonResponse({
                    'error': 'YouTube bot detection bypassed with default cookies, but extraction still failed. The video might require updated authentication.',
                    'debug_video_id': video_id,
                    'suggestion': 'Try providing fresh browser cookies from a logged-in YouTube session'
                }, status=400)
            elif 'private' in error_str or 'unavailable' in error_str:
                return JsonResponse({
                    'error': 'The video is private, unavailable, or restricted.',
                    'debug_video_id': video_id
                }, status=400)
            elif 'no subtitles' in error_str or 'no transcript' in error_str or 'no valid transcript' in error_str:
                return JsonResponse({
                    'error': 'No subtitles or captions are available for this video, or the extracted content was too short.',
                    'debug_info': {
                        'video_id': video_id,
                        'url': youtube_url,
                        'method': 'yt-dlp_with_default_cookies'
                    }
                }, status=400)
            else:
                return JsonResponse({
                    'error': 'Failed to extract transcript from video despite using default cookies.',
                    'debug_info': {
                        'video_id': video_id,
                        'error_details': str(transcript_error)[:300],
                        'used_default_cookies': True,
                        'suggestion': 'The video may have special restrictions or yt-dlp may need updating'
                    }
                }, status=400)
                
    except json.JSONDecodeError:
        return JsonResponse({'error': 'Invalid JSON data'}, status=400)
    except Exception as e:
        print(f"Unexpected error: {str(e)}")
        import traceback
        print(f"Full traceback: {traceback.format_exc()}")
        return JsonResponse({
            'error': f'An unexpected error occurred: {str(e)}'[:200],
            'debug_info': 'Check server logs for more details'
        }, status=500)

def extract_transcript_with_ytdlp(video_url, video_id, cookies_data=None):
    """Extract transcript using yt-dlp with enhanced fallback strategies and default cookies"""
    
    if not YT_DLP_AVAILABLE:
        raise Exception("yt-dlp is not installed. Please install: pip install yt-dlp")
    
    transcript_data = {
        'text': None,
        'method': None,
        'language': 'en',
        'metadata': {}
    }
    
    # Process cookies with fallback to defaults
    cookies_dict = process_cookies_data(cookies_data)
    cookie_file_path = None
    
    try:
        cookie_file_path = create_cookie_jar_from_dict(cookies_dict)
        print(f"Created cookie jar with {len(cookies_dict)} cookies")
    except Exception as cookie_error:
        print(f"Cookie processing error: {cookie_error}")
        pass

    # Enhanced extraction strategies - focus on subtitle extraction only
    extraction_strategies = [
        {
            'name': 'tv_authenticated',
            'opts': {
                'quiet': True,
                'no_warnings': True,
                'writesubtitles': True,
                'writeautomaticsub': True,
                'subtitleslangs': ['en', 'en-US', 'en-GB'],
                'subtitlesformat': 'vtt',
                'skip_download': True,  # This is critical
                'extractor_args': {
                    'youtube': {
                        'player_client': ['tv_embedded'],
                    }
                },
                # Explicitly tell yt-dlp not to download video
                'format': 'best',
                'noplaylist': True,
            }
        },
        {
            'name': 'android_authenticated',
            'opts': {
                'quiet': True,
                'no_warnings': True,
                'writesubtitles': True,
                'writeautomaticsub': True,
                'subtitleslangs': ['en', 'en-US', 'en-GB'],
                'subtitlesformat': 'vtt',
                'skip_download': True,
                'extractor_args': {
                    'youtube': {
                        'player_client': ['android'],
                    }
                },
                'format': 'best',
                'noplaylist': True,
            }
        },
        {
            'name': 'web_authenticated',
            'opts': {
                'quiet': True,
                'no_warnings': True,
                'writesubtitles': True,
                'writeautomaticsub': True,
                'subtitleslangs': ['en', 'en-US', 'en-GB'],
                'subtitlesformat': 'vtt',
                'skip_download': True,
                'format': 'best',
                'noplaylist': True,
            }
        }
    ]

    extraction_error = None
    info = None
    successful_strategy = None

    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            print(f"Using temp directory: {temp_dir}")
            
            # First pass: Get video info without downloading
            for strategy in extraction_strategies:
                try:
                    print(f"Trying strategy: {strategy['name']} for video: {video_id}")
                    
                    opts = strategy['opts'].copy()
                    opts['outtmpl'] = os.path.join(temp_dir, '%(id)s.%(ext)s')
                    
                    if cookie_file_path:
                        opts['cookiefile'] = cookie_file_path
                        print(f"Using cookies with strategy: {strategy['name']}")
                    
                    # CRITICAL FIX: Use download=False for info extraction
                    with yt_dlp.YoutubeDL(opts) as ydl:
                        info = ydl.extract_info(video_url, download=False)
                    
                    print(f"Strategy {strategy['name']} successful!")
                    successful_strategy = strategy['name']
                    break
                    
                except Exception as strategy_error:
                    extraction_error = strategy_error
                    print(f"Strategy {strategy['name']} failed: {strategy_error}")
                    continue
            
            if not info:
                raise Exception(f"All extraction strategies failed. Last error: {extraction_error}")
        
            # Process metadata
            print("Processing video metadata...")
            transcript_data['metadata'] = {
                'title': info.get('title', ''),
                'duration': info.get('duration'),
                'upload_date': info.get('upload_date'),
                'uploader': info.get('uploader', ''),
                'view_count': info.get('view_count'),
                'description': info.get('description', '')[:500] + '...' if info.get('description') and len(info.get('description')) > 500 else info.get('description', '')
            }
            
            print("Checking available subtitles...")
            subtitles = info.get('subtitles', {})
            automatic_captions = info.get('automatic_captions', {})
            
            print(f"Manual subtitles: {list(subtitles.keys())[:10]}")
            print(f"Automatic captions: {list(automatic_captions.keys())[:10]}")
            
            # Extract subtitle text
            subtitle_text = None
            selected_language = None
            extraction_method = None
            
            # Try manual subtitles first, then automatic
            subtitle_sources = [
                ('manual', subtitles),
                ('automatic', automatic_captions)
            ]
            
            for source_type, subtitle_dict in subtitle_sources:
                if subtitle_text:
                    break
                    
                for lang in ['en', 'en-US', 'en-GB', 'en-orig']:
                    if lang in subtitle_dict:
                        print(f"Found {source_type} subtitles for language: {lang}")
                        
                        try:
                            # Download subtitles separately
                            download_opts = {
                                'quiet': True,
                                'no_warnings': True,
                                'writesubtitles': source_type == 'manual',
                                'writeautomaticsub': source_type == 'automatic',
                                'subtitleslangs': [lang],
                                'subtitlesformat': 'vtt',
                                'skip_download': True,
                                'outtmpl': os.path.join(temp_dir, f'{video_id}.%(ext)s'),
                                'noplaylist': True,
                            }
                            
                            if cookie_file_path:
                                download_opts['cookiefile'] = cookie_file_path
                            
                            print(f"Downloading {source_type} subtitle for {lang}...")
                            with yt_dlp.YoutubeDL(download_opts) as sub_ydl:
                                # Download with download=True only for subtitles
                                sub_ydl.download([video_url])
                            
                            # Find subtitle file
                            all_files = os.listdir(temp_dir)
                            print(f"Files in temp dir: {all_files}")
                            
                            subtitle_files = [f for f in all_files if f.endswith(('.vtt', '.srt'))]
                            print(f"Subtitle files found: {subtitle_files}")
                            
                            if subtitle_files:
                                subtitle_file = os.path.join(temp_dir, subtitle_files[0])
                                file_size = os.path.getsize(subtitle_file)
                                print(f"Processing subtitle file: {subtitle_files[0]} ({file_size} bytes)")
                                
                                if file_size > 0:
                                    with open(subtitle_file, 'r', encoding='utf-8', errors='ignore') as f:
                                        raw_content = f.read()
                                    
                                    print(f"Raw content length: {len(raw_content)} chars")
                                    
                                    if len(raw_content.strip()) > 0:
                                        print(f"Content preview: {raw_content[:200]}...")
                                        
                                        # Parse the subtitle content
                                        parsed_text = parse_subtitle_content(raw_content)
                                        print(f"Parsed text length: {len(parsed_text) if parsed_text else 0} chars")
                                        
                                        if parsed_text and len(parsed_text.strip()) > 50:
                                            subtitle_text = parsed_text
                                            selected_language = lang
                                            # Create shorter method name that fits in 50 chars
                                            extraction_method = create_transcription_method_name(
                                                source_type[:4], lang, successful_strategy, bool(cookie_file_path)
                                            )
                                            print(f"SUCCESS: Subtitle extraction completed! Method: {extraction_method}")
                                            break
                                        else:
                                            print(f"Parsed text too short: {len(parsed_text) if parsed_text else 0}")
                                    else:
                                        print("Raw content is empty")
                                else:
                                    print("Subtitle file is empty (0 bytes)")
                            else:
                                print("No subtitle files found after download")
                                
                        except Exception as lang_error:
                            print(f"Error processing {lang}: {lang_error}")
                            import traceback
                            print(f"Lang error traceback: {traceback.format_exc()}")
                            continue
                    
                    if subtitle_text:
                        break
                        
            if subtitle_text and len(subtitle_text.strip()) > 50:
                transcript_data['text'] = subtitle_text.strip()
                transcript_data['language'] = selected_language
                transcript_data['method'] = extraction_method
                print(f"Final transcript: {len(subtitle_text)} characters")
            else:
                raise Exception(f"No valid transcript text extracted. Content length: {len(subtitle_text) if subtitle_text else 0}")
                
    finally:
        # Clean up cookie file
        if cookie_file_path and os.path.exists(cookie_file_path):
            try:
                os.unlink(cookie_file_path)
            except Exception:
                pass
    
    return transcript_data

    
def extract_transcript_with_ytdlp_fixed(video_url, video_id, cookies_data=None):
    """Extract transcript with better exception handling"""
    
    if not YT_DLP_AVAILABLE:
        raise Exception("yt-dlp is not installed. Please install: pip install yt-dlp")
    
    transcript_data = {
        'text': None,
        'method': None,
        'language': 'en',
        'metadata': {}
    }
    
    # Process cookies with fallback to defaults
    cookies_dict = process_cookies_data(cookies_data)
    cookie_file_path = None
    
    try:
        cookie_file_path = create_cookie_jar_from_dict(cookies_dict)
        print(f"Created cookie jar with {len(cookies_dict)} cookies")
    except Exception as cookie_error:
        print(f"Cookie processing error: {cookie_error}")
        pass

    # Use same strategies as before...
    extraction_strategies = [
        {
            'name': 'tv_authenticated',
            'opts': {
                'quiet': True,
                'no_warnings': True,
                'writesubtitles': True,
                'writeautomaticsub': True,
                'subtitleslangs': ['en'],
                'subtitlesformat': 'vtt',
                'skip_download': True,
                'extractor_args': {
                    'youtube': {
                        'player_client': ['tv_embedded'],
                    }
                }
            }
        }
    ]

    extraction_error = None
    info = None
    successful_strategy = None

    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            print(f"Using temp directory: {temp_dir}")
            
            for strategy in extraction_strategies:
                try:
                    print(f"Trying strategy: {strategy['name']} for video: {video_id}")
                    
                    opts = strategy['opts'].copy()
                    opts['outtmpl'] = os.path.join(temp_dir, '%(id)s.%(ext)s')
                    
                    if cookie_file_path:
                        opts['cookiefile'] = cookie_file_path
                        print(f"Using cookies with strategy: {strategy['name']}")
                    
                    with yt_dlp.YoutubeDL(opts) as ydl:
                        info = ydl.extract_info(video_url, download=True)
                    
                    print(f"Strategy {strategy['name']} successful!")
                    successful_strategy = strategy['name']
                    break
                    
                except Exception as strategy_error:
                    extraction_error = strategy_error
                    print(f"Strategy {strategy['name']} failed: {strategy_error}")
                    continue
            
            if not info:
                raise Exception("All extraction strategies failed")
        
            # Process metadata
            print("Processing video metadata...")
            transcript_data['metadata'] = {
                'title': info.get('title', ''),
                'duration': info.get('duration'),
                'upload_date': info.get('upload_date'),
                'uploader': info.get('uploader', ''),
                'view_count': info.get('view_count'),
                'description': info.get('description', '')[:500] + '...' if info.get('description') and len(info.get('description')) > 500 else info.get('description', '')
            }
            
            print("Checking available subtitles...")
            subtitles = info.get('subtitles', {})
            automatic_captions = info.get('automatic_captions', {})
            
            print(f"Available automatic captions: {list(automatic_captions.keys())[:10]}...")
            
            # Extract subtitle text with better error handling
            subtitle_text = None
            selected_language = None
            extraction_method = None
            
            try:
                # Try English captions
                for lang in ['en', 'en-US', 'en-GB', 'en-orig']:
                    if lang in automatic_captions:
                        print(f"Found automatic subtitles for language: {lang}")
                        
                        try:
                            # Setup subtitle download
                            download_opts = {
                                'quiet': True,
                                'no_warnings': True,
                                'writesubtitles': False,
                                'writeautomaticsub': True,
                                'subtitleslangs': [lang],
                                'subtitlesformat': 'vtt',
                                'skip_download': True,
                                'outtmpl': os.path.join(temp_dir, f'{video_id}.%(ext)s')
                            }
                            
                            if cookie_file_path:
                                download_opts['cookiefile'] = cookie_file_path
                            
                            print(f"Starting subtitle download for {lang}...")
                            with yt_dlp.YoutubeDL(download_opts) as sub_ydl:
                                sub_ydl.download([video_url])
                            print(f"Subtitle download completed for {lang}")
                            
                            # List all files and find subtitle
                            all_files = os.listdir(temp_dir)
                            print(f"Files after download: {all_files}")
                            
                            subtitle_files = [f for f in all_files if f.endswith(('.vtt', '.srt'))]
                            print(f"Subtitle files found: {subtitle_files}")
                            
                            if subtitle_files:
                                subtitle_file = os.path.join(temp_dir, subtitle_files[0])
                                file_size = os.path.getsize(subtitle_file)
                                print(f"Processing subtitle file: {subtitle_files[0]} ({file_size} bytes)")
                                
                                if file_size > 0:
                                    with open(subtitle_file, 'r', encoding='utf-8', errors='ignore') as f:
                                        raw_content = f.read()
                                    
                                    print(f"Raw content length: {len(raw_content)} chars")
                                    
                                    if len(raw_content.strip()) > 0:
                                        # Show first bit of content
                                        print(f"Content preview: {raw_content[:200]}...")
                                        
                                        # Parse the subtitle content
                                        parsed_text = parse_subtitle_content(raw_content)
                                        print(f"Parsed text length: {len(parsed_text) if parsed_text else 0} chars")
                                        
                                        if parsed_text and len(parsed_text.strip()) > 50:
                                            subtitle_text = parsed_text
                                            selected_language = lang
                                            extraction_method = f'ytdlp_automatic_{lang}_{successful_strategy}_cookies'
                                            print("SUCCESS: Subtitle extraction completed!")
                                            break
                                        else:
                                            print(f"Parsed text too short: {len(parsed_text) if parsed_text else 0}")
                                    else:
                                        print("Raw content is empty")
                                else:
                                    print("Subtitle file is empty (0 bytes)")
                            else:
                                print("No subtitle files found after download")
                                
                        except Exception as lang_error:
                            print(f"Error processing {lang}: {lang_error}")
                            import traceback
                            print(f"Lang error traceback: {traceback.format_exc()}")
                            continue
                            
            except Exception as subtitle_error:
                print(f"Subtitle extraction error: {subtitle_error}")
                import traceback
                print(f"Subtitle error traceback: {traceback.format_exc()}")
                
            if subtitle_text and len(subtitle_text.strip()) > 50:
                transcript_data['text'] = subtitle_text.strip()
                transcript_data['language'] = selected_language
                transcript_data['method'] = extraction_method
                print(f"Final transcript: {len(subtitle_text)} characters")
            else:
                raise Exception(f"No valid transcript text extracted. Content length: {len(subtitle_text) if subtitle_text else 0}")
                
    finally:
        # Clean up cookie file
        if cookie_file_path and os.path.exists(cookie_file_path):
            try:
                os.unlink(cookie_file_path)
            except Exception:
                pass
    
    return transcript_data

@csrf_exempt
@require_http_methods(["GET"])
def snowai_check_ytdlp_version_and_update(request):
    """Check yt-dlp version and provide update commands"""
    try:
        debug_info = {
            'yt_dlp_available': YT_DLP_AVAILABLE,
        }
        
        if YT_DLP_AVAILABLE:
            debug_info['yt_dlp_version'] = yt_dlp.version.__version__
            
            # Check if update is available (simplified check)
            try:
                result = subprocess.run(['pip', 'show', 'yt-dlp'], capture_output=True, text=True)
                if result.returncode == 0:
                    debug_info['pip_info'] = result.stdout
                    
                # Try to get latest version info
                result = subprocess.run(['yt-dlp', '--version'], capture_output=True, text=True)
                if result.returncode == 0:
                    debug_info['command_version'] = result.stdout.strip()
                    
            except Exception as version_check_error:
                debug_info['version_check_error'] = str(version_check_error)
        
        debug_info['update_commands'] = [
            'pip install --upgrade yt-dlp',
            'yt-dlp -U'
        ]
        
        return JsonResponse(debug_info)
        
    except Exception as e:
        return JsonResponse({
            'error': str(e),
            'yt_dlp_available': YT_DLP_AVAILABLE
        }, status=500)

@csrf_exempt
@require_http_methods(["POST"])
def snowai_test_video_extraction(request):
    """Test video extraction without saving to database"""
    try:
        data = json.loads(request.body)
        youtube_url = data.get('youtube_url', '').strip()
        cookies_data = data.get('cookies')
        
        if not youtube_url:
            return JsonResponse({'error': 'YouTube URL is required'}, status=400)
        
        video_id = extract_youtube_video_id_from_url(youtube_url)
        if not video_id:
            return JsonResponse({'error': 'Invalid YouTube URL format'}, status=400)
        
        print(f"Testing extraction for video ID: {video_id}")
        
        # Test basic info extraction only
        try:
            transcript_result = extract_transcript_with_ytdlp(youtube_url, video_id, cookies_data)
            
            return JsonResponse({
                'success': True,
                'video_id': video_id,
                'title': transcript_result['metadata'].get('title', ''),
                'duration': transcript_result['metadata'].get('duration'),
                'transcript_length': len(transcript_result['text']) if transcript_result['text'] else 0,
                'extraction_method': transcript_result['method'],
                'language': transcript_result['language'],
                'text_preview': transcript_result['text'][:300] + '...' if transcript_result['text'] and len(transcript_result['text']) > 300 else transcript_result['text']
            })
            
        except Exception as test_error:
            return JsonResponse({
                'success': False,
                'error': str(test_error),
                'video_id': video_id,
                'yt_dlp_version': yt_dlp.version.__version__ if YT_DLP_AVAILABLE else 'not available'
            })
            
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def snowai_check_transcript_availability(request):
    """Check if transcript/subtitles are available for a YouTube video using yt-dlp"""
    try:
        data = json.loads(request.body)
        youtube_url = data.get('youtube_url', '').strip()
        
        if not youtube_url:
            return JsonResponse({'error': 'YouTube URL is required'}, status=400)
        
        video_id = extract_youtube_video_id_from_url(youtube_url)
        if not video_id:
            return JsonResponse({'error': 'Invalid YouTube URL format'}, status=400)
        
        if not YT_DLP_AVAILABLE:
            return JsonResponse({'error': 'yt-dlp not available'}, status=500)
        
        try:
            ydl_opts = {
                'quiet': True,
                'no_warnings': True,
                'skip_download': True,
            }
            
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(youtube_url, download=False)
                
                subtitles = info.get('subtitles', {})
                automatic_captions = info.get('automatic_captions', {})
                
                available_subtitles = []
                
                # Process manual subtitles
                for lang, formats in subtitles.items():
                    available_subtitles.append({
                        'language': lang,
                        'type': 'manual',
                        'formats_count': len(formats) if isinstance(formats, list) else 1
                    })
                
                # Process automatic captions
                for lang, formats in automatic_captions.items():
                    available_subtitles.append({
                        'language': lang,
                        'type': 'automatic',
                        'formats_count': len(formats) if isinstance(formats, list) else 1
                    })
                
                return JsonResponse({
                    'video_id': video_id,
                    'video_title': info.get('title', ''),
                    'has_transcripts': len(available_subtitles) > 0,
                    'available_subtitles': available_subtitles,
                    'total_count': len(available_subtitles),
                    'duration': info.get('duration'),
                    'uploader': info.get('uploader', '')
                })
                
        except Exception as e:
            return JsonResponse({
                'video_id': video_id,
                'has_transcripts': False,
                'error': str(e)
            }, status=400)
            
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


# Keep all your existing endpoints (they don't need to change)
@csrf_exempt
@require_http_methods(["GET"])
def snowai_get_all_saved_transcripts(request):
    """Retrieve all saved transcripts with pagination and filtering"""
    try:
        page = int(request.GET.get('page', 1))
        per_page = int(request.GET.get('per_page', 10))
        search_query = request.GET.get('search', '').strip()
        category_filter = request.GET.get('category', '').strip()
        country_filter = request.GET.get('country', '').strip()
        
        # Build query
        queryset = SnowAIVideoTranscriptRecord.objects.all()
        
        if search_query:
            queryset = queryset.filter(
                Q(full_transcript_text__icontains=search_query) |
                Q(video_title__icontains=search_query) |
                Q(primary_speaker_name__icontains=search_query)
            )
        
        if category_filter:
            queryset = queryset.filter(content_category=category_filter)
            
        if country_filter:
            queryset = queryset.filter(
                Q(speaker_country_code=country_filter) |
                Q(speaker_country_name__icontains=country_filter)
            )
        
        # Save search history
        if search_query or category_filter or country_filter:
            SnowAITranscriptSearchHistory.objects.create(
                search_query=search_query,
                search_filters={
                    'category': category_filter,
                    'country': country_filter
                },
                results_count=queryset.count()
            )
        
        # Paginate results
        paginator = Paginator(queryset, per_page)
        page_obj = paginator.get_page(page)
        
        transcripts_data = []
        for transcript in page_obj:
            transcripts_data.append({
                'id': transcript.transcript_uuid,
                'youtube_url': transcript.youtube_url,
                'video_title': transcript.video_title,
                'speaker_name': transcript.primary_speaker_name,
                'country': transcript.speaker_country_name,
                'category': transcript.content_category,
                'word_count': transcript.word_count,
                'duration_seconds': transcript.video_duration_seconds,
                'created_at': transcript.created_at.isoformat(),
                'video_upload_date': transcript.video_upload_date.isoformat() if transcript.video_upload_date else None,
                'transcript_preview': transcript.full_transcript_text[:200] + '...' if len(transcript.full_transcript_text) > 200 else transcript.full_transcript_text
            })
        
        return JsonResponse({
            'transcripts': transcripts_data,
            'pagination': {
                'current_page': page_obj.number,
                'total_pages': paginator.num_pages,
                'total_items': paginator.count,
                'has_next': page_obj.has_next(),
                'has_previous': page_obj.has_previous()
            }
        })
        
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def snowai_get_single_transcript_details(request, transcript_id):
    """Get detailed view of a single transcript"""
    try:
        transcript = SnowAIVideoTranscriptRecord.objects.get(transcript_uuid=transcript_id)
        
        return JsonResponse({
            'id': transcript.transcript_uuid,
            'youtube_url': transcript.youtube_url,
            'youtube_video_id': transcript.youtube_video_id,
            'video_title': transcript.video_title,
            'speaker_name': transcript.primary_speaker_name,
            'speaker_organization': transcript.speaker_organization,
            'country_code': transcript.speaker_country_code,
            'country_name': transcript.speaker_country_name,
            'full_transcript': transcript.full_transcript_text,
            'category': transcript.content_category,
            'word_count': transcript.word_count,
            'duration_seconds': transcript.video_duration_seconds,
            'language': transcript.transcript_language,
            'transcription_method': transcript.transcription_method,
            'created_at': transcript.created_at.isoformat(),
            'video_upload_date': transcript.video_upload_date.isoformat() if transcript.video_upload_date else None,
            'custom_tags': transcript.custom_tags,
            'economic_topics': transcript.economic_topics
        })
        
    except SnowAIVideoTranscriptRecord.DoesNotExist:
        return JsonResponse({'error': 'Transcript not found'}, status=404)
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["DELETE"])
def snowai_delete_transcript_record(request, transcript_id):
    """Delete a specific transcript record"""
    try:
        transcript = SnowAIVideoTranscriptRecord.objects.get(transcript_uuid=transcript_id)
        video_title = transcript.video_title
        transcript.delete()
        
        return JsonResponse({
            'message': f'Transcript "{video_title}" deleted successfully'
        })
        
    except SnowAIVideoTranscriptRecord.DoesNotExist:
        return JsonResponse({'error': 'Transcript not found'}, status=404)
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


# Debug endpoint for yt-dlp
@csrf_exempt
@require_http_methods(["GET"])
def snowai_debug_ytdlp_availability(request):
    """Debug endpoint to test yt-dlp availability and version"""
    try:
        debug_info = {
            'yt_dlp_available': YT_DLP_AVAILABLE,
        }
        
        if YT_DLP_AVAILABLE:
            debug_info['yt_dlp_version'] = yt_dlp.version.__version__
            
            # Test with a known good video (Rick Roll - has captions)
            test_video_url = 'https://www.youtube.com/watch?v=dQw4w9WgXcQ'
            
            try:
                ydl_opts = {
                    'quiet': True,
                    'no_warnings': True,
                    'skip_download': True,
                }
                
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    info = ydl.extract_info(test_video_url, download=False)
                    
                    debug_info['test_video_info'] = {
                        'title': info.get('title', ''),
                        'duration': info.get('duration'),
                        'has_subtitles': bool(info.get('subtitles')),
                        'has_automatic_captions': bool(info.get('automatic_captions')),
                        'subtitle_languages': list(info.get('subtitles', {}).keys()),
                        'automatic_caption_languages': list(info.get('automatic_captions', {}).keys())
                    }
                    debug_info['test_success'] = True
                    
            except Exception as e:
                debug_info['test_success'] = False
                debug_info['test_error'] = str(e)
        
        return JsonResponse(debug_info)
        
    except Exception as e:
        return JsonResponse({
            'error': str(e),
            'yt_dlp_available': YT_DLP_AVAILABLE
        }, status=500)
        
@csrf_exempt
@require_http_methods(["PUT"])
def snowai_update_transcript_metadata(request, transcript_id):
    """Update transcript metadata like speaker info, tags, etc."""
    try:
        transcript = SnowAIVideoTranscriptRecord.objects.get(transcript_uuid=transcript_id)
        data = json.loads(request.body)
        
        # Update allowed fields
        updateable_fields = [
            'primary_speaker_name', 'speaker_organization', 'speaker_country_code',
            'speaker_country_name', 'content_category', 'custom_tags', 'economic_topics'
        ]
        
        for field in updateable_fields:
            if field in data:
                setattr(transcript, field, data[field])
        
        transcript.save()
        
        return JsonResponse({
            'message': 'Transcript metadata updated successfully'
        })
        
    except SnowAIVideoTranscriptRecord.DoesNotExist:
        return JsonResponse({'error': 'Transcript not found'}, status=404)
    except json.JSONDecodeError:
        return JsonResponse({'error': 'Invalid JSON data'}, status=400)
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


# Additional utility functions you might find useful

@csrf_exempt  
@require_http_methods(["GET"])
def snowai_get_transcript_statistics(request):
    """Get overall statistics about the transcript database"""
    try:
        total_transcripts = SnowAIVideoTranscriptRecord.objects.count()
        total_words = sum(SnowAIVideoTranscriptRecord.objects.values_list('word_count', flat=True))
        
        # Category breakdown
        categories = SnowAIVideoTranscriptRecord.objects.values('content_category').distinct()
        category_stats = {}
        for cat in categories:
            if cat['content_category']:
                count = SnowAIVideoTranscriptRecord.objects.filter(content_category=cat['content_category']).count()
                category_stats[cat['content_category']] = count
        
        # Country breakdown
        countries = SnowAIVideoTranscriptRecord.objects.values('speaker_country_name').distinct()
        country_stats = {}
        for country in countries:
            if country['speaker_country_name']:
                count = SnowAIVideoTranscriptRecord.objects.filter(speaker_country_name=country['speaker_country_name']).count()
                country_stats[country['speaker_country_name']] = count
        
        return JsonResponse({
            'total_transcripts': total_transcripts,
            'total_words': total_words,
            'average_words_per_transcript': total_words / total_transcripts if total_transcripts > 0 else 0,
            'category_breakdown': category_stats,
            'country_breakdown': country_stats
        })
        
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["GET"]) 
def snowai_export_transcripts_csv(request):
    """Export transcripts to CSV format"""
    import csv
    from django.http import HttpResponse
    
    try:
        response = HttpResponse(content_type='text/csv')
        response['Content-Disposition'] = 'attachment; filename="snowai_transcripts_export.csv"'
        
        writer = csv.writer(response)
        writer.writerow([
            'ID', 'Video Title', 'Speaker Name', 'Country', 'Category', 
            'Word Count', 'Duration (seconds)', 'YouTube URL', 'Created At', 'Transcript Text'
        ])
        
        transcripts = SnowAIVideoTranscriptRecord.objects.all()
        for transcript in transcripts:
            writer.writerow([
                transcript.transcript_uuid,
                transcript.video_title or '',
                transcript.primary_speaker_name or '',
                transcript.speaker_country_name or '',
                transcript.content_category or '',
                transcript.word_count,
                transcript.video_duration_seconds or 0,
                transcript.youtube_url or '',
                transcript.created_at.isoformat(),
                transcript.full_transcript_text[:1000] + '...' if len(transcript.full_transcript_text) > 1000 else transcript.full_transcript_text
            ])
        
        return response
        
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)



# views.py (add these to your existing views)

import json
import time
import uuid
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.shortcuts import get_object_or_404
from django.core.paginator import Paginator
from django.db.models import Q

@csrf_exempt
@require_http_methods(["GET"])
def snowai_transcript_analysis_dashboard_data_endpoint(request):
    """Get dashboard data for transcript analysis overview"""
    try:
        # Get basic stats
        total_transcripts = SnowAIVideoTranscriptRecord.objects.count()
        analyzed_transcripts = SnowAIVideoTranscriptRecord.objects.filter(ai_analysis__isnull=False).count()
        pending_analysis = total_transcripts - analyzed_transcripts
        
        # Get recent transcripts with analysis status
        transcripts = SnowAIVideoTranscriptRecord.objects.select_related('ai_analysis').order_by('-created_at')[:50]
        
        transcript_data = []
        for transcript in transcripts:
            has_analysis = hasattr(transcript, 'ai_analysis') and transcript.ai_analysis is not None
            transcript_data.append({
                'transcript_uuid': transcript.transcript_uuid,
                'video_title': transcript.video_title or 'Untitled',
                'primary_speaker_name': transcript.primary_speaker_name or 'Unknown',
                'speaker_organization': transcript.speaker_organization or 'N/A',
                'created_at': transcript.created_at.isoformat() if transcript.created_at else None,
                'word_count': transcript.word_count,
                'has_analysis': has_analysis,
                'analysis_sentiment': transcript.ai_analysis.overall_sentiment if has_analysis else None,
                'analysis_created_at': transcript.ai_analysis.analysis_created_at.isoformat() if has_analysis else None,
                'key_insights_count': transcript.ai_analysis.key_insights_count if has_analysis else 0
            })
        
        return JsonResponse({
            'success': True,
            'data': {
                'stats': {
                    'total_transcripts': total_transcripts,
                    'analyzed_transcripts': analyzed_transcripts,
                    'pending_analysis': pending_analysis,
                    'analysis_completion_rate': (analyzed_transcripts / total_transcripts * 100) if total_transcripts > 0 else 0
                },
                'transcripts': transcript_data
            }
        })
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)

@csrf_exempt
@require_http_methods(["POST"])
def snowai_trigger_transcript_analysis_endpoint(request):
    """Trigger AI analysis for a specific transcript"""
    try:
        data = json.loads(request.body)
        transcript_uuid = data.get('transcript_uuid')
        
        if not transcript_uuid:
            return JsonResponse({'success': False, 'error': 'transcript_uuid is required'}, status=400)
        
        transcript = get_object_or_404(SnowAIVideoTranscriptRecord, transcript_uuid=transcript_uuid)
        
        # Check if analysis already exists
        if hasattr(transcript, 'ai_analysis') and transcript.ai_analysis is not None:
            return JsonResponse({
                'success': False, 
                'error': 'Analysis already exists for this transcript. Delete existing analysis first to re-analyze.',
                'existing_analysis_uuid': transcript.ai_analysis.analysis_uuid
            }, status=400)
        
        # Start timing the analysis
        start_time = time.time()
        
        # Create the AI analysis prompt
        analysis_prompt = f"""
        Analyze the following speech transcript from {transcript.primary_speaker_name or 'Unknown Speaker'} 
        from {transcript.speaker_organization or 'Unknown Organization'}. 
        
        Video Title: {transcript.video_title or 'Untitled'}
        Speaker: {transcript.primary_speaker_name or 'Unknown'}
        Organization: {transcript.speaker_organization or 'N/A'}
        
        Transcript Text:
        {transcript.full_transcript_text}
        
        Please provide a comprehensive economic and financial analysis in JSON format with the following structure:
        {{
            "executive_summary": "A comprehensive 3-4 paragraph summary of the key points",
            "key_themes": ["theme1", "theme2", "theme3"],
            "economic_opportunities": [
                {{"opportunity": "Description of opportunity", "confidence": 0.8, "timeframe": "short_term/medium_term/long_term"}}
            ],
            "economic_risks": [
                {{"risk": "Description of risk", "impact_level": "low/medium/high", "probability": 0.7}}
            ],
            "policy_implications": [
                {{"implication": "Policy implication description", "timeframe": "short_term/medium_term/long_term", "sector": "monetary/fiscal/regulatory"}}
            ],
            "overall_sentiment": "positive/negative/neutral/mixed",
            "sentiment_confidence": 0.85,
            "market_outlook": "bullish/bearish/neutral/uncertain",
            "inflation_mentions": {{"current": "value if mentioned", "target": "value if mentioned", "forecast": "value if mentioned"}},
            "interest_rate_mentions": {{"current": "value if mentioned", "next_meeting": "decision if mentioned", "forecast": "value if mentioned"}},
            "gdp_mentions": {{"current": "value if mentioned", "forecast": "value if mentioned"}},
            "unemployment_mentions": {{"current": "value if mentioned", "forecast": "value if mentioned"}},
            "policy_actions_suggested": ["action1", "action2"],
            "market_predictions": [
                {{"prediction": "Prediction description", "timeframe": "1_month/3_months/6_months/1_year", "confidence": 0.6}}
            ],
            "analysis_completeness_score": 0.9
        }}
        
        Ensure all confidence scores are between 0 and 1. If specific economic metrics aren't mentioned, leave those objects empty. Focus on extracting actionable insights and concrete predictions where possible.
        """
        
        # Get AI analysis
        ai_response = chat_gpt(analysis_prompt)
        
        # Try to parse the JSON response
        try:
            analysis_data = json.loads(ai_response)
        except json.JSONDecodeError:
            # If direct parsing fails, try to extract JSON from the response
            import re
            json_match = re.search(r'\{.*\}', ai_response, re.DOTALL)
            if json_match:
                analysis_data = json.loads(json_match.group())
            else:
                return JsonResponse({'success': False, 'error': 'Failed to parse AI response as JSON'}, status=500)
        
        # Calculate analysis duration
        analysis_duration = time.time() - start_time
        
        # Create the analysis record
        analysis = SnowAITranscriptAnalysis.objects.create(
            transcript=transcript,
            analysis_uuid=str(uuid.uuid4()),
            executive_summary=analysis_data.get('executive_summary', ''),
            key_themes=analysis_data.get('key_themes', []),
            economic_opportunities=analysis_data.get('economic_opportunities', []),
            economic_risks=analysis_data.get('economic_risks', []),
            policy_implications=analysis_data.get('policy_implications', []),
            overall_sentiment=analysis_data.get('overall_sentiment', 'neutral'),
            sentiment_confidence=analysis_data.get('sentiment_confidence', 0.0),
            market_outlook=analysis_data.get('market_outlook', 'neutral'),
            inflation_mentions=analysis_data.get('inflation_mentions', {}),
            interest_rate_mentions=analysis_data.get('interest_rate_mentions', {}),
            gdp_mentions=analysis_data.get('gdp_mentions', {}),
            unemployment_mentions=analysis_data.get('unemployment_mentions', {}),
            policy_actions_suggested=analysis_data.get('policy_actions_suggested', []),
            market_predictions=analysis_data.get('market_predictions', []),
            analysis_duration_seconds=analysis_duration,
            analysis_completeness_score=analysis_data.get('analysis_completeness_score', 0.0)
        )
        
        return JsonResponse({
            'success': True,
            'data': {
                'analysis_uuid': analysis.analysis_uuid,
                'executive_summary': analysis.executive_summary,
                'overall_sentiment': analysis.overall_sentiment,
                'key_insights_count': analysis.key_insights_count,
                'analysis_duration': analysis_duration
            }
        })
        
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)

@csrf_exempt
@require_http_methods(["GET"])
def snowai_get_transcript_analysis_details_endpoint(request, transcript_uuid):
    """Get detailed analysis for a specific transcript"""
    try:
        transcript = get_object_or_404(SnowAIVideoTranscriptRecord, transcript_uuid=transcript_uuid)
        
        if not hasattr(transcript, 'ai_analysis') or transcript.ai_analysis is None:
            return JsonResponse({'success': False, 'error': 'No analysis found for this transcript'}, status=404)
        
        analysis = transcript.ai_analysis
        
        return JsonResponse({
            'success': True,
            'data': {
                'transcript_info': {
                    'transcript_uuid': transcript.transcript_uuid,
                    'video_title': transcript.video_title,
                    'primary_speaker_name': transcript.primary_speaker_name,
                    'speaker_organization': transcript.speaker_organization,
                    'word_count': transcript.word_count,
                    'created_at': transcript.created_at.isoformat() if transcript.created_at else None
                },
                'analysis': {
                    'analysis_uuid': analysis.analysis_uuid,
                    'executive_summary': analysis.executive_summary,
                    'key_themes': analysis.key_themes,
                    'economic_opportunities': analysis.economic_opportunities,
                    'economic_risks': analysis.economic_risks,
                    'policy_implications': analysis.policy_implications,
                    'overall_sentiment': analysis.overall_sentiment,
                    'sentiment_confidence': analysis.sentiment_confidence,
                    'market_outlook': analysis.market_outlook,
                    'inflation_mentions': analysis.inflation_mentions,
                    'interest_rate_mentions': analysis.interest_rate_mentions,
                    'gdp_mentions': analysis.gdp_mentions,
                    'unemployment_mentions': analysis.unemployment_mentions,
                    'policy_actions_suggested': analysis.policy_actions_suggested,
                    'market_predictions': analysis.market_predictions,
                    'key_insights_count': analysis.key_insights_count,
                    'analysis_completeness_score': analysis.analysis_completeness_score,
                    'analysis_created_at': analysis.analysis_created_at.isoformat()
                }
            }
        })
        
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)

@csrf_exempt
@require_http_methods(["DELETE"])
def snowai_delete_transcript_analysis_endpoint(request, analysis_uuid):
    """Delete an analysis"""
    try:
        analysis = get_object_or_404(SnowAITranscriptAnalysis, analysis_uuid=analysis_uuid)
        transcript_uuid = analysis.transcript.transcript_uuid
        analysis.delete()
        
        return JsonResponse({
            'success': True,
            'message': 'Analysis deleted successfully',
            'transcript_uuid': transcript_uuid
        })
        
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["GET", "POST"])
def snowai_fetch_market_ohlc_data(request):
    """
    Fetch OHLC market data using yfinance
    Endpoint: /api/snowai-market-ohlc/
    
    Parameters:
    - symbol: Asset symbol (e.g., AAPL, EURUSD=X, GC=F)
    - interval: Timeframe (1m, 5m, 15m, 1h, 4h, 1d, 1wk)
    - period: Data period (1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, max)
    """
    try:
        # Get parameters
        if request.method == 'POST':
            data = json.loads(request.body)
            symbol = data.get('symbol')
            interval = data.get('interval', '1h')
            period = data.get('period', '1mo')
        else:
            symbol = request.GET.get('symbol')
            interval = request.GET.get('interval', '1h')
            period = request.GET.get('period', '1mo')
        
        if not symbol:
            return JsonResponse({
                'error': 'Symbol parameter is required'
            }, status=400)
        
        # Map frontend intervals to yfinance intervals
        interval_mapping = {
            '1m': '1m',
            '5m': '5m',
            '15m': '15m',
            '1h': '1h',
            '4h': '4h',
            '1d': '1d',
            '1w': '1wk'
        }
        
        yf_interval = interval_mapping.get(interval, '1h')
        
        # Fetch data from yfinance
        ticker = yf.Ticker(symbol)
        df = ticker.history(period=period, interval=yf_interval)
        
        if df.empty:
            return JsonResponse({
                'error': f'No data available for {symbol}',
                'symbol': symbol
            }, status=404)
        
        # Convert DataFrame to list of OHLC dictionaries
        data = []
        for index, row in df.iterrows():
            # Convert timezone-aware datetime to Unix timestamp
            timestamp = int(index.timestamp())
            
            data.append({
                'time': timestamp,
                'open': float(row['Open']),
                'high': float(row['High']),
                'low': float(row['Low']),
                'close': float(row['Close']),
                'volume': float(row['Volume']) if 'Volume' in row else 0
            })
        
        # Sort by time
        data.sort(key=lambda x: x['time'])
        
        return JsonResponse({
            'success': True,
            'symbol': symbol,
            'interval': interval,
            'period': period,
            'data_points': len(data),
            'data': data,
            'source': 'yfinance',
            'first_date': datetime.fromtimestamp(data[0]['time']).isoformat() if data else None,
            'last_date': datetime.fromtimestamp(data[-1]['time']).isoformat() if data else None
        })
        
    except Exception as e:
        return JsonResponse({
            'error': str(e),
            'symbol': symbol if 'symbol' in locals() else 'unknown'
        }, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def fetch_latest_ai_council_discussion_for_livingston(request):
    """Fetch the most recent AI Trading Council Discussion for Livingston AI assistant"""
    try:
        latest_discussion = AITradingCouncilConversation.objects.filter(
            status='completed'
        ).order_by('-created_at').first()
        
        # Fetch ALL transcript analyses - only the fields we need
        all_analyses = SnowAITranscriptAnalysis.objects.select_related(
            'transcript'
        ).order_by('-analysis_created_at')
        
        transcript_insights = []
        for analysis in all_analyses:
            transcript_insights.append({
                'executive_summary': analysis.executive_summary,
                'key_themes': analysis.key_themes,
                'speaker_name': analysis.transcript.primary_speaker_name,
                'country_code': analysis.transcript.speaker_country_code,
            })
        
        if not latest_discussion:
            return JsonResponse({
                'success': True,
                'has_discussion': False,
                'message': 'No completed discussions found',
                'has_transcript_insights': len(transcript_insights) > 0,
                'transcript_insights': transcript_insights,
                'transcript_insights_count': len(transcript_insights)
            })
        
        discussion_context = {
            'conversation_id': latest_discussion.conversation_id,
            'title': latest_discussion.title,
            'created_at': latest_discussion.created_at.isoformat(),
            'participating_assets': latest_discussion.participating_assets,
            'total_participants': latest_discussion.total_participants,
            'conversation_summary': latest_discussion.conversation_summary,
            'overall_economic_outlook': latest_discussion.overall_economic_outlook,
            'global_market_sentiment': latest_discussion.global_market_sentiment,
            'market_volatility_level': latest_discussion.market_volatility_level,
            'major_economic_themes': latest_discussion.major_economic_themes,
            'currency_strength_rankings': latest_discussion.currency_strength_rankings,
            'risk_factors_identified': latest_discussion.risk_factors_identified,
            'opportunity_areas': latest_discussion.opportunity_areas,
            'bullish_sentiment_count': latest_discussion.bullish_sentiment_count,
            'bearish_sentiment_count': latest_discussion.bearish_sentiment_count,
            'neutral_sentiment_count': latest_discussion.neutral_sentiment_count,
            'average_confidence_score': latest_discussion.average_confidence_score,
            'dominant_sentiment': latest_discussion.get_dominant_sentiment(),
            'conversation_turns_count': latest_discussion.get_conversation_turns_count(),
            'full_conversation': latest_discussion.conversation_data
        }
        
        return JsonResponse({
            'success': True,
            'has_discussion': True,
            'discussion': discussion_context,
            'has_transcript_insights': len(transcript_insights) > 0,
            'transcript_insights': transcript_insights,
            'transcript_insights_count': len(transcript_insights)
        })
        
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def snowai_fetch_time_separators(request):
    """
    Fetch week and month separator timestamps for chart visualization
    Endpoint: /api/snowai-time-separators/
    
    Parameters:
    - symbol: Asset symbol (e.g., AAPL, EURUSD=X, GC=F)
    - period: Data period (1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, max)
    """
    try:
        data = json.loads(request.body)
        symbol = data.get('symbol')
        period = data.get('period', '1y')
        
        if not symbol:
            return JsonResponse({
                'error': 'Symbol parameter is required'
            }, status=400)
        
        # Fetch weekly data
        ticker = yf.Ticker(symbol)
        weekly_df = ticker.history(period=period, interval='1wk')
        monthly_df = ticker.history(period=period, interval='1mo')
        
        if weekly_df.empty and monthly_df.empty:
            return JsonResponse({
                'error': f'No data available for {symbol}',
                'symbol': symbol
            }, status=404)
        
        # Extract week start timestamps
        week_starts = []
        if not weekly_df.empty:
            for index in weekly_df.index:
                timestamp = int(index.timestamp())
                week_starts.append(timestamp)
        
        # Extract month start timestamps
        month_starts = []
        if not monthly_df.empty:
            for index in monthly_df.index:
                timestamp = int(index.timestamp())
                month_starts.append(timestamp)
        
        # Sort timestamps
        week_starts.sort()
        month_starts.sort()
        
        return JsonResponse({
            'success': True,
            'symbol': symbol,
            'period': period,
            'week_starts': week_starts,
            'month_starts': month_starts,
            'week_count': len(week_starts),
            'month_count': len(month_starts)
        })
        
    except Exception as e:
        return JsonResponse({
            'error': str(e),
            'symbol': symbol if 'symbol' in locals() else 'unknown'
        }, status=500)


import numpy as np
from scipy.stats import pearsonr

# ============================================================================
# ASSET TICKER MAPPINGS - ADDED STOCKS SECTION (20 major stocks)
# ============================================================================
ASSET_TICKERS = {
    'forex': {
        'EURUSD': 'EURUSD=X',
        'GBPUSD': 'GBPUSD=X',
        'USDJPY': 'USDJPY=X',
        'USDCHF': 'USDCHF=X',
        'AUDUSD': 'AUDUSD=X',
        'NZDUSD': 'NZDUSD=X',
        'USDCAD': 'USDCAD=X',
        'DXY': 'DX-Y.NYB'
    },
    'bonds': {
        'US30Y': 'ZB=F',
        'US10Y': '^TNX',
        'US5Y': '^FVX',
        'US2Y': '^IRX',
        'German 10Y': '^TNX',
        'UK 10Y': '^TNX'
    },
    'commodities': {
        'Gold': 'GC=F',
        'Silver': 'SI=F',
        'Crude Oil': 'CL=F',
        'Copper': 'HG=F',
        'Natural Gas': 'NG=F',
        'Platinum': 'PL=F'
    },
    'indices': {
        'S&P 500': '^GSPC',
        'Nasdaq': '^IXIC',
        'Dow Jones': '^DJI',
        'Russell 2000': '^RUT',
        'VIX': '^VIX',
        'FTSE 100': '^FTSE',
        'DAX': '^GDAXI',
        'Nikkei': '^N225'
    },
    # ========== NEW: STOCKS SECTION ==========
    'stocks': {
        'Apple': 'AAPL',
        'Microsoft': 'MSFT',
        'Amazon': 'AMZN',
        'Google': 'GOOGL',
        'Tesla': 'TSLA',
        'NVIDIA': 'NVDA',
        'Meta': 'META',
        'Berkshire': 'BRK-B',
        'JPMorgan': 'JPM',
        'Visa': 'V',
        'Johnson & Johnson': 'JNJ',
        'Walmart': 'WMT',
        'Exxon Mobil': 'XOM',
        'Procter & Gamble': 'PG',
        'Mastercard': 'MA',
        'Home Depot': 'HD',
        'Chevron': 'CVX',
        'Pfizer': 'PFE',
        'Coca-Cola': 'KO',
        'Boeing': 'BA',

        'Eli Lilly': 'LLY',
        'Cencora': 'COR',
        'Cisco Systems': 'CSCO',
        'Intel': 'INTC',
        'Oracle': 'ORCL',
        'IBM': 'IBM',
        'Salesforce': 'CRM',
        'PayPal': 'PYPL',
        'Netflix': 'NFLX',
        'Adobe': 'ADBE',
        'Goldman Sachs': 'GS',
        'Bank of America': 'BAC',
        'Citigroup': 'C',
        'UnitedHealth': 'UNH',
        'PepsiCo': 'PEP',
        'McDonald\'s': 'MCD',
        'Nike': 'NKE',
        'Caterpillar': 'CAT'
    }

}

def calculate_correlation(ticker1, ticker2, period='3mo'):
    """Calculate correlation between two assets"""
    try:
        stock1 = yf.Ticker(ticker1)
        stock2 = yf.Ticker(ticker2)
        
        hist1 = stock1.history(period=period)
        hist2 = stock2.history(period=period)
        
        if hist1.empty or hist2.empty:
            return None
        
        # Align the data by date
        df1 = hist1['Close'].pct_change().dropna()
        df2 = hist2['Close'].pct_change().dropna()
        
        # Find common dates
        common_dates = df1.index.intersection(df2.index)
        
        if len(common_dates) < 20:  # Need at least 20 data points
            return None
        
        returns1 = df1.loc[common_dates]
        returns2 = df2.loc[common_dates]
        
        correlation, p_value = pearsonr(returns1, returns2)
        
        return {
            'correlation': round(correlation, 3),
            'p_value': round(p_value, 4),
            'significant': p_value < 0.05
        }
    except Exception as e:
        print(f"Error calculating correlation: {str(e)}")
        return None

# ============================================================================
# GENERATE INTERMARKET INSIGHTS - ADDED STOCKS SECTION
# ============================================================================
def generate_intermarket_insights(asset_class, asset_data):
    """Generate trading insights based on intermarket analysis"""
    insights = []
    
    try:
        # Get DXY (Dollar Index) data
        dxy_changes = calculate_price_changes(ASSET_TICKERS['forex']['DXY'])
        
        # Get Gold data
        gold_changes = calculate_price_changes(ASSET_TICKERS['commodities']['Gold'])
        
        # Get US10Y data
        us10y_changes = calculate_price_changes(ASSET_TICKERS['bonds']['US10Y'])
        
        # Get VIX data
        vix_changes = calculate_price_changes(ASSET_TICKERS['indices']['VIX'])
        
        # Forex-specific insights
        if asset_class == 'forex':
            if dxy_changes and dxy_changes.get('1d', {}).get('percent'):
                dxy_daily = dxy_changes['1d']['percent']
                
                if dxy_daily < -0.5:
                    insights.append({
                        'type': 'bullish',
                        'message': f'USD Index down {abs(dxy_daily):.2f}% today. Consider long positions on EURUSD, GBPUSD, AUDUSD.',
                        'strength': 'moderate' if abs(dxy_daily) < 1 else 'strong'
                    })
                elif dxy_daily > 0.5:
                    insights.append({
                        'type': 'bearish',
                        'message': f'USD Index up {dxy_daily:.2f}% today. Consider short positions on EUR/GBP pairs or long USDJPY.',
                        'strength': 'moderate' if dxy_daily < 1 else 'strong'
                    })
            
            if gold_changes and gold_changes.get('1wk', {}).get('percent'):
                gold_weekly = gold_changes['1wk']['percent']
                if gold_weekly > 2:
                    insights.append({
                        'type': 'bullish',
                        'message': f'Gold up {gold_weekly:.2f}% this week. Risk-off sentiment may support safe-haven currencies like JPY, CHF.',
                        'strength': 'moderate'
                    })
        
        # Commodities insights
        elif asset_class == 'commodities':
            if dxy_changes:
                dxy_monthly = dxy_changes.get('1mo', {}).get('percent', 0)
                if abs(dxy_monthly) > 2:
                    direction = 'inverse' if dxy_monthly > 0 else 'positive'
                    insights.append({
                        'type': 'correlation',
                        'message': f'USD Index {dxy_monthly:+.2f}% this month. Typically {direction} correlation with commodities.',
                        'strength': 'strong' if abs(dxy_monthly) > 4 else 'moderate'
                    })
            
            if us10y_changes:
                yield_monthly = us10y_changes.get('1mo', {}).get('percent', 0)
                if abs(yield_monthly) > 5:
                    insights.append({
                        'type': 'info',
                        'message': f'10Y Yields {yield_monthly:+.2f}% this month. Rising yields typically pressure Gold/Silver.',
                        'strength': 'moderate'
                    })
        
        # Bonds insights
        elif asset_class == 'bonds':
            if vix_changes:
                vix_weekly = vix_changes.get('1wk', {}).get('percent', 0)
                if vix_weekly > 10:
                    insights.append({
                        'type': 'bullish',
                        'message': f'VIX up {vix_weekly:.2f}% this week. Flight to safety may boost bond prices (lower yields).',
                        'strength': 'strong'
                    })
            
            # Check yield curve
            us10y_curr = us10y_changes.get('current_price', 0)
            us2y_changes = calculate_price_changes(ASSET_TICKERS['bonds']['US2Y'])
            us2y_curr = us2y_changes.get('current_price', 0) if us2y_changes else 0
            
            if us10y_curr and us2y_curr:
                spread = us10y_curr - us2y_curr
                if spread < 0:
                    insights.append({
                        'type': 'warning',
                        'message': f'Inverted yield curve detected (spread: {spread:.2f}%). Historically indicates recession risk.',
                        'strength': 'strong'
                    })
                elif spread > 2:
                    insights.append({
                        'type': 'info',
                        'message': f'Steep yield curve (spread: {spread:.2f}%). Typically indicates strong growth expectations.',
                        'strength': 'moderate'
                    })
        
        # Indices insights
        elif asset_class == 'indices':
            if us10y_changes:
                yield_weekly = us10y_changes.get('1wk', {}).get('percent', 0)
                if yield_weekly > 3:
                    insights.append({
                        'type': 'bearish',
                        'message': f'10Y yields surging (+{yield_weekly:.2f}% weekly). May pressure equity valuations.',
                        'strength': 'moderate'
                    })
            
            if vix_changes:
                vix_curr = vix_changes.get('current_price', 0)
                if vix_curr > 25:
                    insights.append({
                        'type': 'warning',
                        'message': f'VIX elevated at {vix_curr:.2f}. High volatility environment - exercise caution.',
                        'strength': 'strong'
                    })
                elif vix_curr < 15:
                    insights.append({
                        'type': 'info',
                        'message': f'VIX low at {vix_curr:.2f}. Low volatility may indicate complacency or stable conditions.',
                        'strength': 'moderate'
                    })
        
        # ========== NEW: STOCKS INSIGHTS ==========
        elif asset_class == 'stocks':
            if vix_changes:
                vix_curr = vix_changes.get('current_price', 0)
                vix_weekly = vix_changes.get('1wk', {}).get('percent', 0)
                
                if vix_curr > 25:
                    insights.append({
                        'type': 'warning',
                        'message': f'VIX at {vix_curr:.2f}. Expect increased stock volatility. Consider defensive positions or hedges.',
                        'strength': 'strong'
                    })
                elif vix_weekly > 20:
                    insights.append({
                        'type': 'caution',
                        'message': f'VIX spiking +{vix_weekly:.2f}% this week. Market uncertainty rising - volatility ahead.',
                        'strength': 'moderate'
                    })
            
            if us10y_changes:
                yield_curr = us10y_changes.get('current_price', 0)
                yield_monthly = us10y_changes.get('1mo', {}).get('percent', 0)
                
                if yield_curr > 4.5:
                    insights.append({
                        'type': 'bearish',
                        'message': f'10Y yields at {yield_curr:.2f}%. High rates pressure growth stocks and valuations.',
                        'strength': 'moderate'
                    })
                elif yield_monthly > 10:
                    insights.append({
                        'type': 'warning',
                        'message': f'Yields surging +{yield_monthly:.2f}% monthly. Tech/growth sectors most vulnerable.',
                        'strength': 'strong'
                    })
            
            if dxy_changes:
                dxy_monthly = dxy_changes.get('1mo', {}).get('percent', 0)
                if dxy_monthly > 3:
                    insights.append({
                        'type': 'info',
                        'message': f'Strong USD (+{dxy_monthly:.2f}% monthly) may pressure multinational earnings and export-heavy stocks.',
                        'strength': 'moderate'
                    })
            
            # Check tech correlation with Nasdaq
            nasdaq_changes = calculate_price_changes(ASSET_TICKERS['indices']['Nasdaq'])
            if nasdaq_changes:
                nasdaq_daily = nasdaq_changes.get('1d', {}).get('percent', 0)
                if abs(nasdaq_daily) > 2:
                    direction = 'rallying' if nasdaq_daily > 0 else 'selling off'
                    insights.append({
                        'type': 'info',
                        'message': f'Nasdaq {direction} {abs(nasdaq_daily):.2f}% today. Tech stocks showing strong directional movement.',
                        'strength': 'moderate' if abs(nasdaq_daily) < 3 else 'strong'
                    })
        
    except Exception as e:
        print(f"Error generating insights: {str(e)}")
    
    return insights


@csrf_exempt
@require_http_methods(["POST"])
def snowai_asset_correlation_calculate_correlations(request):
    """Calculate correlations between selected assets"""
    try:
        data = json.loads(request.body)
        asset_class = data.get('asset_class', 'forex')
        period = data.get('period', '3mo')
        
        if asset_class not in ASSET_TICKERS:
            return JsonResponse({'error': 'Invalid asset class'}, status=400)
        
        assets = ASSET_TICKERS[asset_class]
        correlations = []
        
        # Calculate correlations between all pairs
        asset_list = list(assets.items())
        for i in range(len(asset_list)):
            for j in range(i + 1, len(asset_list)):
                name1, ticker1 = asset_list[i]
                name2, ticker2 = asset_list[j]
                
                corr_data = calculate_correlation(ticker1, ticker2, period)
                
                if corr_data:
                    correlations.append({
                        'asset1': name1,
                        'asset2': name2,
                        'correlation': corr_data['correlation'],
                        'significant': corr_data['significant']
                    })
        
        # Sort by absolute correlation value
        correlations.sort(key=lambda x: abs(x['correlation']), reverse=True)
        
        return JsonResponse({
            'success': True,
            'correlations': correlations,
            'period': period,
            'asset_class': asset_class
        })
    
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)

@csrf_exempt
@require_http_methods(["GET"])
def snowai_asset_correlation_get_all_classes(request):
    """Get available asset classes"""
    return JsonResponse({
        'success': True,
        'asset_classes': list(ASSET_TICKERS.keys())
    })


def calculate_price_changes(ticker, periods=['1d', '1wk', '1mo', '3mo']):
    """Calculate price changes for different periods"""
    try:
        stock = yf.Ticker(ticker)
        hist = stock.history(period='6mo')
        
        if hist.empty:
            return None
        
        current_price = hist['Close'].iloc[-1]
        changes = {'current_price': round(current_price, 4)}
        
        period_days = {
            '1d': 1,
            '1wk': 5,
            '1mo': 21,
            '3mo': 63
        }
        
        for period, days in period_days.items():
            if len(hist) > days:
                past_price = hist['Close'].iloc[-(days + 1)]
                change_pct = ((current_price - past_price) / past_price) * 100
                change_abs = current_price - past_price
                changes[period] = {
                    'percent': round(change_pct, 2),
                    'absolute': round(change_abs, 4),
                    'past_price': round(past_price, 4)
                }
            else:
                changes[period] = None
        
        return changes
    except Exception as e:
        print(f"Error calculating changes for {ticker}: {str(e)}")
        return None


@csrf_exempt
@require_http_methods(["GET"])
def snowai_asset_correlation_get_data(request):
    """Get asset price changes and correlation data"""
    try:
        asset_class = request.GET.get('asset_class', 'forex')
        
        if asset_class not in ASSET_TICKERS:
            return JsonResponse({'error': 'Invalid asset class'}, status=400)
        
        assets = ASSET_TICKERS[asset_class]
        result = {}
        
        for name, ticker in assets.items():
            changes = calculate_price_changes(ticker)
            if changes:
                result[name] = changes
        
        return JsonResponse({
            'success': True,
            'asset_class': asset_class,
            'data': result,
            'timestamp': datetime.now().isoformat()
        })
    
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def snowai_intermarket_council_driven_asset_sentiment_analysis_v2(request):
    """
    Get AI-powered sentiment analysis for a specific asset based on latest AI Trading Council discussion
    Very unique name to avoid conflicts in large codebase
    """
    try:
        data = json.loads(request.body)
        asset_name = data.get('asset_name')
        asset_class = data.get('asset_class')
        all_asset_movements = data.get('all_asset_movements', {})
        openai_api_key = data.get('openai_api_key')
        
        if not asset_name or not asset_class:
            return JsonResponse({'error': 'Asset name and class required'}, status=400)
        
        # Get the most recent AI Trading Council conversation
        latest_council = AITradingCouncilConversation.objects.filter(
            status='completed'
        ).order_by('-created_at').first()
        
        if not latest_council:
            return JsonResponse({
                'error': 'No completed AI Trading Council discussions found'
            }, status=404)
        
        # Prepare council summary
        council_summary = {
            'economic_outlook': latest_council.overall_economic_outlook,
            'market_sentiment': latest_council.global_market_sentiment,
            'volatility': latest_council.market_volatility_level,
            'major_themes': latest_council.major_economic_themes,
            'risk_factors': latest_council.risk_factors_identified,
            'opportunities': latest_council.opportunity_areas,
            'summary': latest_council.conversation_summary,
            'timestamp': latest_council.created_at.isoformat()
        }
        
        # Format all asset movements for context
        movements_text = "\n".join([
            f"{name}: Current ${info.get('current_price', 'N/A')}, "
            f"Daily: {info.get('1d', {}).get('percent', 'N/A')}%, "
            f"Weekly: {info.get('1wk', {}).get('percent', 'N/A')}%, "
            f"Monthly: {info.get('1mo', {}).get('percent', 'N/A')}%"
            for name, info in all_asset_movements.items()
        ])
        
        # Create AI prompt
        prompt = f"""You are a financial analyst. Based on the latest AI Trading Council discussion and current market movements, provide a sentiment analysis for {asset_name}.

AI Trading Council Summary (MOST RECENT):
- Economic Outlook: {council_summary['economic_outlook']}
- Market Sentiment: {council_summary['market_sentiment']}
- Volatility Level: {council_summary['volatility']}
- Major Themes: {', '.join(council_summary['major_themes'][:5]) if council_summary['major_themes'] else 'None'}
- Key Risks: {', '.join(council_summary['risk_factors'][:3]) if council_summary['risk_factors'] else 'None'}

Current Market Movements:
{movements_text}

Provide analysis in this EXACT format:
SENTIMENT: [bullish/bearish/neutral]
CONFIDENCE: [number between 0-100]
REASONING: [2-3 sentences explaining why, referencing the council discussion and current movements]

Be specific and actionable. Reference the council's insights."""

        # Call OpenAI API
        import requests
        response = requests.post(
            'https://api.openai.com/v1/chat/completions',
            headers={
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {openai_api_key}'
            },
            json={
                'model': 'gpt-4o-mini',
                'messages': [{'role': 'user', 'content': prompt}],
                'max_tokens': 250,
                'temperature': 0.7
            }
        )
        
        ai_response = response.json()
        analysis_text = ai_response['choices'][0]['message']['content'].strip()
        
        # Parse the response
        lines = analysis_text.split('\n')
        sentiment = 'neutral'
        confidence = 50
        reasoning = ''
        
        for line in lines:
            if line.startswith('SENTIMENT:'):
                sentiment = line.split(':', 1)[1].strip().lower()
            elif line.startswith('CONFIDENCE:'):
                try:
                    confidence = int(''.join(filter(str.isdigit, line.split(':', 1)[1])))
                except:
                    confidence = 50
            elif line.startswith('REASONING:'):
                reasoning = line.split(':', 1)[1].strip()
        
        # Ensure valid sentiment
        if sentiment not in ['bullish', 'bearish', 'neutral']:
            sentiment = 'neutral'
        
        return JsonResponse({
            'success': True,
            'asset_name': asset_name,
            'sentiment': sentiment,
            'confidence': confidence,
            'reasoning': reasoning,
            'council_reference': {
                'discussion_id': latest_council.conversation_id,
                'timestamp': council_summary['timestamp'],
                'economic_outlook': council_summary['economic_outlook']
            }
        })
        
    except Exception as e:
        return JsonResponse({
            'error': str(e),
            'success': False
        }, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def snowai_advanced_volume_proportion_analyzer_for_trading_assets_v2(request):
    """
    Analyze current volume in proportion to historical volume for a specific asset
    Very unique name to avoid conflicts in large codebase
    """
    try:
        data = json.loads(request.body)
        asset_name = data.get('asset_name')
        asset_class = data.get('asset_class')
        analysis_period = data.get('period', '1y')  # Default to 1 year

        if not asset_name or not asset_class:
            return JsonResponse({'error': 'Asset name and class required'}, status=400)

        # Get the ticker for this asset
        if asset_class not in ASSET_TICKERS:
            return JsonResponse({'error': 'Invalid asset class'}, status=400)

        ticker = ASSET_TICKERS[asset_class].get(asset_name)
        if not ticker:
            return JsonResponse({'error': 'Asset not found'}, status=404)

        # Fetch volume data
        stock = yf.Ticker(ticker)
        hist = stock.history(period=analysis_period)

        if hist.empty or 'Volume' not in hist.columns:
            return JsonResponse({
                'error': 'Volume data not available for this asset',
                'success': False
            }, status=404)

        # Get current volume (most recent day)
        current_volume = hist['Volume'].iloc[-1]

        # Calculate volume statistics
        volumes = hist['Volume'].values
        avg_volume = np.mean(volumes)
        std_volume = np.std(volumes)
        median_volume = np.median(volumes)

        # Calculate percentile
        percentile = (np.sum(volumes < current_volume) / len(volumes)) * 100

        # Determine volume level
        if current_volume > avg_volume + std_volume:
            volume_level = 'high'
            description = f'Significantly above average (Top {100 - percentile:.0f}%)'
        elif current_volume > avg_volume:
            volume_level = 'medium'
            description = f'Above average ({percentile:.0f}th percentile)'
        elif current_volume > avg_volume - std_volume:
            volume_level = 'medium'
            description = f'Around average ({percentile:.0f}th percentile)'
        else:
            volume_level = 'low'
            description = f'Below average (Bottom {percentile:.0f}%)'

        # Safe division for comparison metrics
        vs_avg_percent = ((current_volume - avg_volume) / avg_volume) * 100 if avg_volume else 0
        vs_median_percent = ((current_volume - median_volume) / median_volume) * 100 if median_volume else 0

        # Get recent trend (last 5 days vs previous 20 days)
        if len(volumes) >= 25:
            recent_avg = np.mean(volumes[-5:])
            prior_avg = np.mean(volumes[-25:-5])
            trend_change = ((recent_avg - prior_avg) / prior_avg) * 100 if prior_avg else 0

            if trend_change > 15:
                trend = 'increasing'
            elif trend_change < -15:
                trend = 'decreasing'
            else:
                trend = 'stable'
        else:
            trend = 'insufficient_data'
            trend_change = 0

        return JsonResponse({
            'success': True,
            'asset_name': asset_name,
            'volume_level': volume_level,
            'description': description,
            'current_volume': int(current_volume),
            'average_volume': int(avg_volume),
            'median_volume': int(median_volume),
            'vs_average_percent': round(vs_avg_percent, 2),
            'vs_median_percent': round(vs_median_percent, 2),
            'percentile': round(percentile, 1),
            'trend': trend,
            'trend_change_percent': round(trend_change, 2),
            'analysis_period': analysis_period,
            'data_points': len(volumes)
        })

    except Exception as e:
        return JsonResponse({
            'error': str(e),
            'success': False
        }, status=500)


def is_high_volume_backtest(data):
    """
    Determine if current volume is high relative to historical volume
    Returns boolean True if high volume, False otherwise
    """
    try:
        # Check if data is valid
        if data is None or data.empty:
            print("Warning: Empty dataframe provided to is_high_volume_backtest")
            return False
        
        # Check if Volume column exists
        if 'Volume' not in data.columns:
            print("Warning: 'Volume' column not found in dataframe for is_high_volume_backtest")
            return False
        
        volumes = data['Volume'].values
        
        # Need at least 2 data points for meaningful analysis
        if len(volumes) < 2:
            print(f"Warning: Insufficient data points ({len(volumes)}) for is_high_volume_backtest")
            return False
        
        # Get current volume (most recent)
        current_volume = volumes[-1]
        
        # Handle NaN or invalid current volume
        if pd.isna(current_volume) or current_volume < 0:
            print(f"Warning: Invalid current volume ({current_volume}) in is_high_volume_backtest")
            return False
        
        # Calculate statistics
        avg_volume = np.mean(volumes)
        std_volume = np.std(volumes)
        
        # Handle edge case where all volumes are the same (std = 0)
        if std_volume == 0:
            print("Warning: Zero standard deviation in volume data for is_high_volume_backtest")
            return False
        
        # High volume defined as > avg + 1 std deviation
        is_high = current_volume > (avg_volume + std_volume)
        
        return bool(is_high)
        
    except Exception as e:
        print(f"ERROR in is_high_volume_backtest: {type(e).__name__}: {str(e)}")
        return False


def is_low_volume_backtest(data):
    """
    Determine if current volume is low relative to historical volume
    Returns boolean True if low volume, False otherwise
    """
    try:
        # Check if data is valid
        if data is None or data.empty:
            print("Warning: Empty dataframe provided to is_low_volume_backtest")
            return False
        
        # Check if Volume column exists
        if 'Volume' not in data.columns:
            print("Warning: 'Volume' column not found in dataframe for is_low_volume_backtest")
            return False
        
        volumes = data['Volume'].values
        
        # Need at least 2 data points for meaningful analysis
        if len(volumes) < 2:
            print(f"Warning: Insufficient data points ({len(volumes)}) for is_low_volume_backtest")
            return False
        
        # Get current volume (most recent)
        current_volume = volumes[-1]
        
        # Handle NaN or invalid current volume
        if pd.isna(current_volume) or current_volume < 0:
            print(f"Warning: Invalid current volume ({current_volume}) in is_low_volume_backtest")
            return False
        
        # Calculate statistics
        avg_volume = np.mean(volumes)
        std_volume = np.std(volumes)
        
        # Handle edge case where all volumes are the same (std = 0)
        if std_volume == 0:
            print("Warning: Zero standard deviation in volume data for is_low_volume_backtest")
            return False
        
        # Low volume defined as < avg - 1 std deviation
        is_low = current_volume < (avg_volume - std_volume)
        
        return bool(is_low)
        
    except Exception as e:
        print(f"ERROR in is_low_volume_backtest: {type(e).__name__}: {str(e)}")
        return False


def is_medium_volume_backtest(data):
    """
    Determine if current volume is medium/average relative to historical volume
    Returns boolean True if medium volume, False otherwise
    """
    try:
        # Check if data is valid
        if data is None or data.empty:
            print("Warning: Empty dataframe provided to is_medium_volume_backtest")
            return False
        
        # Check if Volume column exists
        if 'Volume' not in data.columns:
            print("Warning: 'Volume' column not found in dataframe for is_medium_volume_backtest")
            return False
        
        volumes = data['Volume'].values
        
        # Need at least 2 data points for meaningful analysis
        if len(volumes) < 2:
            print(f"Warning: Insufficient data points ({len(volumes)}) for is_medium_volume_backtest")
            return False
        
        # Get current volume (most recent)
        current_volume = volumes[-1]
        
        # Handle NaN or invalid current volume
        if pd.isna(current_volume) or current_volume < 0:
            print(f"Warning: Invalid current volume ({current_volume}) in is_medium_volume_backtest")
            return False
        
        # Calculate statistics
        avg_volume = np.mean(volumes)
        std_volume = np.std(volumes)
        
        # Handle edge case where all volumes are the same (std = 0)
        if std_volume == 0:
            print("Warning: Zero standard deviation in volume data for is_medium_volume_backtest")
            # If all volumes are the same, current is "medium" by default
            return True
        
        # Medium volume defined as within 1 std deviation of average
        # Between (avg - std) and (avg + std)
        lower_bound = avg_volume - std_volume
        upper_bound = avg_volume + std_volume
        
        is_medium = lower_bound <= current_volume <= upper_bound
        
        return bool(is_medium)
        
    except Exception as e:
        print(f"ERROR in is_medium_volume_backtest: {type(e).__name__}: {str(e)}")
        return False


@csrf_exempt
@require_http_methods(["GET"])
def snowai_fetch_latest_council_discussion_summary_for_frontend_v2(request):
    """
    Fetch the latest AI Trading Council discussion summary for frontend use
    Very unique name to avoid conflicts in large codebase
    """
    try:
        latest_council = AITradingCouncilConversation.objects.filter(
            status='completed'
        ).order_by('-created_at').first()
        
        if not latest_council:
            return JsonResponse({
                'success': False,
                'error': 'No completed AI Trading Council discussions found'
            }, status=404)
        
        return JsonResponse({
            'success': True,
            'council_data': {
                'conversation_id': latest_council.conversation_id,
                'title': latest_council.title,
                'created_at': latest_council.created_at.isoformat(),
                'completed_at': latest_council.completed_at.isoformat() if latest_council.completed_at else None,
                'economic_outlook': latest_council.overall_economic_outlook,
                'market_sentiment': latest_council.global_market_sentiment,
                'volatility_level': latest_council.market_volatility_level,
                'major_themes': latest_council.major_economic_themes,
                'currency_strength': latest_council.currency_strength_rankings,
                'risk_factors': latest_council.risk_factors_identified,
                'opportunities': latest_council.opportunity_areas,
                'summary': latest_council.conversation_summary,
                'participating_assets': latest_council.participating_assets,
                'sentiment_breakdown': {
                    'bullish': latest_council.bullish_sentiment_count,
                    'bearish': latest_council.bearish_sentiment_count,
                    'neutral': latest_council.neutral_sentiment_count
                },
                'confidence_score': latest_council.average_confidence_score
            }
        })
        
    except Exception as e:
        return JsonResponse({
            'error': str(e),
            'success': False
        }, status=500)
        

from sklearn.linear_model import LinearRegression

@csrf_exempt
def calculate_market_stability_score(request):
    """
    Calculate Market Stability Score for given assets using improved formula
    """
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            symbols = data.get('symbols', [])
            period_days = data.get('period', 60)
            
            results = []
            all_volatilities = []
            temp_results = []
            
            # First pass: collect all data and volatilities for normalization
            for symbol in symbols:
                try:
                    # Download data
                    ticker = yf.Ticker(symbol)
                    hist = ticker.history(period=f"{period_days}d")
                    
                    if len(hist) < 20:
                        continue
                    
                    # Calculate returns
                    hist['returns'] = hist['Close'].pct_change()
                    
                    # Calculate volatility ()
                    volatility = hist['returns'].std()
                    
                    # Calculate R (trend clarity)
                    prices = hist['Close'].values
                    X = np.arange(len(prices)).reshape(-1, 1)
                    y = prices.reshape(-1, 1)
                    
                    model = LinearRegression()
                    model.fit(X, y)
                    r_squared = model.score(X, y)
                    
                    # Calculate trend consistency (directional strength)
                    returns = hist['returns'].dropna()
                    if len(returns) > 0:
                        positive_days = (returns > 0).sum()
                        trend_consistency = abs(positive_days / len(returns) - 0.5) * 2  # 0-1 scale
                    else:
                        trend_consistency = 0
                    
                    # Calculate trend strength (magnitude of slope relative to price)
                    if len(prices) > 0 and prices[0] != 0:
                        slope_per_day = model.coef_[0][0]
                        avg_price = np.mean(prices)
                        trend_strength = abs(slope_per_day * len(prices)) / avg_price if avg_price != 0 else 0
                        trend_strength = min(trend_strength, 1.0)  # Cap at 1.0
                    else:
                        trend_strength = 0
                    
                    # Calculate liquidity factor (0.8 to 1.2 based on volume)
                    avg_volume = hist['Volume'].mean()
                    
                    # Normalize volume to liquidity factor
                    if avg_volume > 10000000:  # High volume
                        liquidity_factor = 1.2
                    elif avg_volume > 1000000:  # Medium volume
                        liquidity_factor = 1.0
                    elif avg_volume > 100000:  # Low volume
                        liquidity_factor = 0.9
                    else:  # Very low volume
                        liquidity_factor = 0.8
                    
                    # Get current price and change
                    current_price = hist['Close'].iloc[-1]
                    price_change = ((hist['Close'].iloc[-1] - hist['Close'].iloc[0]) / hist['Close'].iloc[0]) * 100
                    
                    temp_results.append({
                        'symbol': symbol,
                        'volatility': volatility,
                        'r_squared': r_squared,
                        'trend_consistency': trend_consistency,
                        'trend_strength': trend_strength,
                        'liquidity_factor': liquidity_factor,
                        'current_price': current_price,
                        'price_change': price_change,
                        'data_points': len(hist),
                        'avg_volume': avg_volume
                    })
                    
                    all_volatilities.append(volatility)
                    
                except Exception as e:
                    print(f"Error processing {symbol}: {str(e)}")
                    continue
            
            # Calculate max volatility for normalization
            max_volatility = max(all_volatilities) if all_volatilities else 1.0
            
            # Second pass: calculate MSS with normalized volatility
            for temp in temp_results:
                # Normalize volatility (0 to 1, where 1 is highest volatility)
                normalized_volatility = temp['volatility'] / max_volatility if max_volatility > 0 else 0
                
                # Improved MSS Formula:
                # Combines R, trend consistency, and trend strength with softened volatility penalty
                
                # 1. Enhanced trend score (0-100)
                trend_score = (
                    temp['r_squared'] * 0.5 +           # R contribution (50%)
                    temp['trend_consistency'] * 0.3 +   # Directional consistency (30%)
                    temp['trend_strength'] * 0.2        # Magnitude of trend (20%)
                ) * 100
                
                # 2. Soften volatility penalty using square root
                # This makes moderate volatility less punishing
                stability_factor = (1 - normalized_volatility) ** 0.6
                
                # 3. Calculate base MSS
                mss = trend_score * stability_factor * temp['liquidity_factor']
                
                # 4. Ensure 0-100 range
                mss = min(max(mss, 0), 100)
                
                # Determine category with improved thresholds
                if mss >= 47:
                    category = "stable"
                    status = "Trending - Good Conditions"
                    color = "#10b981"
                elif mss >= 30:
                    category = "choppy"
                    status = "Choppy - Moderate Risk"
                    color = "#f59e0b"
                else:
                    category = "volatile"
                    status = "Volatile - Avoid Trading"
                    color = "#ef4444"
                
                results.append({
                    'symbol': temp['symbol'],
                    'mss': round(mss, 2),
                    'volatility': round(temp['volatility'], 4),
                    'normalized_volatility': round(normalized_volatility, 4),
                    'r_squared': round(temp['r_squared'], 4),
                    'trend_consistency': round(temp['trend_consistency'], 4),
                    'trend_strength': round(temp['trend_strength'], 4),
                    'liquidity_factor': round(temp['liquidity_factor'], 2),
                    'category': category,
                    'status': status,
                    'color': color,
                    'current_price': round(temp['current_price'], 2),
                    'price_change': round(temp['price_change'], 2),
                    'data_points': temp['data_points'],
                    'avg_volume': int(temp['avg_volume'])
                })
            
            # Sort by MSS descending
            results.sort(key=lambda x: x['mss'], reverse=True)
            
            return JsonResponse({
                'success': True,
                'data': results,
                'timestamp': datetime.now().isoformat()
            })
            
        except Exception as e:
            return JsonResponse({
                'success': False,
                'error': str(e)
            }, status=400)
    
    return JsonResponse({'error': 'Method not allowed'}, status=405)


@csrf_exempt
def get_mss_historical_data(request):
    """
    Get historical MSS data for a specific symbol using improved formula
    """
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            symbol = data.get('symbol')
            period_days = data.get('period', 180)
            
            ticker = yf.Ticker(symbol)
            hist = ticker.history(period=f"{period_days}d")
            
            if len(hist) < 30:
                return JsonResponse({
                    'success': False,
                    'error': 'Insufficient data'
                }, status=400)
            
            # Calculate rolling MSS
            window = 20
            mss_history = []
            all_window_volatilities = []
            
            # First pass: collect all window volatilities for normalization
            for i in range(window, len(hist)):
                window_data = hist.iloc[i-window:i]
                returns = window_data['Close'].pct_change()
                volatility = returns.std()
                all_window_volatilities.append(volatility)
            
            max_volatility = max(all_window_volatilities) if all_window_volatilities else 1.0
            
            # Second pass: calculate MSS with normalization
            for i in range(window, len(hist)):
                window_data = hist.iloc[i-window:i]
                returns = window_data['Close'].pct_change()
                volatility = returns.std()
                
                # Normalize volatility
                normalized_volatility = volatility / max_volatility if max_volatility > 0 else 0
                
                # Calculate R
                prices = window_data['Close'].values
                X = np.arange(len(prices)).reshape(-1, 1)
                y = prices.reshape(-1, 1)
                
                model = LinearRegression()
                model.fit(X, y)
                r_squared = model.score(X, y)
                
                # Calculate trend consistency
                returns_clean = returns.dropna()
                if len(returns_clean) > 0:
                    positive_days = (returns_clean > 0).sum()
                    trend_consistency = abs(positive_days / len(returns_clean) - 0.5) * 2
                else:
                    trend_consistency = 0
                
                # Calculate trend strength
                if len(prices) > 0 and prices[0] != 0:
                    slope_per_day = model.coef_[0][0]
                    avg_price = np.mean(prices)
                    trend_strength = abs(slope_per_day * len(prices)) / avg_price if avg_price != 0 else 0
                    trend_strength = min(trend_strength, 1.0)
                else:
                    trend_strength = 0
                
                # Calculate liquidity factor
                avg_volume = window_data['Volume'].mean()
                if avg_volume > 10000000:
                    liquidity_factor = 1.2
                elif avg_volume > 1000000:
                    liquidity_factor = 1.0
                elif avg_volume > 100000:
                    liquidity_factor = 0.9
                else:
                    liquidity_factor = 0.8
                
                # Improved MSS calculation
                trend_score = (
                    r_squared * 0.5 +
                    trend_consistency * 0.3 +
                    trend_strength * 0.2
                ) * 100
                
                stability_factor = (1 - normalized_volatility) ** 0.6
                mss = trend_score * stability_factor * liquidity_factor
                mss = min(max(mss, 0), 100)
                
                mss_history.append({
                    'date': window_data.index[-1].strftime('%Y-%m-%d'),
                    'mss': round(mss, 2),
                    'price': round(window_data['Close'].iloc[-1], 2)
                })
            
            return JsonResponse({
                'success': True,
                'symbol': symbol,
                'data': mss_history
            })
            
        except Exception as e:
            return JsonResponse({
                'success': False,
                'error': str(e)
            }, status=400)
    
    return JsonResponse({'error': 'Method not allowed'}, status=405)

@csrf_exempt
def get_predefined_asset_lists(request):
    """
    Get predefined lists of assets by category
    """
    asset_lists = {
        'forex': [
            'EURUSD=X', 'GBPUSD=X', 'USDJPY=X', 'AUDUSD=X', 
            'USDCAD=X', 'USDCHF=X', 'NZDUSD=X', 'EURGBP=X',
            'EURJPY=X', 'GBPJPY=X', 'AUDJPY=X', 'EURCHF=X'
        ],
        'stocks': [
            # Tech Giants
            'AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN', 'NVDA', 
            'TSLA', 'META', 'AMD', 'INTC', 'ORCL', 'CSCO',
            'ADBE', 'CRM', 'AVGO', 'QCOM', 'TXN', 'AMAT',
            'LRCX', 'KLAC', 'SNPS', 'CDNS', 'MRVL', 'NXPI',
            
            # Financial Services
            'JPM', 'BAC', 'WFC', 'C', 'GS', 'MS', 'BLK',
            'SCHW', 'AXP', 'SPGI', 'CME', 'ICE', 'MCO',
            'BK', 'USB', 'PNC', 'TFC', 'COF',
            
            # Healthcare & Pharma
            'JNJ', 'LLY', 'UNH', 'PFE', 'ABBV', 'MRK', 'TMO',
            'ABT', 'DHR', 'BMY', 'AMGN', 'GILD', 'CVS',
            'CI', 'ELV', 'HUM', 'VRTX', 'REGN', 'ISRG',
            
            # Consumer Discretionary
            'AMZN', 'TSLA', 'HD', 'MCD', 'NKE', 'SBUX', 'TJX',
            'LOW', 'BKNG', 'MAR', 'CMG', 'F', 'GM', 'ABNB',
            
            # Consumer Staples
            'WMT', 'PG', 'KO', 'PEP', 'COST', 'PM', 'MO',
            'MDLZ', 'CL', 'KMB', 'GIS', 'KHC', 'STZ',
            
            # Energy
            'XOM', 'CVX', 'COP', 'EOG', 'SLB', 'MPC', 'PSX',
            'VLO', 'OXY', 'HAL', 'DVN', 'HES', 'BKR',
            
            # Industrials
            'BA', 'HON', 'UNP', 'CAT', 'GE', 'RTX', 'LMT',
            'UPS', 'DE', 'MMM', 'GD', 'NOC', 'FDX', 'CSX',
            
            # Telecom & Media
            'T', 'VZ', 'CMCSA', 'NFLX', 'DIS', 'TMUS', 'CHTR',
            
            # Real Estate & REITs
            'AMT', 'PLD', 'CCI', 'EQIX', 'PSA', 'SPG', 'O',
            
            # Materials
            'LIN', 'APD', 'SHW', 'ECL', 'DD', 'NEM', 'FCX',
            
            # Utilities
            'NEE', 'DUK', 'SO', 'D', 'AEP', 'EXC', 'SRE',
            
            # Payment Processors
            'V', 'MA', 'PYPL', 'ADP', 'FISV', 'FIS',
            
            # E-commerce & Retail
            'SHOP', 'MELI', 'EBAY', 'ETSY', 'TGT', 'ROST',
            
            # Semiconductors (Additional)
            'MU', 'ADI', 'MPWR', 'SWKS', 'QRVO', 'ON',
            
            # Software & Cloud
            'NOW', 'INTU', 'WDAY', 'PANW', 'CRWD', 'ZS',
            'DDOG', 'NET', 'SNOW', 'PLTR', 'TEAM',
            
            # Biotech
            'BIIB', 'MRNA', 'BNTX', 'SGEN', 'ALNY', 'BGNE',
            
            # Aerospace & Defense
            'HWM', 'TDG', 'HEI', 'LHX', 'TXT',
            
            # Chinese ADRs
            'BABA', 'JD', 'PDD', 'BIDU', 'NIO', 'XPEV', 'LI',
            
            # EVs & Battery
            'RIVN', 'LCID', 'ALB', 'SQM',
            
            # Cybersecurity
            'FTNT', 'OKTA', 'S', 'CYBR',
            
            # Entertainment & Gaming
            'EA', 'TTWO', 'RBLX', 'U',
            
            # Travel & Hospitality
            'AAL', 'DAL', 'UAL', 'LUV', 'CCL', 'RCL',
            
            # Insurance
            'BRK-B', 'PGR', 'ALL', 'TRV', 'AIG', 'MET', 'PRU',
            
            # Food & Beverage
            'MCD', 'YUM', 'SBUX', 'DPZ', 'QSR', 'CMG',
            
            # Pharma Services
            'MCK', 'CAH', 'COR', 'IDXX', 'A', 'WAT',
            
            # Chemicals
            'DOW', 'LYB', 'CE', 'ALB', 'EMN',
        ],
        'indices': [
            # US Indices
            '^GSPC', '^DJI', '^IXIC', '^RUT', '^VIX',
            
            # European Indices
            '^FTSE', '^GDAXI', '^FCHI', '^IBEX', '^AEX',
            '^SSMI', '^OMXS30', '^BFX',
            
            # Asian Indices
            '^N225', '^HSI', '000001.SS', '^STI', '^BSESN',
            '^NSEI', '^KS11', '^TWII', '^JKSE',
            
            # Other Global Indices
            '^AXJO', '^GSPTSE', '^MXX', '^BVSP', '^MERV',
        ],
        'commodities': [
            # Precious Metals
            'GC=F', 'SI=F', 'PL=F', 'PA=F',
            
            # Energy
            'CL=F', 'BZ=F', 'NG=F', 'RB=F', 'HO=F',
            
            # Base Metals
            'HG=F', 'ALI=F',
            
            # Agricultural
            'ZC=F', 'ZW=F', 'ZS=F', 'KC=F', 'SB=F',
            'CT=F', 'CC=F', 'LBS=F',
        ],
        'bonds': [
            # US Treasury Yields
            '^TNX', '^TYX', '^FVX', '^IRX',
            
            # Treasury Futures
            'ZN=F', 'ZB=F', 'ZT=F', 'ZF=F',
        ]
    }
    
    return JsonResponse({
        'success': True,
        'asset_lists': asset_lists
    })


@csrf_exempt
@require_http_methods(["GET"])
def snowai_get_all_hedge_funds(request):
    """Get all hedge funds with their related data"""
    try:
        hedge_funds = SnowAIHedgeFundEntity.objects.all()
        
        data = []
        for fund in hedge_funds:
            key_people = fund.key_people.all()
            resources = fund.resources.all()
            performance = fund.performance_data.all()
            
            fund_data = {
                'id': fund.id,
                'name': fund.name,
                'logo_url': fund.logo_url,
                'description': fund.description,
                'founded_year': fund.founded_year,
                'aum': fund.aum,
                'strategy': fund.strategy,
                'headquarters': fund.headquarters,
                'website': fund.website,
                'key_people': [
                    {
                        'id': person.id,
                        'name': person.name,
                        'role': person.role,
                        'wikipedia_url': person.wikipedia_url,
                        'linkedin_url': person.linkedin_url,
                        'bio': person.bio,
                        'photo_url': person.photo_url
                    } for person in key_people
                ],
                'resources': [
                    {
                        'id': resource.id,
                        'title': resource.title,
                        'url': resource.url,
                        'description': resource.description,
                        'resource_type': resource.resource_type
                    } for resource in resources
                ],
                'performance': [
                    {
                        'id': perf.id,
                        'year': perf.year,
                        'return_percentage': float(perf.return_percentage),
                        'notes': perf.notes
                    } for perf in performance
                ]
            }
            data.append(fund_data)
        
        return JsonResponse({'success': True, 'data': data})
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def snowai_create_hedge_fund(request):
    """Create a new hedge fund"""
    try:
        data = json.loads(request.body)
        
        fund = SnowAIHedgeFundEntity.objects.create(
            name=data.get('name'),
            logo_url=data.get('logo_url'),
            description=data.get('description'),
            founded_year=data.get('founded_year'),
            aum=data.get('aum'),
            strategy=data.get('strategy'),
            headquarters=data.get('headquarters'),
            website=data.get('website')
        )
        
        return JsonResponse({
            'success': True, 
            'data': {
                'id': fund.id,
                'name': fund.name
            }
        })
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["PUT"])
def snowai_update_hedge_fund(request, fund_id):
    """Update an existing hedge fund"""
    try:
        data = json.loads(request.body)
        fund = SnowAIHedgeFundEntity.objects.get(id=fund_id)
        
        for key, value in data.items():
            if hasattr(fund, key):
                setattr(fund, key, value)
        
        fund.save()
        
        return JsonResponse({'success': True, 'message': 'Fund updated successfully'})
    except SnowAIHedgeFundEntity.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Fund not found'}, status=404)
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["DELETE"])
def snowai_delete_hedge_fund(request, fund_id):
    """Delete a hedge fund"""
    try:
        fund = SnowAIHedgeFundEntity.objects.get(id=fund_id)
        fund.delete()
        return JsonResponse({'success': True, 'message': 'Fund deleted successfully'})
    except SnowAIHedgeFundEntity.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Fund not found'}, status=404)
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def snowai_add_key_person(request, fund_id):
    """Add a key person to a hedge fund"""
    try:
        data = json.loads(request.body)
        fund = SnowAIHedgeFundEntity.objects.get(id=fund_id)
        
        person = SnowAIHedgeFundKeyPerson.objects.create(
            hedge_fund=fund,
            name=data.get('name'),
            role=data.get('role'),
            wikipedia_url=data.get('wikipedia_url'),
            linkedin_url=data.get('linkedin_url'),
            bio=data.get('bio'),
            photo_url=data.get('photo_url')
        )
        
        return JsonResponse({'success': True, 'data': {'id': person.id}})
    except SnowAIHedgeFundEntity.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Fund not found'}, status=404)
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["DELETE"])
def snowai_delete_key_person(request, person_id):
    """Delete a key person"""
    try:
        person = SnowAIHedgeFundKeyPerson.objects.get(id=person_id)
        person.delete()
        return JsonResponse({'success': True, 'message': 'Person deleted successfully'})
    except SnowAIHedgeFundKeyPerson.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Person not found'}, status=404)
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def snowai_add_resource(request, fund_id):
    """Add a resource to a hedge fund"""
    try:
        data = json.loads(request.body)
        fund = SnowAIHedgeFundEntity.objects.get(id=fund_id)
        
        resource = SnowAIHedgeFundResource.objects.create(
            hedge_fund=fund,
            title=data.get('title'),
            url=data.get('url'),
            description=data.get('description'),
            resource_type=data.get('resource_type', 'article')
        )
        
        return JsonResponse({'success': True, 'data': {'id': resource.id}})
    except SnowAIHedgeFundEntity.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Fund not found'}, status=404)
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["DELETE"])
def snowai_delete_resource(request, resource_id):
    """Delete a resource"""
    try:
        resource = SnowAIHedgeFundResource.objects.get(id=resource_id)
        resource.delete()
        return JsonResponse({'success': True, 'message': 'Resource deleted successfully'})
    except SnowAIHedgeFundResource.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Resource not found'}, status=404)
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def snowai_add_performance(request, fund_id):
    """Add performance data to a hedge fund"""
    try:
        data = json.loads(request.body)
        fund = SnowAIHedgeFundEntity.objects.get(id=fund_id)
        
        performance, created = SnowAIHedgeFundPerformance.objects.update_or_create(
            hedge_fund=fund,
            year=data.get('year'),
            defaults={
                'return_percentage': data.get('return_percentage'),
                'notes': data.get('notes', '')
            }
        )
        
        return JsonResponse({'success': True, 'data': {'id': performance.id}})
    except SnowAIHedgeFundEntity.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Fund not found'}, status=404)
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["DELETE"])
def snowai_delete_performance(request, performance_id):
    """Delete performance data"""
    try:
        performance = SnowAIHedgeFundPerformance.objects.get(id=performance_id)
        performance.delete()
        return JsonResponse({'success': True, 'message': 'Performance data deleted successfully'})
    except SnowAIHedgeFundPerformance.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Performance data not found'}, status=404)
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


# Add these to your views.py file

@csrf_exempt
@require_http_methods(["PUT"])
def snowai_update_key_person(request, person_id):
    """Update a key person"""
    try:
        data = json.loads(request.body)
        person = SnowAIHedgeFundKeyPerson.objects.get(id=person_id)
        
        for key, value in data.items():
            if hasattr(person, key) and key != 'id':
                setattr(person, key, value)
        
        person.save()
        
        return JsonResponse({'success': True, 'message': 'Person updated successfully'})
    except SnowAIHedgeFundKeyPerson.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Person not found'}, status=404)
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["PUT"])
def snowai_update_resource(request, resource_id):
    """Update a resource"""
    try:
        data = json.loads(request.body)
        resource = SnowAIHedgeFundResource.objects.get(id=resource_id)
        
        for key, value in data.items():
            if hasattr(resource, key) and key != 'id':
                setattr(resource, key, value)
        
        resource.save()
        
        return JsonResponse({'success': True, 'message': 'Resource updated successfully'})
    except SnowAIHedgeFundResource.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Resource not found'}, status=404)
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["PUT"])
def snowai_update_performance(request, performance_id):
    """Update performance data"""
    try:
        data = json.loads(request.body)
        performance = SnowAIHedgeFundPerformance.objects.get(id=performance_id)
        
        for key, value in data.items():
            if hasattr(performance, key) and key != 'id':
                setattr(performance, key, value)
        
        performance.save()
        
        return JsonResponse({'success': True, 'message': 'Performance data updated successfully'})
    except SnowAIHedgeFundPerformance.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Performance data not found'}, status=404)
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


def is_bullish_bias(asset):
    """
    Check if asset has bullish bias
    
    Args:
        asset_name: Name of the asset (e.g., 'EURUSD', 'Apple')
    
    Returns:
        bool: True if bullish, False otherwise
    """
    try:
        recommendation = AssetBiasRecommendation.objects.get(asset_name=asset)
        return recommendation.bias == 'bullish'
    except AssetBiasRecommendation.DoesNotExist:
        return False


def is_bearish_bias(asset):
    """
    Check if asset has bearish bias
    
    Args:
        asset_name: Name of the asset (e.g., 'EURUSD', 'Apple')
    
    Returns:
        bool: True if bearish, False otherwise
    """
    try:
        recommendation = AssetBiasRecommendation.objects.get(asset_name=asset)
        return recommendation.bias == 'bearish'
    except AssetBiasRecommendation.DoesNotExist:
        return False


def is_neutral_bias(asset):
    """
    Check if asset has neutral bias
    
    Args:
        asset_name: Name of the asset (e.g., 'EURUSD', 'Apple')
    
    Returns:
        bool: True if neutral, False otherwise
    """
    try:
        recommendation = AssetBiasRecommendation.objects.get(asset_name=asset)
        return recommendation.bias == 'neutral'
    except AssetBiasRecommendation.DoesNotExist:
        return False


def is_high_volume(asset):
    """
    Check if asset has high volume
    
    Args:
        asset_name: Name of the asset (e.g., 'EURUSD', 'Apple')
    
    Returns:
        bool: True if high volume, False otherwise
    """
    try:
        recommendation = AssetBiasRecommendation.objects.get(asset_name=asset)
        return recommendation.volume == 'high'
    except AssetBiasRecommendation.DoesNotExist:
        return False


def is_medium_volume(asset):
    """
    Check if asset has medium volume
    
    Args:
        asset_name: Name of the asset (e.g., 'EURUSD', 'Apple')
    
    Returns:
        bool: True if medium volume, False otherwise
    """
    try:
        recommendation = AssetBiasRecommendation.objects.get(asset_name=asset)
        return recommendation.volume == 'medium'
    except AssetBiasRecommendation.DoesNotExist:
        return False


def is_low_volume(asset):
    """
    Check if asset has low volume
    
    Args:
        asset_name: Name of the asset (e.g., 'EURUSD', 'Apple')
    
    Returns:
        bool: True if low volume, False otherwise
    """
    try:
        recommendation = AssetBiasRecommendation.objects.get(asset_name=asset)
        return recommendation.volume == 'low'
    except AssetBiasRecommendation.DoesNotExist:
        return False


def get_asset_bias(asset_name):
    """
    Get the full bias recommendation for an asset
    
    Args:
        asset_name: Name of the asset (e.g., 'EURUSD', 'Apple')
    
    Returns:
        dict: {'bias': str, 'volume': str} or None if not found
    """
    try:
        recommendation = AssetBiasRecommendation.objects.get(asset_name=asset_name)
        return {
            'bias': recommendation.bias,
            'volume': recommendation.volume,
            'created_at': recommendation.created_at,
            'updated_at': recommendation.updated_at
        }
    except AssetBiasRecommendation.DoesNotExist:
        return None


def save_asset_bias(asset_name, bias=None, volume=None):
    """
    Save or update bias/volume for an asset
    Only updates provided fields, leaves others unchanged
    
    Args:
        asset_name: Name of the asset (e.g., 'EURUSD', 'Apple')
        bias: 'bullish', 'bearish', or 'neutral' (optional)
        volume: 'high', 'medium', or 'low' (optional)
    
    Returns:
        AssetBiasRecommendation instance
    """
    defaults = {}
    if bias is not None:
        defaults['bias'] = bias
    if volume is not None:
        defaults['volume'] = volume
    
    recommendation, created = AssetBiasRecommendation.objects.update_or_create(
        asset_name=asset_name,
        defaults=defaults
    )
    return recommendation


def delete_asset_bias(asset_name):
    """
    Delete bias recommendation for an asset
    
    Args:
        asset_name: Name of the asset (e.g., 'EURUSD', 'Apple')
    
    Returns:
        bool: True if deleted, False if not found
    """
    try:
        recommendation = AssetBiasRecommendation.objects.get(asset_name=asset_name)
        recommendation.delete()
        return True
    except AssetBiasRecommendation.DoesNotExist:
        return False


@csrf_exempt
@require_http_methods(["POST"])
def snowai_save_asset_bias_recommendation_v2(request):
    """
    Save or update bias/volume recommendation for an asset
    Endpoint: /api/snowai_save_asset_bias_recommendation_v2/
    
    Request body:
    {
        "asset_name": "EURUSD",
        "bias": "bullish",  // optional: 'bullish', 'bearish', 'neutral'
        "volume": "high"    // optional: 'high', 'medium', 'low'
    }
    """
    try:
        data = json.loads(request.body)
        asset_name = data.get('asset_name')
        bias = data.get('bias')
        volume = data.get('volume')
        
        if not asset_name:
            return JsonResponse({'error': 'asset_name is required'}, status=400)
        
        if not bias and not volume:
            return JsonResponse({'error': 'Either bias or volume must be provided'}, status=400)
        
        # Validate bias if provided
        if bias and bias not in ['bullish', 'bearish', 'neutral']:
            return JsonResponse({'error': 'Invalid bias value'}, status=400)
        
        # Validate volume if provided
        if volume and volume not in ['high', 'medium', 'low']:
            return JsonResponse({'error': 'Invalid volume value'}, status=400)
        
        # Save or update
        recommendation = save_asset_bias(asset_name, bias=bias, volume=volume)
        
        return JsonResponse({
            'success': True,
            'message': 'Asset bias/volume saved successfully',
            'asset_name': recommendation.asset_name,
            'bias': recommendation.bias,
            'volume': recommendation.volume,
            'updated_at': recommendation.updated_at.isoformat()
        })
        
    except Exception as e:
        return JsonResponse({
            'error': str(e),
            'success': False
        }, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def snowai_get_all_saved_asset_biases_v2(request):
    """
    Get all saved asset bias recommendations
    Endpoint: /api/snowai_get_all_saved_asset_biases_v2/
    """
    try:
        biases = AssetBiasRecommendation.objects.all()
        
        result = []
        for bias in biases:
            result.append({
                'asset_name': bias.asset_name,
                'bias': bias.bias,
                'volume': bias.volume,
                'created_at': bias.created_at.isoformat(),
                'updated_at': bias.updated_at.isoformat()
            })
        
        return JsonResponse({
            'success': True,
            'biases': result,
            'count': len(result)
        })
        
    except Exception as e:
        return JsonResponse({
            'error': str(e),
            'success': False
        }, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def snowai_get_asset_bias_by_name_v2(request):
    """
    Get saved bias/volume for a specific asset
    Endpoint: /api/snowai_get_asset_bias_by_name_v2/?asset_name=EURUSD
    """
    try:
        asset_name = request.GET.get('asset_name')
        
        if not asset_name:
            return JsonResponse({'error': 'asset_name parameter is required'}, status=400)
        
        try:
            bias = AssetBiasRecommendation.objects.get(asset_name=asset_name)
            
            return JsonResponse({
                'success': True,
                'asset_name': bias.asset_name,
                'bias': bias.bias,
                'volume': bias.volume,
                'created_at': bias.created_at.isoformat(),
                'updated_at': bias.updated_at.isoformat()
            })
        except AssetBiasRecommendation.DoesNotExist:
            return JsonResponse({
                'success': False,
                'error': 'No bias recommendation found for this asset'
            }, status=404)
        
    except Exception as e:
        return JsonResponse({
            'error': str(e),
            'success': False
        }, status=500)


@csrf_exempt
@require_http_methods(["DELETE"])
def snowai_delete_asset_bias_v2(request):
    """
    Delete saved bias/volume for a specific asset
    Endpoint: /api/snowai_delete_asset_bias_v2/
    
    Request body:
    {
        "asset_name": "EURUSD"
    }
    """
    try:
        data = json.loads(request.body)
        asset_name = data.get('asset_name')
        
        if not asset_name:
            return JsonResponse({'error': 'asset_name is required'}, status=400)
        
        try:
            bias = AssetBiasRecommendation.objects.get(asset_name=asset_name)
            bias.delete()
            
            return JsonResponse({
                'success': True,
                'message': f'Bias recommendation for {asset_name} deleted successfully'
            })
        except AssetBiasRecommendation.DoesNotExist:
            return JsonResponse({
                'success': False,
                'error': 'No bias recommendation found for this asset'
            }, status=404)
        
    except Exception as e:
        return JsonResponse({
            'error': str(e),
            'success': False
        }, status=500)


from django.db.models import Count, Q


@csrf_exempt
@require_http_methods(["POST"])
def calculate_trade_probability(request):
    """
    Calculate the probability of winning a trade based on historical data
    """
    try:
        data = json.loads(request.body)
        account_name = data.get('account_name')
        day_of_week = data.get('day_of_week')
        trading_session = data.get('trading_session')
        asset = data.get('asset')
        order_type = data.get('order_type')

        # Validate inputs
        if not all([account_name, day_of_week, trading_session, asset, order_type]):
            return JsonResponse({
                'error': 'Missing required fields'
            }, status=400)

        # Get the account
        try:
            account = Account.objects.get(account_name=account_name)
        except Account.DoesNotExist:
            return JsonResponse({
                'error': 'Account not found'
            }, status=404)

        # Get asset bias if available
        asset_bias = None
        try:
            bias_rec = AssetBiasRecommendation.objects.get(asset_name=asset)
            asset_bias = bias_rec.bias
        except AssetBiasRecommendation.DoesNotExist:
            pass

        # Calculate probability using multiple factors
        probability_score = calculate_weighted_probability(
            account=account,
            day_of_week=day_of_week,
            trading_session=trading_session,
            asset=asset,
            order_type=order_type,
            asset_bias=asset_bias
        )

        # Get detailed stats
        stats = get_trade_statistics(
            account=account,
            day_of_week=day_of_week,
            trading_session=trading_session,
            asset=asset,
            order_type=order_type
        )

        return JsonResponse({
            'win_probability': probability_score,
            'bias': asset_bias,
            'stats': stats,
            'factors': generate_factors_description(stats, asset_bias)
        })

    except Exception as e:
        return JsonResponse({
            'error': str(e)
        }, status=500)


def calculate_weighted_probability(account, day_of_week, trading_session, asset, order_type, asset_bias):
    """
    Calculate probability using a weighted approach based on multiple factors
    """
    weights = {
        'exact_match': 0.40,      # Exact match (day + session + asset + order_type)
        'asset_performance': 0.25, # Overall performance on this asset
        'day_performance': 0.15,   # Performance on this day
        'session_performance': 0.15, # Performance in this session
        'bias_alignment': 0.05     # Alignment with asset bias
    }
    
    scores = {}
    
    # 1. Exact match scenarios (highest weight)
    exact_matches = AccountTrades.objects.filter(
        account=account,
        day_of_week_entered=day_of_week,
        trading_session_entered=trading_session,
        asset=asset,
        order_type=order_type
    )
    
    exact_total = exact_matches.count()
    # Changed: Look for "Win" instead of "Profit"
    exact_wins = exact_matches.filter(outcome='Win').count()
    scores['exact_match'] = (exact_wins / exact_total * 100) if exact_total > 0 else 50
    
    # 2. Asset-specific performance
    asset_trades = AccountTrades.objects.filter(account=account, asset=asset)
    asset_total = asset_trades.count()
    asset_wins = asset_trades.filter(outcome='Win').count()
    scores['asset_performance'] = (asset_wins / asset_total * 100) if asset_total > 0 else 50
    
    # 3. Day of week performance
    day_trades = AccountTrades.objects.filter(account=account, day_of_week_entered=day_of_week)
    day_total = day_trades.count()
    day_wins = day_trades.filter(outcome='Win').count()
    scores['day_performance'] = (day_wins / day_total * 100) if day_total > 0 else 50
    
    # 4. Trading session performance
    session_trades = AccountTrades.objects.filter(account=account, trading_session_entered=trading_session)
    session_total = session_trades.count()
    session_wins = session_trades.filter(outcome='Win').count()
    scores['session_performance'] = (session_wins / session_total * 100) if session_total > 0 else 50
    
    # 5. Bias alignment bonus
    bias_score = 50  # Neutral default
    if asset_bias:
        if (asset_bias == 'bullish' and order_type == 'Buy') or \
           (asset_bias == 'bearish' and order_type == 'Sell'):
            bias_score = 70  # Aligned with bias
        elif (asset_bias == 'bullish' and order_type == 'Sell') or \
             (asset_bias == 'bearish' and order_type == 'Buy'):
            bias_score = 30  # Against bias
    scores['bias_alignment'] = bias_score
    
    # Calculate weighted probability
    weighted_probability = sum(scores[key] * weights[key] for key in weights.keys())
    
    # Apply confidence adjustment based on data availability
    confidence_factor = calculate_confidence_factor(exact_total, asset_total, day_total, session_total)
    adjusted_probability = (weighted_probability * confidence_factor) + (50 * (1 - confidence_factor))
    
    return round(adjusted_probability, 2)


def calculate_confidence_factor(exact_total, asset_total, day_total, session_total):
    """
    Calculate confidence based on the amount of historical data available
    Returns a value between 0.5 and 1.0
    """
    # Minimum trades needed for high confidence
    thresholds = {
        'exact': 10,
        'asset': 20,
        'day': 30,
        'session': 30
    }
    
    # Calculate individual confidence scores
    exact_confidence = min(exact_total / thresholds['exact'], 1.0)
    asset_confidence = min(asset_total / thresholds['asset'], 1.0)
    day_confidence = min(day_total / thresholds['day'], 1.0)
    session_confidence = min(session_total / thresholds['session'], 1.0)
    
    # Weighted average of confidence scores
    overall_confidence = (
        exact_confidence * 0.4 +
        asset_confidence * 0.3 +
        day_confidence * 0.15 +
        session_confidence * 0.15
    )
    
    # Scale to 0.5-1.0 range (never go below 50% confidence)
    return 0.5 + (overall_confidence * 0.5)


def get_trade_statistics(account, day_of_week, trading_session, asset, order_type):
    """
    Get detailed statistics for the UI display
    """
    # Similar trades (exact match)
    similar_trades = AccountTrades.objects.filter(
        account=account,
        day_of_week_entered=day_of_week,
        trading_session_entered=trading_session,
        asset=asset,
        order_type=order_type
    )
    
    total_similar = similar_trades.count()
    # Changed: Look for "Win" and "Loss" instead of "Profit" and "Loss"
    wins_similar = similar_trades.filter(outcome='Win').count()
    losses_similar = similar_trades.filter(outcome='Loss').count()
    
    # Overall account stats
    all_trades = AccountTrades.objects.filter(account=account)
    total_all = all_trades.count()
    wins_all = all_trades.filter(outcome='Win').count()
    losses_all = all_trades.filter(outcome='Loss').count()
    
    return {
        'similar_trades': total_similar,
        'total_trades': total_all,
        'wins': wins_similar if total_similar > 0 else wins_all,
        'losses': losses_similar if total_similar > 0 else losses_all,
    }


def generate_factors_description(stats, asset_bias):
    """
    Generate a human-readable description of the factors
    """
    factors = []
    
    if stats['similar_trades'] > 0:
        win_rate = (stats['wins'] / stats['similar_trades'] * 100) if stats['similar_trades'] > 0 else 0
        factors.append(f"Found {stats['similar_trades']} similar trade(s) with {win_rate:.1f}% win rate")
    else:
        factors.append("No exact historical matches found - using broader patterns")
    
    if asset_bias:
        factors.append(f"Current market bias for {asset_bias}")
    
    if stats['total_trades'] < 20:
        factors.append("Limited historical data - probability may be less accurate")
    
    return "  ".join(factors)


@csrf_exempt
def obliterate_latest_backtest_results(request, count=1):
    """
    Deletes the latest `count` BacktestResult entries.
    Accessible via GET in the URL: /obliterate-latest/<count>/
    """
    try:
        count = int(count)
        if count <= 0:
            return JsonResponse({"error": "Count must be a positive integer."}, status=400)

        # Get the latest N entries
        latest_entries = BacktestResult.objects.order_by("-created_at")[:count]

        # Collect their IDs
        ids_to_delete = [entry.id for entry in latest_entries]

        # Delete them in bulk
        deleted_count, _ = BacktestResult.objects.filter(id__in=ids_to_delete).delete()

        return JsonResponse({
            "status": "success",
            "deleted_count": deleted_count,
            "message": f"Successfully obliterated {deleted_count} latest BacktestResult entries."
        })

    except Exception as e:
        return JsonResponse({"error": str(e)}, status=500)


from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.db.models import Sum, Avg, Count, Q
from .models import Account, AccountTrades
import json
from datetime import datetime


@csrf_exempt
@require_http_methods(["GET"])
def fetch_multi_account_performance_overview_data(request):
    """
    Fetches performance overview for all accounts including best, worst, and average performers
    """
    try:
        accounts = Account.objects.all()
        
        account_performance_list = []
        
        for account in accounts:
            trades = AccountTrades.objects.filter(account=account)
            
            # Calculate metrics
            total_trades = trades.count()
            winning_trades = trades.filter(outcome='Win').count()
            losing_trades = trades.filter(outcome='Loss').count()
            
            # Calculate net P&L: wins are positive, losses need to be negative
            total_wins = trades.filter(outcome='Win').aggregate(Sum('amount'))['amount__sum'] or 0
            total_losses = trades.filter(outcome='Loss').aggregate(Sum('amount'))['amount__sum'] or 0
            net_pnl = total_wins - total_losses  # Subtract losses since they're stored as positive
            
            win_rate = (winning_trades / total_trades * 100) if total_trades > 0 else 0
            
            # Calculate ROI: total return / initial capital * 100
            roi = (net_pnl / account.initial_capital * 100) if account.initial_capital > 0 else 0
            
            account_performance_list.append({
                'account_id': account.id,
                'account_name': account.account_name,
                'main_assets': account.main_assets,
                'initial_capital': account.initial_capital,
                'total_trades': total_trades,
                'winning_trades': winning_trades,
                'losing_trades': losing_trades,
                'net_pnl': round(net_pnl, 2),
                'win_rate': round(win_rate, 2),
                'roi': round(roi, 2),
                'current_balance': round(account.initial_capital + net_pnl, 2)
            })
        
        # Sort by ROI to find best and worst
        sorted_by_roi = sorted(account_performance_list, key=lambda x: x['roi'], reverse=True)
        
        best_performer = sorted_by_roi[0] if sorted_by_roi else None
        worst_performer = sorted_by_roi[-1] if sorted_by_roi else None
        
        # Calculate averages
        avg_roi = sum(acc['roi'] for acc in account_performance_list) / len(account_performance_list) if account_performance_list else 0
        avg_win_rate = sum(acc['win_rate'] for acc in account_performance_list) / len(account_performance_list) if account_performance_list else 0
        avg_net_pnl = sum(acc['net_pnl'] for acc in account_performance_list) / len(account_performance_list) if account_performance_list else 0
        
        return JsonResponse({
            'success': True,
            'all_accounts': account_performance_list,
            'best_performer': best_performer,
            'worst_performer': worst_performer,
            'averages': {
                'avg_roi': round(avg_roi, 2),
                'avg_win_rate': round(avg_win_rate, 2),
                'avg_net_pnl': round(avg_net_pnl, 2)
            },
            'total_accounts': len(account_performance_list)
        })
        
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def fetch_account_equity_curve_progression_data(request, account_id):
    """
    Fetches equity curve data for a specific account
    """
    try:
        account = Account.objects.get(id=account_id)
        trades = AccountTrades.objects.filter(account=account).order_by('date_entered')
        
        equity_curve_points = []
        running_balance = account.initial_capital
        
        equity_curve_points.append({
            'date': None,
            'balance': running_balance,
            'trade_number': 0,
            'label': 'Initial Capital'
        })
        
        for idx, trade in enumerate(trades, 1):
            # If it's a loss, subtract the amount (since losses are stored as positive)
            if trade.outcome == 'Loss':
                trade_pnl = -abs(trade.amount)  # Force negative
            else:
                trade_pnl = abs(trade.amount)  # Force positive
            
            running_balance += trade_pnl
            equity_curve_points.append({
                'date': trade.date_entered.isoformat() if trade.date_entered else None,
                'balance': round(running_balance, 2),
                'trade_number': idx,
                'trade_amount': round(trade_pnl, 2),
                'asset': trade.asset,
                'outcome': trade.outcome
            })
        
        return JsonResponse({
            'success': True,
            'account_name': account.account_name,
            'initial_capital': account.initial_capital,
            'current_balance': running_balance,
            'equity_curve': equity_curve_points
        })
        
    except Account.DoesNotExist:
        return JsonResponse({
            'success': False,
            'error': 'Account not found'
        }, status=404)
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def fetch_all_accounts_equity_curves_comparison_data(request):
    """
    Fetches equity curve data for all accounts for comparison
    """
    try:
        accounts = Account.objects.all()
        
        all_curves_data = []
        
        for account in accounts:
            trades = AccountTrades.objects.filter(account=account).order_by('date_entered')
            
            equity_points = []
            running_balance = account.initial_capital
            
            for idx, trade in enumerate(trades, 1):
                # If it's a loss, subtract the amount (since losses are stored as positive)
                if trade.outcome == 'Loss':
                    trade_pnl = -abs(trade.amount)  # Force negative
                else:
                    trade_pnl = abs(trade.amount)  # Force positive
                    
                running_balance += trade_pnl
                equity_points.append({
                    'trade_number': idx,
                    'balance': round(running_balance, 2),
                    'date': trade.date_entered.isoformat() if trade.date_entered else None
                })
            
            all_curves_data.append({
                'account_id': account.id,
                'account_name': account.account_name,
                'initial_capital': account.initial_capital,
                'final_balance': running_balance,
                'equity_curve': equity_points
            })
        
        return JsonResponse({
            'success': True,
            'accounts_equity_data': all_curves_data
        })
        
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def execute_portfolio_monte_carlo_risk_simulation(request):
    """
    Runs a Monte Carlo simulation for an account based on its historical trade performance
    """
    try:
        data = json.loads(request.body)
        account_id = data.get('account_id')
        num_simulations = data.get('num_simulations', 1000)
        num_trades = data.get('num_trades', 100)
        
        account = Account.objects.get(id=account_id)
        trades = AccountTrades.objects.filter(account=account).order_by('date_entered')
        
        if trades.count() == 0:
            return JsonResponse({
                'success': False,
                'error': 'No trades available for this account'
            }, status=400)
        
        # Calculate average trading frequency (trades per day)
        trades_with_dates = trades.exclude(date_entered__isnull=True)
        if trades_with_dates.count() > 1:
            first_trade = trades_with_dates.first().date_entered
            last_trade = trades_with_dates.last().date_entered
            days_trading = (last_trade - first_trade).days
            
            if days_trading > 0:
                avg_trades_per_day = trades_with_dates.count() / days_trading
            else:
                avg_trades_per_day = 1  # Default if all trades on same day
        else:
            avg_trades_per_day = 1  # Default
        
        # Calculate estimated time horizons
        estimated_days = num_trades / avg_trades_per_day if avg_trades_per_day > 0 else num_trades
        estimated_weeks = estimated_days / 7
        estimated_months = estimated_days / 30
        
        # Extract all trade returns as percentages
        trade_returns = []
        for trade in trades:
            if trade.outcome == 'Loss':
                trade_pnl = -abs(trade.amount)
            else:
                trade_pnl = abs(trade.amount)
            
            # Calculate return as percentage of initial capital
            trade_return_pct = (trade_pnl / account.initial_capital) * 100
            trade_returns.append(trade_return_pct)
        
        # Run Monte Carlo simulations
        import random
        import statistics
        
        simulation_results = []
        final_balances = []
        
        for sim in range(num_simulations):
            balance = account.initial_capital
            equity_curve = [balance]
            
            for _ in range(num_trades):
                # Randomly sample a trade return from historical data
                random_return_pct = random.choice(trade_returns)
                trade_amount = (random_return_pct / 100) * balance
                balance += trade_amount
                equity_curve.append(balance)
            
            final_balances.append(balance)
            
            # Store only every 50th simulation to reduce data size
            if sim % 50 == 0:
                simulation_results.append({
                    'simulation_number': sim,
                    'final_balance': round(balance, 2),
                    'equity_curve': [round(b, 2) for b in equity_curve]
                })
        
        # Calculate statistics
        final_balances.sort()
        
        percentile_5 = final_balances[int(len(final_balances) * 0.05)]
        percentile_25 = final_balances[int(len(final_balances) * 0.25)]
        percentile_50 = statistics.median(final_balances)
        percentile_75 = final_balances[int(len(final_balances) * 0.75)]
        percentile_95 = final_balances[int(len(final_balances) * 0.95)]
        
        mean_balance = statistics.mean(final_balances)
        std_dev = statistics.stdev(final_balances)
        
        # Calculate probability of profit
        profitable_sims = sum(1 for b in final_balances if b > account.initial_capital)
        prob_profit = (profitable_sims / num_simulations) * 100
        
        # Calculate worst drawdown risk
        max_loss = account.initial_capital - min(final_balances)
        max_loss_pct = (max_loss / account.initial_capital) * 100
        
        return JsonResponse({
            'success': True,
            'account_name': account.account_name,
            'initial_capital': account.initial_capital,
            'num_simulations': num_simulations,
            'num_trades_simulated': num_trades,
            'time_horizon': {
                'avg_trades_per_day': round(avg_trades_per_day, 2),
                'estimated_days': round(estimated_days, 1),
                'estimated_weeks': round(estimated_weeks, 1),
                'estimated_months': round(estimated_months, 1)
            },
            'statistics': {
                'mean_final_balance': round(mean_balance, 2),
                'median_final_balance': round(percentile_50, 2),
                'std_deviation': round(std_dev, 2),
                'percentile_5': round(percentile_5, 2),
                'percentile_25': round(percentile_25, 2),
                'percentile_75': round(percentile_75, 2),
                'percentile_95': round(percentile_95, 2),
                'probability_of_profit': round(prob_profit, 2),
                'max_potential_loss': round(max_loss, 2),
                'max_loss_percentage': round(max_loss_pct, 2)
            },
            'sample_simulations': simulation_results
        })
        
    except Account.DoesNotExist:
        return JsonResponse({
            'success': False,
            'error': 'Account not found'
        }, status=404)
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=500)
        

from django.core.exceptions import ObjectDoesNotExist
@csrf_exempt
@require_http_methods(["GET"])
def fetch_snowai_video_categories(request):
    """Fetch all video categories"""
    try:
        categories = SnowAIVideoCategory.objects.all()
        categories_data = [
            {
                'id': cat.id,
                'category_name': cat.category_name,
                'created_at': cat.created_at.isoformat()
            }
            for cat in categories
        ]
        return JsonResponse({'categories': categories_data}, status=200)
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def create_snowai_video_category(request):
    """Create a new video category"""
    try:
        data = json.loads(request.body)
        category_name = data.get('category_name', '').strip()
        
        if not category_name:
            return JsonResponse({'error': 'Category name is required'}, status=400)
        
        category, created = SnowAIVideoCategory.objects.get_or_create(
            category_name=category_name
        )
        
        return JsonResponse({
            'id': category.id,
            'category_name': category.category_name,
            'created_at': category.created_at.isoformat(),
            'created': created
        }, status=201 if created else 200)
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def fetch_snowai_video_entries(request):
    """Fetch all video entries with optional category filter"""
    try:
        category_id = request.GET.get('category_id')
        
        if category_id:
            videos = SnowAIVideoEntry.objects.filter(category_id=category_id)
        else:
            videos = SnowAIVideoEntry.objects.all()
        
        videos_data = [
            {
                'id': video.id,
                'video_title': video.video_title,
                'video_url': video.video_url,
                'category_id': video.category.id,
                'category_name': video.category.category_name,
                'date_entered': video.date_entered.isoformat(),
                'notes': video.notes,
                'youtube_embed_id': video.get_youtube_embed_id()
            }
            for video in videos
        ]
        
        return JsonResponse({'videos': videos_data}, status=200)
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def create_snowai_video_entry(request):
    """Create a new video entry"""
    try:
        data = json.loads(request.body)
        video_title = data.get('video_title', '').strip()
        video_url = data.get('video_url', '').strip()
        category_id = data.get('category_id')
        notes = data.get('notes', '').strip()
        
        if not video_title or not video_url or not category_id:
            return JsonResponse({
                'error': 'Video title, URL, and category are required'
            }, status=400)
        
        try:
            category = SnowAIVideoCategory.objects.get(id=category_id)
        except ObjectDoesNotExist:
            return JsonResponse({'error': 'Category not found'}, status=404)
        
        video = SnowAIVideoEntry.objects.create(
            video_title=video_title,
            video_url=video_url,
            category=category,
            notes=notes if notes else None
        )
        
        return JsonResponse({
            'id': video.id,
            'video_title': video.video_title,
            'video_url': video.video_url,
            'category_id': video.category.id,
            'category_name': video.category.category_name,
            'date_entered': video.date_entered.isoformat(),
            'notes': video.notes,
            'youtube_embed_id': video.get_youtube_embed_id()
        }, status=201)
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["PUT"])
def update_snowai_video_entry(request, video_id):
    """Update an existing video entry"""
    try:
        data = json.loads(request.body)
        
        try:
            video = SnowAIVideoEntry.objects.get(id=video_id)
        except ObjectDoesNotExist:
            return JsonResponse({'error': 'Video not found'}, status=404)
        
        video.video_title = data.get('video_title', video.video_title).strip()
        video.video_url = data.get('video_url', video.video_url).strip()
        video.notes = data.get('notes', video.notes)
        
        if 'category_id' in data:
            try:
                category = SnowAIVideoCategory.objects.get(id=data['category_id'])
                video.category = category
            except ObjectDoesNotExist:
                return JsonResponse({'error': 'Category not found'}, status=404)
        
        video.save()
        
        return JsonResponse({
            'id': video.id,
            'video_title': video.video_title,
            'video_url': video.video_url,
            'category_id': video.category.id,
            'category_name': video.category.category_name,
            'date_entered': video.date_entered.isoformat(),
            'notes': video.notes,
            'youtube_embed_id': video.get_youtube_embed_id()
        }, status=200)
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["DELETE"])
def delete_snowai_video_entry(request, video_id):
    """Delete a video entry"""
    try:
        try:
            video = SnowAIVideoEntry.objects.get(id=video_id)
        except ObjectDoesNotExist:
            return JsonResponse({'error': 'Video not found'}, status=404)
        
        video.delete()
        return JsonResponse({'message': 'Video deleted successfully'}, status=200)
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)



import math
def clean_value(value):
    """Convert NaN and None to None for JSON serialization"""
    if value is None:
        return None
    try:
        if math.isnan(value) or math.isinf(value):
            return None
    except (TypeError, ValueError):
        pass
    return value

@csrf_exempt
@require_http_methods(["GET"])
def snowai_stock_screener_fetch_data(request):
    """
    Unique endpoint for SnowAI Stock Screener to fetch comprehensive stock data
    No authentication or CSRF required - personal use only
    """
    ticker_symbol = request.GET.get('ticker', '').upper()
    
    if not ticker_symbol:
        return JsonResponse({'error': 'Ticker symbol is required'}, status=400)
    
    try:
        ticker = yf.Ticker(ticker_symbol)
        info = ticker.info
        
        # Get stock overview data with cleaned values
        stock_info = {
            'symbol': ticker_symbol,
            'longName': info.get('longName'),
            'sector': info.get('sector'),
            'industry': info.get('industry'),
            'currentPrice': clean_value(info.get('currentPrice')),
            'marketCap': clean_value(info.get('marketCap')),
            'trailingPE': clean_value(info.get('trailingPE')),
            'fiftyTwoWeekHigh': clean_value(info.get('fiftyTwoWeekHigh')),
            'fiftyTwoWeekLow': clean_value(info.get('fiftyTwoWeekLow')),
            'dividendYield': clean_value(info.get('dividendYield')),
            'longBusinessSummary': info.get('longBusinessSummary'),
        }
        
        # Get financial statements using new API
        financials_df = ticker.financials
        financials_data = None
        
        if financials_df is not None and not financials_df.empty:
            # Get top financial metrics
            metrics = ['Total Revenue', 'Gross Profit', 'Operating Income', 'Net Income']
            financial_rows = []
            
            for metric in metrics:
                if metric in financials_df.index:
                    # Clean all values in the row
                    values = [clean_value(v) for v in financials_df.loc[metric].tolist()]
                    row_data = {
                        'metric': metric,
                        'values': values
                    }
                    financial_rows.append(row_data)
            
            financials_data = {
                'columns': [col.strftime('%Y') for col in financials_df.columns],
                'data': financial_rows
            }
        
        # Get quarterly earnings using new API - use quarterly_income_stmt
        earnings_data = []
        try:
            quarterly_income = ticker.quarterly_income_stmt
            
            if quarterly_income is not None and not quarterly_income.empty:
                # Get Total Revenue and Net Income from quarterly statements
                for col in quarterly_income.columns[:8]:  # Last 8 quarters
                    revenue = None
                    earnings = None
                    
                    if 'Total Revenue' in quarterly_income.index:
                        revenue = quarterly_income.loc['Total Revenue', col]
                        revenue = clean_value(revenue)
                    
                    if 'Net Income' in quarterly_income.index:
                        earnings = quarterly_income.loc['Net Income', col]
                        earnings = clean_value(earnings)
                    
                    earnings_data.append({
                        'quarter': col.strftime('%Y-%m-%d') if hasattr(col, 'strftime') else str(col),
                        'revenue': revenue,
                        'earnings': earnings
                    })
        except Exception as e:
            print(f"Error fetching earnings: {e}")
        
        # Get news
        news_data = []
        try:
            news = ticker.news
            for article in news[:10]:
                news_data.append({
                    'title': article.get('title'),
                    'publisher': article.get('publisher'),
                    'link': article.get('link'),
                    'providerPublishTime': datetime.fromtimestamp(
                        article.get('providerPublishTime', 0)
                    ).strftime('%Y-%m-%d %H:%M') if article.get('providerPublishTime') else 'N/A'
                })
        except:
            pass
        
        return JsonResponse({
            'stock_info': stock_info,
            'financials': financials_data,
            'earnings': earnings_data,
            'news': news_data
        })
        
    except Exception as e:
        return JsonResponse({'error': f'Failed to fetch data: {str(e)}'}, status=500)


import json
import logging
import time
from datetime import timedelta
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils import timezone
from django.db.models import Q

logger = logging.getLogger(__name__)


# Helper function to determine asset type and relevant currencies
def get_economic_currencies_for_asset(asset):
    """
    Determine which currencies' economic data should be used for an asset
    """
    # US Stocks - use USD economic data
    us_stocks = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'TSLA', 'META', 'JPMC',
                 'BRK.B', 'JNJ', 'V', 'WMT', 'PG', 'DIS', 'NFLX', 'ADBE']
    
    # US Indices - use USD economic data
    us_indices = ['SPY', 'QQQ', 'IWM', 'DIA', 'VTI']
    
    # Commodities mapping
    commodity_currencies = {
        'XAUUSD': ['USD'],  # Gold - USD data
        'XAGUUSD': ['USD'],  # Silver - USD data
        'XPTUSD': ['USD'],  # Platinum - USD data
        'XPDUSD': ['USD'],  # Palladium - USD data
        'CRUDEOIL': ['USD'],  # Oil - USD data
        'NATGAS': ['USD'],  # Natural Gas - USD data
        'COPPER': ['USD'],  # Copper - USD data
        'WHEAT': ['USD'],  # Wheat - USD data
        'CORN': ['USD'],  # Corn - USD data
        'SOYBEANS': ['USD'],  # Soybeans - USD data
    }
    
    # Emerging Markets & China - use multiple currencies
    emerging_markets = {
        'EEM': ['USD'],  # Emerging markets ETF - USD data
        'FXI': ['CNY', 'USD'],  # China ETF - CNY and USD data
    }
    
    # European Indices - EUR data
    european_indices = {
        'FTSE': ['GBP', 'EUR'],  # FTSE 100 - GBP and EUR
        'DAX': ['EUR', 'USD'],  # DAX - EUR and USD
        'CAC40': ['EUR', 'USD'],  # CAC 40 - EUR and USD
    }
    
    # Asian Indices
    asian_indices = {
        'NIKKEI': ['JPY', 'USD'],  # Nikkei - JPY and USD
        'HSI': ['HKD', 'CNY'],  # Hang Seng - HKD and CNY
    }
    
    # Check if it's a Forex pair
    if len(asset) == 6 and asset.isupper() and asset not in us_stocks and asset not in commodity_currencies:
        base = asset[:3]
        quote = asset[3:]
        return [base, quote]
    
    # Check US stocks
    if asset in us_stocks:
        return ['USD']
    
    # Check US indices
    if asset in us_indices:
        return ['USD']
    
    # Check commodities
    if asset in commodity_currencies:
        return commodity_currencies[asset]
    
    # Check emerging markets
    if asset in emerging_markets:
        return emerging_markets[asset]
    
    # Check European indices
    if asset in european_indices:
        return european_indices[asset]
    
    # Check Asian indices
    if asset in asian_indices:
        return asian_indices[asset]
    
    # Default to USD for unknown assets
    return ['USD']


@csrf_exempt
@require_http_methods(["GET"])
def fetch_watched_trading_assets_view(request):
    """Fetch all watched trading assets"""
    try:
        watched_assets = WatchedTradingAsset.objects.filter(is_active=True).order_by('asset')
        assets_data = [
            {
                'id': asset.id,
                'asset': asset.asset,
                'created_at': asset.created_at.isoformat()
            }
            for asset in watched_assets
        ]
        
        return JsonResponse({
            'success': True,
            'watched_assets': assets_data
        })
    except Exception as e:
        logger.error(f"Error fetching watched assets: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["POST"])
def add_trading_asset_to_watch_view(request):
    """Add a trading asset to the watch list"""
    try:
        data = json.loads(request.body)
        asset = data.get('asset')
        
        if not asset:
            return JsonResponse({'success': False, 'error': 'Asset is required'}, status=400)
        
        # Check if asset is already being watched
        if WatchedTradingAsset.objects.filter(asset=asset, is_active=True).exists():
            return JsonResponse({'success': False, 'error': 'Asset is already being watched'}, status=400)
        
        # Create or reactivate the watched asset
        watched_asset, created = WatchedTradingAsset.objects.get_or_create(
            asset=asset,
            defaults={'is_active': True}
        )
        
        if not created:
            watched_asset.is_active = True
            watched_asset.save()
        
        return JsonResponse({
            'success': True,
            'message': f'{asset} added to watch list',
            'asset_id': watched_asset.id
        })
    except Exception as e:
        logger.error(f"Error adding asset to watch: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["DELETE"])
def remove_watched_trading_asset_view(request):
    """Remove a trading asset from the watch list"""
    try:
        data = json.loads(request.body)
        asset_id = data.get('asset_id')
        
        if not asset_id:
            return JsonResponse({'success': False, 'error': 'Asset ID is required'}, status=400)
        
        watched_asset = WatchedTradingAsset.objects.get(id=asset_id)
        watched_asset.is_active = False
        watched_asset.save()
        
        return JsonResponse({
            'success': True,
            'message': 'Asset removed from watch list'
        })
    except WatchedTradingAsset.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Asset not found'}, status=404)
    except Exception as e:
        logger.error(f"Error removing watched asset: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def fetch_trader_gpt_analyses_view(request):
    """Fetch the latest TraderGPT analyses for all watched assets"""
    try:
        analyses = []
        watched_assets = WatchedTradingAsset.objects.filter(is_active=True)
        
        for asset in watched_assets:
            latest_analysis = TraderGPTAnalysisRecord.objects.filter(
                asset=asset.asset
            ).order_by('-analysis_timestamp').first()
            
            if latest_analysis:
                analyses.append({
                    'id': latest_analysis.id,
                    'asset': latest_analysis.asset,
                    'market_sentiment': latest_analysis.market_sentiment,
                    'confidence_score': latest_analysis.confidence_score,
                    'risk_level': latest_analysis.risk_level,
                    'time_horizon': latest_analysis.time_horizon,
                    'entry_strategy': latest_analysis.entry_strategy,
                    'key_factors': latest_analysis.key_factors,
                    'stop_loss_level': latest_analysis.stop_loss_level,
                    'take_profit_level': latest_analysis.take_profit_level,
                    'support_level': latest_analysis.support_level,
                    'resistance_level': latest_analysis.resistance_level,
                    'analysis_timestamp': latest_analysis.analysis_timestamp.isoformat(),
                    'updated_at': latest_analysis.updated_at.isoformat(),
                })
        
        return JsonResponse({
            'success': True,
            'analyses': analyses
        })
    except Exception as e:
        logger.error(f"Error fetching analyses: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


def validate_choice_field(value, valid_choices, default):
    """Validate that a value matches one of the valid choices"""
    if value and str(value).lower() in [choice[0] for choice in valid_choices]:
        return str(value).lower()
    return default


@csrf_exempt
@require_http_methods(["POST"])
def run_fresh_trader_analysis_view(request):
    """Run a fresh analysis for a specific asset"""
    try:
        data = json.loads(request.body)
        asset = data.get('asset')
        
        if not asset:
            return JsonResponse({'success': False, 'error': 'Asset is required'}, status=400)
        
        # Check if asset is being watched
        if not WatchedTradingAsset.objects.filter(asset=asset, is_active=True).exists():
            return JsonResponse({'success': False, 'error': 'Asset is not in watch list'}, status=400)
        
        # Create execution log
        execution_log = AnalysisExecutionLog.objects.create(
            asset=asset,
            status='running'
        )
        
        start_time = time.time()
        
        try:
            # Run the analysis
            analysis_result = execute_trader_gpt_analysis_for_asset(asset)
            
            if analysis_result['success']:
                execution_log.status = 'completed'
                execution_log.completed_at = timezone.now()
                execution_log.execution_time_seconds = time.time() - start_time
                execution_log.save()
                
                return JsonResponse({
                    'success': True,
                    'message': f'Fresh analysis completed for {asset}',
                    'analysis_id': analysis_result['analysis_id']
                })
            else:
                execution_log.status = 'failed'
                execution_log.error_message = analysis_result.get('error', 'Unknown error')
                execution_log.execution_time_seconds = time.time() - start_time
                execution_log.save()
                
                return JsonResponse({
                    'success': False,
                    'error': analysis_result.get('error', 'Analysis failed')
                }, status=500)
                
        except Exception as analysis_error:
            execution_log.status = 'failed'
            execution_log.error_message = str(analysis_error)
            execution_log.execution_time_seconds = time.time() - start_time
            execution_log.save()
            raise
            
    except Exception as e:
        logger.error(f"Error running fresh analysis: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)


def get_economic_events_for_currencies(currencies):
    """Get recent economic events for specified currencies"""
    try:
        thirty_days_ago = timezone.now() - timezone.timedelta(days=30)
        events = EconomicEvent.objects.filter(
            currency__in=currencies,
            date_time__gte=thirty_days_ago
        ).order_by('-date_time')
        
        return events
    
    except Exception as e:
        logger.error(f"Error retrieving economic events: {str(e)}")
        return EconomicEvent.objects.none()


def execute_trader_gpt_analysis_for_asset(asset):
    """Execute TraderGPT analysis for a specific asset"""
    try:
        # Determine relevant currencies for this asset
        relevant_currencies = get_economic_currencies_for_asset(asset)
        
        # Fetch news and economic data
        user_email = "system@tradergpt.com"
        news_and_events_data = fetch_news_data([asset], user_email)
        
        # Get recent economic events for the relevant currencies
        recent_events_objects = get_economic_events_for_currencies(relevant_currencies)
        recent_events_text = format_economic_events_for_prompt(recent_events_objects)
        
        # Build asset type context
        asset_type = determine_asset_type(asset)
        
        # Prepare the prompt for GPT
        prompt = f"""
        Analyze the {asset} ({asset_type}) and provide a comprehensive trading analysis.
        
        Asset Type: {asset_type}
        Relevant Economic Data Currencies: {', '.join(relevant_currencies)}
        
        Recent News Data:
        {json.dumps(news_and_events_data.get('message', [])[:5], indent=2)}
        
        Recent Economic Events:
        {recent_events_text}
        
        Please provide your analysis in the following JSON format with strict character limits:
        {{
            "market_sentiment": "bullish|bearish|neutral (must be exactly one of these three words)",
            "confidence_score": 85,
            "risk_level": "low|medium|high (must be exactly one of these three words)",
            "time_horizon": "short|medium|long (must be exactly one of these three words)",
            "entry_strategy": "Detailed entry strategy with specific levels (max 1000 chars)",
            "key_factors": "Key factors influencing this analysis (max 1000 chars)",
            "stop_loss_level": "Recommended stop loss level (max 200 chars)",
            "take_profit_level": "Recommended take profit level (max 200 chars)",
            "support_level": "Current support level (max 200 chars)",
            "resistance_level": "Current resistance level (max 200 chars)",
            "summary": "Brief overall summary of the analysis (max 500 chars)"
        }}
        
        IMPORTANT: 
        - Use ONLY the exact words for sentiment (bullish/bearish/neutral), risk_level (low/medium/high), and time_horizon (short/medium/long)
        - Keep all responses within the specified character limits
        - Use concise, specific language with actual price levels where possible
        - Base your analysis on current market conditions, news sentiment, economic events, and technical factors
        - For stocks: Consider earnings, company fundamentals, sector trends, and US economic indicators
        - For indices: Consider broad market trends, monetary policy, and GDP data
        - For commodities: Consider supply/demand dynamics, USD strength, and geopolitical factors
        """
        
        # Call GPT
        gpt_response = chat_gpt(prompt)
        
        # Try to parse JSON from the response
        try:
            start_idx = gpt_response.find('{')
            end_idx = gpt_response.rfind('}') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = gpt_response[start_idx:end_idx]
                analysis_data = json.loads(json_str)
            else:
                analysis_data = json.loads(gpt_response)
                
        except json.JSONDecodeError as json_error:
            logger.error(f"JSON parsing error for {asset}: {str(json_error)}")
            logger.error(f"GPT Response: {gpt_response}")
            
            analysis_data = {
                "market_sentiment": "neutral",
                "confidence_score": 50,
                "risk_level": "medium",
                "time_horizon": "medium",
                "entry_strategy": "Wait for clearer market signals before entering position",
                "key_factors": "Analysis could not be parsed properly from GPT response",
                "stop_loss_level": "TBD - Analysis parsing failed",
                "take_profit_level": "TBD - Analysis parsing failed",
                "support_level": "TBD - Analysis parsing failed",
                "resistance_level": "TBD - Analysis parsing failed",
                "summary": "Analysis parsing failed, manual review required"
            }
        
        # Validate choice fields
        sentiment = validate_choice_field(
            analysis_data.get('market_sentiment'),
            TraderGPTAnalysisRecord.SENTIMENT_CHOICES,
            'neutral'
        )
        
        risk_level = validate_choice_field(
            analysis_data.get('risk_level'),
            TraderGPTAnalysisRecord.RISK_CHOICES,
            'medium'
        )
        
        time_horizon = validate_choice_field(
            analysis_data.get('time_horizon'),
            TraderGPTAnalysisRecord.TIME_HORIZON_CHOICES,
            'medium'
        )
        
        # Ensure confidence score is valid
        confidence_score = analysis_data.get('confidence_score', 50)
        if isinstance(confidence_score, str):
            try:
                confidence_score = int(confidence_score)
            except ValueError:
                confidence_score = 50
        confidence_score = min(100, max(1, confidence_score))
        
        # Create analysis record
        analysis_record = TraderGPTAnalysisRecord.objects.create(
            asset=asset,
            market_sentiment=sentiment,
            confidence_score=confidence_score,
            risk_level=risk_level,
            time_horizon=time_horizon,
            entry_strategy=str(analysis_data.get('entry_strategy', ''))[:1000],
            key_factors=str(analysis_data.get('key_factors', ''))[:1000],
            stop_loss_level=str(analysis_data.get('stop_loss_level', ''))[:200],
            take_profit_level=str(analysis_data.get('take_profit_level', ''))[:200],
            support_level=str(analysis_data.get('support_level', ''))[:200],
            resistance_level=str(analysis_data.get('resistance_level', ''))[:200],
            raw_analysis=gpt_response,
            news_data_used=news_and_events_data.get('message', []),
            economic_events_used=[{
                'date': event.date_time.isoformat(),
                'currency': event.currency,
                'event': event.event_name,
                'impact': event.impact,
                'actual': event.actual,
                'forecast': event.forecast,
                'previous': event.previous
            } for event in recent_events_objects[:10]],
            analysis_timestamp=timezone.now()
        )
        
        logger.info(f"Successfully created analysis record {analysis_record.id} for {asset}")
        
        return {
            'success': True,
            'analysis_id': analysis_record.id,
            'message': f'Analysis completed for {asset}'
        }
        
    except Exception as e:
        logger.error(f"Error in execute_trader_gpt_analysis_for_asset for {asset}: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }


def determine_asset_type(asset):
    """Determine the type of asset (Forex, Stock, Index, or Commodity)"""
    us_stocks = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'TSLA', 'META', 'JPMC',
                 'BRK.B', 'JNJ', 'V', 'WMT', 'PG', 'DIS', 'NFLX', 'ADBE']
    indices = ['SPY', 'QQQ', 'IWM', 'DIA', 'VTI', 'EEM', 'FXI', 'FTSE', 'DAX', 'CAC40', 'NIKKEI', 'HSI']
    commodities = ['XAUUSD', 'XAGUUSD', 'XPTUSD', 'XPDUSD', 'CRUDEOIL', 'NATGAS',
                   'COPPER', 'WHEAT', 'CORN', 'SOYBEANS']
    
    if asset in us_stocks:
        return "US Stock"
    elif asset in indices:
        return "Index"
    elif asset in commodities:
        return "Commodity"
    elif len(asset) == 6 and asset.isupper():
        return "Forex Pair"
    else:
        return "Asset"


def format_economic_events_for_prompt(events):
    """Format economic events into a readable string for the GPT prompt"""
    if not events:
        return "No recent economic events found."
    
    formatted = []
    for event in events[:10]:
        formatted.append(
            f"- {event.date_time.strftime('%Y-%m-%d %H:%M')} - {event.currency}: "
            f"{event.event_name} (Impact: {event.impact}) "
            f"Actual: {event.actual}, Forecast: {event.forecast}, Previous: {event.previous}"
        )
    
    return "\n".join(formatted) if formatted else "No recent economic events found."


def run_scheduled_trader_gpt_analyses():
    """
    Scheduled function to run TraderGPT analyses for all watched assets.
    """
    try:
        watched_assets = WatchedTradingAsset.objects.filter(is_active=True)
        
        if not watched_assets.exists():
            logger.info("No watched assets found for analysis")
            return
        
        logger.info(f"Starting scheduled analysis for {watched_assets.count()} assets")
        
        results = {
            'successful': 0,
            'failed': 0,
            'errors': []
        }
        
        for asset in watched_assets:
            try:
                # Check if we already have a recent analysis (within last 4 hours)
                recent_analysis = TraderGPTAnalysisRecord.objects.filter(
                    asset=asset.asset,
                    analysis_timestamp__gte=timezone.now() - timedelta(hours=4)
                ).exists()
                
                if not recent_analysis:
                    logger.info(f"Running analysis for {asset.asset}")
                    result = execute_trader_gpt_analysis_for_asset(asset.asset)
                    
                    if result['success']:
                        results['successful'] += 1
                        logger.info(f"Analysis completed for {asset.asset}")
                    else:
                        results['failed'] += 1
                        results['errors'].append(f"{asset.asset}: {result.get('error', 'Unknown error')}")
                        logger.error(f"Analysis failed for {asset.asset}: {result.get('error')}")
                else:
                    logger.info(f"Skipping {asset.asset} - recent analysis exists")
                    
                time.sleep(2)
                
            except Exception as e:
                results['failed'] += 1
                results['errors'].append(f"{asset.asset}: {str(e)}")
                logger.error(f"Error analyzing {asset.asset}: {str(e)}")
        
        logger.info(f"Scheduled analysis completed. Successful: {results['successful']}, Failed: {results['failed']}")
        return results
        
    except Exception as e:
        logger.error(f"Error in run_scheduled_trader_gpt_analyses: {str(e)}")
        return {
            'successful': 0,
            'failed': 0,
            'errors': [str(e)]
        }


@csrf_exempt
@require_http_methods(["POST"])
def trigger_bulk_analysis_view(request):
    """Manual trigger for bulk analysis of all watched assets"""
    try:
        results = run_scheduled_trader_gpt_analyses()
        
        return JsonResponse({
            'success': True,
            'message': 'Bulk analysis completed',
            'results': results
        })
    except Exception as e:
        logger.error(f"Error in trigger_bulk_analysis_view: {str(e)}")
        return JsonResponse({'success': False, 'error': str(e)}, status=500)



import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression


def _calculate_mss(data, lookback_period):
    """
    Internal function to calculate Market Stability Score
    
    Args:
        data (pd.DataFrame): DataFrame with 'Close' and 'Volume' columns and datetime index
        lookback_period (int): Number of days to analyze
        
    Returns:
        float: MSS value or None if calculation fails
    """
    try:
        # Filter data to lookback period
        hist = data.tail(lookback_period).copy()
        
        if len(hist) < 20:
            return None
        
        # Calculate returns
        hist['returns'] = hist['Close'].pct_change()
        
        # Calculate volatility ()
        volatility = hist['returns'].std()
        
        # Calculate R (trend clarity)
        prices = hist['Close'].values
        X = np.arange(len(prices)).reshape(-1, 1)
        y = prices.reshape(-1, 1)
        
        model = LinearRegression()
        model.fit(X, y)
        r_squared = model.score(X, y)
        
        # Calculate trend consistency (directional strength)
        returns = hist['returns'].dropna()
        if len(returns) > 0:
            positive_days = (returns > 0).sum()
            trend_consistency = abs(positive_days / len(returns) - 0.5) * 2
        else:
            trend_consistency = 0
        
        # Calculate trend strength (magnitude of slope relative to price)
        if len(prices) > 0 and prices[0] != 0:
            slope_per_day = model.coef_[0][0]
            avg_price = np.mean(prices)
            trend_strength = abs(slope_per_day * len(prices)) / avg_price if avg_price != 0 else 0
            trend_strength = min(trend_strength, 1.0)
        else:
            trend_strength = 0
        
        # Calculate liquidity factor
        avg_volume = hist['Volume'].mean()
        
        if avg_volume > 10000000:
            liquidity_factor = 1.2
        elif avg_volume > 1000000:
            liquidity_factor = 1.0
        elif avg_volume > 100000:
            liquidity_factor = 0.9
        else:
            liquidity_factor = 0.8
        
        # Normalize volatility (simple normalization)
        # In single-asset case, we use a fixed reference
        normalized_volatility = min(volatility / 0.05, 1.0)  # 0.05 as reference volatility
        
        # Calculate trend score
        trend_score = (
            r_squared * 0.5 +
            trend_consistency * 0.3 +
            trend_strength * 0.2
        ) * 100
        
        # Apply stability factor
        stability_factor = (1 - normalized_volatility) ** 0.6
        
        # Calculate MSS
        mss = trend_score * stability_factor * liquidity_factor
        mss = min(max(mss, 0), 100)
        
        return mss
        
    except Exception as e:
        return None


def is_stable_market(data, lookback_period):
    """
    Check if market is stable (MSS >= 47)
    
    Args:
        data (pd.DataFrame): DataFrame with 'Close' and 'Volume' columns and datetime index
        lookback_period (int): Number of days to analyze
        
    Returns:
        bool: True if stable (MSS >= 47), False otherwise
    """
    try:
        mss = _calculate_mss(data, lookback_period)
        if mss is None:
            return False
        return mss >= 47
    except Exception:
        return False


def is_choppy_market(data, lookback_period):
    """
    Check if market is choppy (30 <= MSS < 47)
    
    Args:
        data (pd.DataFrame): DataFrame with 'Close' and 'Volume' columns and datetime index
        lookback_period (int): Number of days to analyze
        
    Returns:
        bool: True if choppy (30 <= MSS < 47), False otherwise
    """
    try:
        mss = _calculate_mss(data, lookback_period)
        if mss is None:
            return False
        return 30 <= mss < 47
    except Exception:
        return False


def is_volatile_market(data, lookback_period):
    """
    Check if market is volatile (MSS < 30)
    
    Args:
        data (pd.DataFrame): DataFrame with 'Close' and 'Volume' columns and datetime index
        lookback_period (int): Number of days to analyze
        
    Returns:
        bool: True if volatile (MSS < 30), False otherwise
    """
    try:
        mss = _calculate_mss(data, lookback_period)
        if mss is None:
            return False
        return mss < 30
    except Exception:
        return False



# HELPER FUNCTION FOR EASY REGIME DETECTION

def get_market_regime(data, lookback_period):
    """
    Get the current market regime as a string
    
    Returns:
        str: One of 'bullish_stable', 'bullish_choppy', 'bullish_volatile',
             'bearish_stable', 'bearish_choppy', 'bearish_volatile', or 'unknown'
    """
    try:
        mss, is_bullish = _calculate_mss(data, lookback_period)
        if mss is None:
            return 'unknown'
        
        direction = 'bullish' if is_bullish else 'bearish'
        
        if mss >= 50:
            stability = 'stable'
        elif mss >= 30:
            stability = 'choppy'
        else:
            stability = 'volatile'
        
        return f'{direction}_{stability}'
    except Exception:
        return 'unknown'


# ============================================================================
# FIX: Enhanced save with logging and validation
# ============================================================================

@csrf_exempt
@require_http_methods(["POST"])
def snowai_save_trading_weights(request):
    """
    Save neural network weights for a SnowAI trading agent.
    POST /api/snowai-trading-weights/save/
    Body: {
        "agent_name": "Snow-Alpha",
        "weights": {w1, b1, w2, b2, w3, b3},
        "metadata": {lr, hiddenSize, etc} (optional)
    }
    """
    try:
        data = json.loads(request.body)
        agent_name = data.get('agent_name')
        weights = data.get('weights')
        metadata = data.get('metadata', {})
        
        #  VALIDATION
        if not agent_name or not weights:
            return JsonResponse({
                'success': False,
                'error': 'agent_name and weights are required'
            }, status=400)
        
        #  VALIDATE WEIGHTS STRUCTURE
        required_keys = ['w1', 'b1', 'w2', 'b2', 'w3', 'b3']
        if not all(key in weights for key in required_keys):
            return JsonResponse({
                'success': False,
                'error': f'Weights must contain: {required_keys}'
            }, status=400)
        
        #  LOG THE SAVE OPERATION
        logger.info(f" Saving weights for agent: {agent_name}")
        logger.info(f"   Metadata: {metadata}")
        
        # Update or create
        obj, created = SnowAITradingWeights.objects.update_or_create(
            snow_agent_name=agent_name,
            defaults={
                'snow_weights_data': weights,
                'snow_metadata': metadata
            }
        )
        
        #  LOG SUCCESS
        action = "created" if created else "updated"
        logger.info(f" Weights {action} for {agent_name}")
        
        return JsonResponse({
            'success': True,
            'message': f'Weights {action} for {agent_name}',
            'agent_name': agent_name,
            'created': created  #  Return whether it was newly created
        })
        
    except json.JSONDecodeError:
        return JsonResponse({
            'success': False,
            'error': 'Invalid JSON'
        }, status=400)
    except Exception as e:
        logger.error(f" Error saving weights: {str(e)}")
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=500)


# ============================================================================
# FIX: Enhanced load with better error messages
# ============================================================================

@csrf_exempt
@require_http_methods(["GET"])
def snowai_load_trading_weights(request, agent_name):
    """
    Load neural network weights for a SnowAI trading agent.
    GET /api/snowai-trading-weights/load/<agent_name>/
    """
    try:
        #  LOG THE LOAD ATTEMPT
        logger.info(f" Loading weights for agent: {agent_name}")
        
        obj = SnowAITradingWeights.objects.get(snow_agent_name=agent_name)
        
        #  VALIDATE WEIGHTS EXIST
        if not obj.snow_weights_data:
            logger.warning(f" Weights found but empty for {agent_name}")
            return JsonResponse({
                'success': False,
                'error': f'Weights exist but are empty for: {agent_name}'
            }, status=404)
        
        #  LOG SUCCESS
        logger.info(f" Weights loaded for {agent_name}")
        
        return JsonResponse({
            'success': True,
            'agent_name': obj.snow_agent_name,
            'weights': obj.snow_weights_data,
            'metadata': obj.snow_metadata,
            'updated_at': obj.snow_updated_at.isoformat()
        })
        
    except SnowAITradingWeights.DoesNotExist:
        logger.warning(f" No weights found for agent: {agent_name}")
        return JsonResponse({
            'success': False,
            'error': f'No weights found for agent: {agent_name}'
        }, status=404)
    except Exception as e:
        logger.error(f" Error loading weights for {agent_name}: {str(e)}")
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def snowai_list_trading_weights(request):
    """
    List all saved SnowAI trading agent weights.
    GET /api/snowai-trading-weights/list/
    """
    try:
        weights = SnowAITradingWeights.objects.all().order_by('-snow_updated_at')
        
        data = [{
            'agent_name': w.snow_agent_name,
            'metadata': w.snow_metadata,
            'updated_at': w.snow_updated_at.isoformat(),
            'created_at': w.snow_created_at.isoformat()
        } for w in weights]
        
        return JsonResponse({
            'success': True,
            'count': len(data),
            'weights': data
        })
        
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=500)


@csrf_exempt
@require_http_methods(["DELETE"])
def snowai_delete_trading_weights(request, agent_name):
    """
    Delete neural network weights for a SnowAI trading agent.
    DELETE /api/snowai-trading-weights/delete/<agent_name>/
    """
    try:
        obj = SnowAITradingWeights.objects.get(snow_agent_name=agent_name)
        obj.delete()
        
        return JsonResponse({
            'success': True,
            'message': f'Weights deleted for {agent_name}'
        })
        
    except SnowAITradingWeights.DoesNotExist:
        return JsonResponse({
            'success': False,
            'error': f'No weights found for agent: {agent_name}'
        }, status=404)
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=500)


"""
SnowAI Trading Model Inference Functions
Pure Python implementation - no TensorFlow or PyTorch required
ASYNC-SAFE for Django
"""

from asgiref.sync import sync_to_async
import numpy as np
import logging
from typing import List, Dict, Optional, Tuple
from .models import SnowAITradingWeights

logger = logging.getLogger(__name__)


# ============================================================================
# ASYNC-SAFE DATABASE ACCESS
# ============================================================================

@sync_to_async
def _get_weights_from_db(agent_name: str) -> Optional[Dict]:
    """Async-safe database fetch"""
    try:
        weights_obj = SnowAITradingWeights.objects.get(snow_agent_name=agent_name)
        return weights_obj.snow_weights_data
    except SnowAITradingWeights.DoesNotExist:
        return None
    except Exception as e:
        logger.error(f"Database error fetching weights for {agent_name}: {e}")
        return None


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def calculate_sma(data: List[Dict], period: int) -> Optional[float]:
    """Calculate Simple Moving Average"""
    if len(data) < period:
        return None
    sliced = data[-period:]
    total = sum(d['close'] for d in sliced)
    return total / period


def calculate_volatility(data: List[Dict], period: int) -> float:
    """Calculate price volatility (standard deviation)"""
    if len(data) < period:
        return 0.0
    
    sliced = data[-period:]
    sma = calculate_sma(data, period)
    if sma is None:
        return 0.0
    
    variance = sum((d['close'] - sma) ** 2 for d in sliced) / period
    return np.sqrt(variance)


def normalize(value: float, min_val: float, max_val: float) -> float:
    """Normalize value between 0 and 1"""
    if min_val == max_val:
        return 0.5
    return (value - min_val) / (max_val - min_val)


# ============================================================================
# FEATURE ENGINEERING
# ============================================================================

# ============================================================================
# REAL FIX: Orthogonal Feature Engineering
# Each agent sees the market through a completely different lens
# ============================================================================

def calculate_model_inputs(
    data: List[Dict],
    state_features: str = 'standard',
    position_size: float = 0.0,
    unrealized_pnl: float = 0.0
) -> List[float]:
    """
    TRUE FIX: Each agent gets COMPLETELY DIFFERENT features.
    Not just different parameters - different mathematical transformations!
    """
    try:
        if len(data) < 20:
            return [0.0] * 7
        
        current_price = data[-1]['close']
        
        # ================================================================
        # RAW FEATURE EXTRACTORS
        # ================================================================
        
        def get_price_percentile(lookback):
            """Where is current price in recent range? (0-1)"""
            recent = data[-lookback:]
            prices = [d['close'] for d in recent]
            min_p, max_p = min(prices), max(prices)
            if max_p == min_p:
                return 0.5
            return (current_price - min_p) / (max_p - min_p)
        
        def get_momentum(lookback):
            """Rate of change over period"""
            if len(data) < lookback + 1:
                return 0.0
            old_price = data[-lookback]['close']
            if old_price == 0:
                return 0.0
            return (current_price - old_price) / old_price
        
        def get_volatility_normalized(period):
            """Volatility as % of price"""
            vol = calculate_volatility(data, period)
            return vol / current_price if current_price != 0 else 0
        
        def get_trend_alignment():
            """Are short/medium/long trends aligned?"""
            if len(data) < 50:
                return 0.0
            
            avg_5 = sum(d['close'] for d in data[-5:]) / 5
            avg_20 = sum(d['close'] for d in data[-20:]) / 20
            avg_50 = sum(d['close'] for d in data[-50:]) / 50
            
            # All aligned up = 1.0, all aligned down = -1.0, mixed = 0.0
            if avg_5 > avg_20 > avg_50:
                return 1.0
            elif avg_5 < avg_20 < avg_50:
                return -1.0
            else:
                return 0.0
        
        def get_acceleration():
            """Is momentum accelerating or decelerating?"""
            if len(data) < 10:
                return 0.0
            
            mom_recent = get_momentum(3)
            mom_older = get_momentum(6)
            
            # Positive = accelerating, negative = decelerating
            return mom_recent - mom_older
        
        def get_range_position():
            """Where in recent high-low range?"""
            if len(data) < 20:
                return 0.5
            
            recent = data[-20:]
            high = max(d['high'] for d in recent)
            low = min(d['low'] for d in recent)
            
            if high == low:
                return 0.5
            
            return (current_price - low) / (high - low)
        
        def get_body_ratio():
            """Candle body size vs full range (strength of move)"""
            last = data[-1]
            body = abs(last['close'] - last['open'])
            full_range = last['high'] - last['low']
            
            if full_range == 0:
                return 0.5
            
            return body / full_range
        
        def get_higher_high_lower_low():
            """Market structure: HH/HL (uptrend) or LH/LL (downtrend)"""
            if len(data) < 10:
                return 0.0
            
            recent_highs = [d['high'] for d in data[-10:]]
            recent_lows = [d['low'] for d in data[-10:]]
            
            # Compare recent highs/lows to older ones
            newer_high = max(recent_highs[-5:])
            older_high = max(recent_highs[:5])
            newer_low = min(recent_lows[-5:])
            older_low = min(recent_lows[:5])
            
            if newer_high > older_high and newer_low > older_low:
                return 1.0  # Higher highs and higher lows (uptrend)
            elif newer_high < older_high and newer_low < older_low:
                return -1.0  # Lower highs and lower lows (downtrend)
            else:
                return 0.0  # Mixed/ranging
        
        def get_volume_surge():
            """Would detect volume spikes if volume data available"""
            # Most CSV data doesn't have volume
            return 0.0
        
        def get_distance_from_extremes():
            """How far from recent high/low?"""
            if len(data) < 20:
                return 0.0
            
            recent = data[-20:]
            high = max(d['high'] for d in recent)
            low = min(d['low'] for d in recent)
            
            dist_from_high = (high - current_price) / current_price if current_price != 0 else 0
            dist_from_low = (current_price - low) / current_price if current_price != 0 else 0
            
            # Net signal: negative = near high, positive = near low
            return dist_from_low - dist_from_high
        
        # ================================================================
        # AGENT-SPECIFIC FEATURE COMBINATIONS
        # Each agent gets 7 COMPLETELY DIFFERENT features!
        # ================================================================
        
        if state_features == 'standard':
            # SNOW-ALPHA: Balanced - mix of everything
            return [
                get_price_percentile(20),
                get_momentum(10),
                get_volatility_normalized(10),
                get_trend_alignment(),
                float(position_size),
                float(np.clip(unrealized_pnl * 10, -1, 1)),
                get_acceleration()
            ]
        
        elif state_features == 'short-term':
            # ICE-BETA: Scalper - ultra-short-term signals
            return [
                get_price_percentile(3),           # Micro range
                get_momentum(2),                   # 2-candle momentum
                get_body_ratio(),                  # Candle strength
                get_acceleration(),                # Momentum acceleration
                float(position_size),
                float(np.clip(unrealized_pnl * 20, -1, 1)),  # Hypersensitive to P&L
                get_volatility_normalized(5)       # Very short vol
            ]
        
        elif state_features == 'long-term':
            # FROST-GAMMA: Trend follower - slow signals
            return [
                get_price_percentile(50),          # Long-term range
                get_momentum(20),                  # Slow momentum
                get_trend_alignment(),             # Trend confirmation
                get_higher_high_lower_low(),       # Market structure
                float(position_size),
                float(np.clip(unrealized_pnl * 5, -1, 1)),  # Patient with P&L
                get_volatility_normalized(20)      # Long-term vol
            ]
        
        elif state_features == 'momentum':
            # GLACIER-X, BLIZZARD-OMEGA: Momentum traders
            return [
                get_momentum(5),                   # Fast momentum
                get_momentum(10),                  # Medium momentum
                get_acceleration(),                # Momentum change
                get_trend_alignment(),             # Trend strength
                float(position_size),
                float(np.clip(unrealized_pnl * 10, -1, 1)),
                get_higher_high_lower_low()        # Momentum structure
            ]
        
        elif state_features == 'mean-reversion':
            # TUNDRA-SIGMA: Mean reversion
            return [
                get_range_position(),              # Where in range?
                get_distance_from_extremes(),      # Near high/low?
                -get_momentum(10),                 # INVERTED momentum
                -get_acceleration(),               # INVERTED acceleration
                float(position_size),
                float(np.clip(unrealized_pnl * 10, -1, 1)),
                get_volatility_normalized(15)      # Look for volatility compression
            ]
        
        elif state_features == 'volatility':
            # AVALANCHE-Z, ARCTIC-DELTA: Volatility hunters
            return [
                get_volatility_normalized(5),      # Short-term vol
                get_volatility_normalized(10),     # Medium-term vol
                get_volatility_normalized(20),     # Long-term vol (for comparison)
                get_body_ratio(),                  # Strong candles
                float(position_size),
                float(np.clip(unrealized_pnl * 15, -1, 1)),
                get_acceleration()                 # Explosive moves
            ]
        
        elif state_features == 'contrarian':
            # PERMAFROST-THETA: Contrarian
            return [
                1.0 - get_range_position(),        # INVERTED range position
                -get_momentum(10),                 # INVERTED momentum
                -get_trend_alignment(),            # INVERTED trend
                get_distance_from_extremes(),      # Look for extremes
                float(position_size),
                float(np.clip(unrealized_pnl * 10, -1, 1)),
                -get_acceleration()                # INVERTED acceleration
            ]
        
        elif state_features == 'risk-aware':
            # POLAR-PRIME: Conservative
            return [
                get_price_percentile(50),          # Long-term position
                get_momentum(30),                  # Very slow momentum
                get_trend_alignment(),             # Only trade with clear trend
                get_volatility_normalized(20) * 0.5,  # Dampened volatility
                float(position_size),
                float(np.clip(unrealized_pnl * 3, -1, 1)),  # Very patient
                get_higher_high_lower_low()        # Structural confirmation
            ]
        
        else:
            # Fallback
            return [
                get_price_percentile(20),
                get_momentum(10),
                get_volatility_normalized(10),
                get_trend_alignment(),
                float(position_size),
                float(np.clip(unrealized_pnl * 10, -1, 1)),
                get_acceleration()
            ]
        
    except Exception as e:
        logger.error(f"Error calculating model inputs: {e}")
        return [0.0] * 7

# ============================================================================
# NEURAL NETWORK FORWARD PASS
# ============================================================================

def relu(x: np.ndarray) -> np.ndarray:
    """ReLU activation function"""
    return np.maximum(0, x)


def forward_pass(inputs: List[float], weights: Dict) -> List[float]:
    """
    Perform forward pass through the neural network.
    
    Args:
        inputs: List of 7 input features
        weights: Dict containing w1, b1, w2, b2, w3, b3
    
    Returns:
        List of Q-values for each action
    """
    try:
        # Convert to numpy arrays
        x = np.array(inputs, dtype=np.float32)  # Shape: (7,)
        
        # ================================================================
        # Layer 1: Input -> Hidden1 (ReLU)
        # ================================================================
        w1 = np.array(weights['w1'], dtype=np.float32)  # Shape: (7, 32) - NO TRANSPOSE
        b1 = np.array(weights['b1'], dtype=np.float32)  # Shape: (32,)
        hidden1 = relu(np.dot(x, w1) + b1)  # (7,) @ (7, 32) = (32,)
        
        # ================================================================
        # Layer 2: Hidden1 -> Hidden2 (ReLU)
        # ================================================================
        w2 = np.array(weights['w2'], dtype=np.float32)  # Shape: (32, 16) - NO TRANSPOSE
        b2 = np.array(weights['b2'], dtype=np.float32)  # Shape: (16,)
        hidden2 = relu(np.dot(hidden1, w2) + b2)  # (32,) @ (32, 16) = (16,)
        
        # ================================================================
        # Layer 3: Hidden2 -> Output (Linear)
        # ================================================================
        w3 = np.array(weights['w3'], dtype=np.float32)  # Shape: (16, 5) - NO TRANSPOSE
        b3 = np.array(weights['b3'], dtype=np.float32)  # Shape: (5,)
        output = np.dot(hidden2, w3) + b3  # (16,) @ (16, 5) = (5,)
        
        return output.tolist()
        
    except Exception as e:
        logger.error(f"Error in forward pass: {e}")
        logger.error(f"Input shape: {np.array(inputs).shape}")
        logger.error(f"w1 shape: {np.array(weights['w1']).shape}")
        logger.error(f"w2 shape: {np.array(weights['w2']).shape}")
        logger.error(f"w3 shape: {np.array(weights['w3']).shape}")
        return [0.0, 0.0, 0.0, 0.0, 0.0]


def get_best_action(
    q_values: List[float], 
    current_position: str = 'none',
    agent_personality: str = 'balanced'
) -> int:
    """
    Get the best action based on Q-values, position, and agent personality.
    
    THIS IS THE KEY FIX: Agent personalities create behavioral differences
    even when Q-values are similar!
    
    Args:
        q_values: List of Q-values for [BUY, HOLD, SELL, SHORT, COVER]
        current_position: 'long', 'short', or 'none'
        agent_personality: Trading style that biases action selection
    
    Returns:
        Action index (0=BUY, 1=HOLD, 2=SELL, 3=SHORT, 4=COVER)
    """
    # Create mutable copy
    q_copy = np.array(q_values, dtype=np.float32).copy()
    
    # ========================================================================
    # APPLY PERSONALITY BIASES (BEFORE POSITION CONSTRAINTS)
    # This is what makes agents behave differently!
    # ========================================================================
    
    if agent_personality == 'aggressive':
        # Boost action-taking, penalize holding
        q_copy[0] += 0.15  # BUY boost
        q_copy[3] += 0.15  # SHORT boost
        q_copy[1] -= 0.2   # HOLD penalty
        
    elif agent_personality == 'scalper':
        # Strong preference for quick entries/exits
        q_copy[0] += 0.2   # Strong BUY boost
        q_copy[3] += 0.2   # Strong SHORT boost
        q_copy[1] -= 0.25  # Heavy HOLD penalty
        q_copy[2] += 0.1   # Slight SELL boost (quick exits)
        q_copy[4] += 0.1   # Slight COVER boost (quick exits)
        
    elif agent_personality == 'trend':
        # Prefer holding positions longer
        q_copy[1] += 0.1   # HOLD bonus
        q_copy[2] -= 0.05  # Slight SELL penalty (resist early exits)
        q_copy[4] -= 0.05  # Slight COVER penalty (resist early exits)
        
    elif agent_personality == 'momentum':
        # Aggressive entries, hold for momentum
        q_copy[0] += 0.12  # BUY boost
        q_copy[3] += 0.12  # SHORT boost
        q_copy[1] += 0.05  # Slight HOLD bonus
        
    elif agent_personality == 'conservative':
        # Strong HOLD bias, reluctant to act
        q_copy[1] += 0.15  # Strong HOLD bonus
        q_copy[0] -= 0.1   # BUY penalty
        q_copy[3] -= 0.1   # SHORT penalty
        
    elif agent_personality == 'contrarian':
        # Invert BUY/SHORT preferences
        # If model wants to BUY, contrarian wants to SHORT and vice versa
        temp_buy = q_copy[0]
        q_copy[0] = q_copy[3]  # BUY gets SHORT's Q-value
        q_copy[3] = temp_buy   # SHORT gets BUY's Q-value
        q_copy[1] -= 0.1       # Slight HOLD penalty
    
    elif agent_personality == 'balanced':
        # No biases - pure Q-value selection
        pass
    
    # ========================================================================
    # APPLY POSITION CONSTRAINTS (PREVENT IMPOSSIBLE ACTIONS)
    # ========================================================================
    
    if current_position == 'long':
        q_copy[0] = -np.inf  # Can't BUY again
        q_copy[3] = -np.inf  # Can't SHORT while long
        q_copy[4] = -np.inf  # Can't COVER while long
        
    elif current_position == 'short':
        q_copy[0] = -np.inf  # Can't BUY while short
        q_copy[2] = -np.inf  # Can't SELL while short
        q_copy[3] = -np.inf  # Can't SHORT again
        
    else:  # No position
        q_copy[2] = -np.inf  # Can't SELL without long position
        q_copy[4] = -np.inf  # Can't COVER without short position
    
    # ========================================================================
    # SELECT BEST ACTION
    # ========================================================================
    
    best_action = int(np.argmax(q_copy))
    
    return best_action


# ============================================================================
# FIX: Better error handling in prediction function
# ============================================================================

async def _load_and_predict_async(
    agent_name: str,
    data: List[Dict],
    state_features: str,
    current_position: str = 'none',
    position_size: float = 0.0,
    unrealized_pnl: float = 0.0
) -> Tuple[bool, int, str]:
    """
    Async-safe internal function to load weights and make prediction.
    
    Returns:
        (success, action, error_message)
    """
    try:
        #  LOG WHICH AGENT IS BEING LOADED
        logger.info(f" Predicting for agent: {agent_name}")
        
        # Async-safe database fetch
        weights = await _get_weights_from_db(agent_name)
        
        #  FAIL LOUDLY IF WEIGHTS NOT FOUND
        if weights is None:
            error_msg = f" NO WEIGHTS FOUND FOR {agent_name} - Cannot predict"
            logger.error(error_msg)
            return False, 1, error_msg  # Return HOLD as default
        
        #  VALIDATE WEIGHTS STRUCTURE
        required_keys = ['w1', 'b1', 'w2', 'b2', 'w3', 'b3']
        if not all(key in weights for key in required_keys):
            error_msg = f" INVALID WEIGHTS FOR {agent_name} - Missing keys"
            logger.error(error_msg)
            return False, 1, error_msg
        
        #  LOG SUCCESSFUL WEIGHT LOAD
        logger.info(f" Weights loaded for {agent_name}")
        
        # Calculate input features
        inputs = calculate_model_inputs(
            data, 
            state_features=state_features,
            position_size=position_size,
            unrealized_pnl=unrealized_pnl
        )
        
        # Forward pass
        q_values = forward_pass(inputs, weights)
        
        # Get best action
        action = get_best_action(q_values, current_position)
        
        #  LOG THE PREDICTION
        action_names = ['BUY', 'HOLD', 'SELL', 'SHORT', 'COVER']
        logger.info(f" {agent_name} predicts: {action_names[action]}")
        
        return True, action, ""
        
    except Exception as e:
        error_msg = f" Error in {agent_name} prediction: {str(e)}"
        logger.error(error_msg)
        return False, 1, error_msg


# ============================================================================
# DEBUG: Check if all agents have unique weights
# ============================================================================

@csrf_exempt
@require_http_methods(["GET"])
def snowai_debug_weights(request):
    """
    DEBUG ENDPOINT: Show first 5 weights from each agent for comparison
    GET /api/snowai-trading-weights/debug/
    """
    try:
        all_agents = SnowAITradingWeights.objects.all()
        
        debug_info = []
        for agent in all_agents:
            weights = agent.snow_weights_data
            
            # Get first 5 weights from w1 for comparison
            w1_sample = []
            if 'w1' in weights and len(weights['w1']) > 0:
                w1_sample = [weights['w1'][0][:5]]  # First 5 from first row
            
            debug_info.append({
                'agent_name': agent.snow_agent_name,
                'w1_sample': w1_sample,
                'metadata': agent.snow_metadata,
                'updated_at': agent.snow_updated_at.isoformat()
            })
        
        return JsonResponse({
            'success': True,
            'agents': debug_info,
            'note': 'If w1_sample is identical across agents, they have the same weights!'
        })
        
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=500)

# ============================================================================
# SNOW-ALPHA (Balanced DDQN)
# ============================================================================

async def snow_alpha_predict(
    data: List[Dict],
    current_position: str = 'none',
    position_size: float = 0.0,
    unrealized_pnl: float = 0.0
) -> Dict:
    """
    Snow-Alpha: Balanced Deep Double Q-Network
    
    Args:
        data: List of OHLCV dicts
        current_position: 'long', 'short', or 'none'
        position_size: Current position size as ratio (0-1)
        unrealized_pnl: Unrealized P&L as percentage (-1 to 1)
    
    Returns:
        {
            'success': bool,
            'action': int (0=BUY, 1=HOLD, 2=SELL, 3=SHORT, 4=COVER),
            'action_name': str,
            'error': str
        }
    """
    success, action, error = await _load_and_predict_async(
        'Snow-Alpha', data, 'standard', current_position, position_size, unrealized_pnl
    )
    
    action_names = ['BUY', 'HOLD', 'SELL', 'SHORT', 'COVER']
    return {
        'success': success,
        'action': action,
        'action_name': action_names[action],
        'error': error
    }


async def snow_alpha_buy(data: List[Dict]) -> bool:
    """Returns True if Snow-Alpha recommends BUY"""
    result = await snow_alpha_predict(data, current_position='none')
    return result['success'] and result['action'] == 0


async def snow_alpha_sell(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    """Returns True if Snow-Alpha recommends SELL"""
    result = await snow_alpha_predict(data, current_position='long', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 2


async def snow_alpha_short(data: List[Dict]) -> bool:
    """Returns True if Snow-Alpha recommends SHORT"""
    result = await snow_alpha_predict(data, current_position='none')
    return result['success'] and result['action'] == 3


async def snow_alpha_cover(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    """Returns True if Snow-Alpha recommends COVER"""
    result = await snow_alpha_predict(data, current_position='short', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 4


# ============================================================================
# ICE-BETA (Scalper)
# ============================================================================

async def ice_beta_predict(
    data: List[Dict],
    current_position: str = 'none',
    position_size: float = 0.0,
    unrealized_pnl: float = 0.0
) -> Dict:
    """Ice-Beta: High-frequency scalper"""
    success, action, error = await _load_and_predict_async(
        'Ice-Beta', data, 'short-term', current_position, position_size, unrealized_pnl
    )
    
    action_names = ['BUY', 'HOLD', 'SELL', 'SHORT', 'COVER']
    return {
        'success': success,
        'action': action,
        'action_name': action_names[action],
        'error': error
    }


async def ice_beta_buy(data: List[Dict]) -> bool:
    result = await ice_beta_predict(data, current_position='none')
    return result['success'] and result['action'] == 0


async def ice_beta_sell(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await ice_beta_predict(data, current_position='long', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 2


async def ice_beta_short(data: List[Dict]) -> bool:
    result = await ice_beta_predict(data, current_position='none')
    return result['success'] and result['action'] == 3


async def ice_beta_cover(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await ice_beta_predict(data, current_position='short', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 4


# ============================================================================
# FROST-GAMMA (Trend Follower)
# ============================================================================

async def frost_gamma_predict(
    data: List[Dict],
    current_position: str = 'none',
    position_size: float = 0.0,
    unrealized_pnl: float = 0.0
) -> Dict:
    """Frost-Gamma: Conservative trend-following"""
    success, action, error = await _load_and_predict_async(
        'Frost-Gamma', data, 'long-term', current_position, position_size, unrealized_pnl
    )
    
    action_names = ['BUY', 'HOLD', 'SELL', 'SHORT', 'COVER']
    return {
        'success': success,
        'action': action,
        'action_name': action_names[action],
        'error': error
    }


async def frost_gamma_buy(data: List[Dict]) -> bool:
    result = await frost_gamma_predict(data, current_position='none')
    return result['success'] and result['action'] == 0


async def frost_gamma_sell(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await frost_gamma_predict(data, current_position='long', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 2


async def frost_gamma_short(data: List[Dict]) -> bool:
    result = await frost_gamma_predict(data, current_position='none')
    return result['success'] and result['action'] == 3


async def frost_gamma_cover(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await frost_gamma_predict(data, current_position='short', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 4


# ============================================================================
# GLACIER-X (Deep-Hold / Momentum)
# ============================================================================

async def glacier_x_predict(
    data: List[Dict],
    current_position: str = 'none',
    position_size: float = 0.0,
    unrealized_pnl: float = 0.0
) -> Dict:
    """Glacier-X: Position trader with momentum focus"""
    success, action, error = await _load_and_predict_async(
        'Glacier-X', data, 'momentum', current_position, position_size, unrealized_pnl
    )
    
    action_names = ['BUY', 'HOLD', 'SELL', 'SHORT', 'COVER']
    return {
        'success': success,
        'action': action,
        'action_name': action_names[action],
        'error': error
    }


async def glacier_x_buy(data: List[Dict]) -> bool:
    result = await glacier_x_predict(data, current_position='none')
    return result['success'] and result['action'] == 0


async def glacier_x_sell(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await glacier_x_predict(data, current_position='long', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 2


async def glacier_x_short(data: List[Dict]) -> bool:
    result = await glacier_x_predict(data, current_position='none')
    return result['success'] and result['action'] == 3


async def glacier_x_cover(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await glacier_x_predict(data, current_position='short', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 4


# ============================================================================
# AVALANCHE-Z (Aggressive)
# ============================================================================

async def avalanche_z_predict(
    data: List[Dict],
    current_position: str = 'none',
    position_size: float = 0.0,
    unrealized_pnl: float = 0.0
) -> Dict:
    """Avalanche-Z: Ultra-aggressive high-risk trader"""
    success, action, error = await _load_and_predict_async(
        'Avalanche-Z', data, 'volatility', current_position, position_size, unrealized_pnl
    )
    
    action_names = ['BUY', 'HOLD', 'SELL', 'SHORT', 'COVER']
    return {
        'success': success,
        'action': action,
        'action_name': action_names[action],
        'error': error
    }


async def avalanche_z_buy(data: List[Dict]) -> bool:
    result = await avalanche_z_predict(data, current_position='none')
    return result['success'] and result['action'] == 0


async def avalanche_z_sell(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await avalanche_z_predict(data, current_position='long', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 2


async def avalanche_z_short(data: List[Dict]) -> bool:
    result = await avalanche_z_predict(data, current_position='none')
    return result['success'] and result['action'] == 3


async def avalanche_z_cover(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await avalanche_z_predict(data, current_position='short', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 4


# ============================================================================
# POLAR-PRIME (Conservative)
# ============================================================================

async def polar_prime_predict(
    data: List[Dict],
    current_position: str = 'none',
    position_size: float = 0.0,
    unrealized_pnl: float = 0.0
) -> Dict:
    """Polar-Prime: Ultra-conservative capital preservation"""
    success, action, error = await _load_and_predict_async(
        'Polar-Prime', data, 'risk-aware', current_position, position_size, unrealized_pnl
    )
    
    action_names = ['BUY', 'HOLD', 'SELL', 'SHORT', 'COVER']
    return {
        'success': success,
        'action': action,
        'action_name': action_names[action],
        'error': error
    }


async def polar_prime_buy(data: List[Dict]) -> bool:
    result = await polar_prime_predict(data, current_position='none')
    return result['success'] and result['action'] == 0


async def polar_prime_sell(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await polar_prime_predict(data, current_position='long', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 2


async def polar_prime_short(data: List[Dict]) -> bool:
    result = await polar_prime_predict(data, current_position='none')
    return result['success'] and result['action'] == 3


async def polar_prime_cover(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await polar_prime_predict(data, current_position='short', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 4


# ============================================================================
# BLIZZARD-OMEGA (Momentum Specialist)
# ============================================================================

async def blizzard_omega_predict(
    data: List[Dict],
    current_position: str = 'none',
    position_size: float = 0.0,
    unrealized_pnl: float = 0.0
) -> Dict:
    """Blizzard-Omega: Momentum specialist"""
    success, action, error = await _load_and_predict_async(
        'Blizzard-Omega', data, 'momentum', current_position, position_size, unrealized_pnl
    )
    
    action_names = ['BUY', 'HOLD', 'SELL', 'SHORT', 'COVER']
    return {
        'success': success,
        'action': action,
        'action_name': action_names[action],
        'error': error
    }


async def blizzard_omega_buy(data: List[Dict]) -> bool:
    result = await blizzard_omega_predict(data, current_position='none')
    return result['success'] and result['action'] == 0


async def blizzard_omega_sell(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await blizzard_omega_predict(data, current_position='long', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 2


async def blizzard_omega_short(data: List[Dict]) -> bool:
    result = await blizzard_omega_predict(data, current_position='none')
    return result['success'] and result['action'] == 3


async def blizzard_omega_cover(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await blizzard_omega_predict(data, current_position='short', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 4


# ============================================================================
# TUNDRA-SIGMA (Mean Reversion)
# ============================================================================

async def tundra_sigma_predict(
    data: List[Dict],
    current_position: str = 'none',
    position_size: float = 0.0,
    unrealized_pnl: float = 0.0
) -> Dict:
    """Tundra-Sigma: Mean reversion specialist"""
    success, action, error = await _load_and_predict_async(
        'Tundra-Sigma', data, 'mean-reversion', current_position, position_size, unrealized_pnl
    )
    
    action_names = ['BUY', 'HOLD', 'SELL', 'SHORT', 'COVER']
    return {
        'success': success,
        'action': action,
        'action_name': action_names[action],
        'error': error
    }


async def tundra_sigma_buy(data: List[Dict]) -> bool:
    result = await tundra_sigma_predict(data, current_position='none')
    return result['success'] and result['action'] == 0


async def tundra_sigma_sell(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await tundra_sigma_predict(data, current_position='long', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 2


async def tundra_sigma_short(data: List[Dict]) -> bool:
    result = await tundra_sigma_predict(data, current_position='none')
    return result['success'] and result['action'] == 3


async def tundra_sigma_cover(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await tundra_sigma_predict(data, current_position='short', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 4


# ============================================================================
# ARCTIC-DELTA (Volatility Hunter)
# ============================================================================

async def arctic_delta_predict(
    data: List[Dict],
    current_position: str = 'none',
    position_size: float = 0.0,
    unrealized_pnl: float = 0.0
) -> Dict:
    """Arctic-Delta: Volatility hunter"""
    success, action, error = await _load_and_predict_async(
        'Arctic-Delta', data, 'volatility', current_position, position_size, unrealized_pnl
    )
    
    action_names = ['BUY', 'HOLD', 'SELL', 'SHORT', 'COVER']
    return {
        'success': success,
        'action': action,
        'action_name': action_names[action],
        'error': error
    }


async def arctic_delta_buy(data: List[Dict]) -> bool:
    result = await arctic_delta_predict(data, current_position='none')
    return result['success'] and result['action'] == 0


async def arctic_delta_sell(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await arctic_delta_predict(data, current_position='long', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 2


async def arctic_delta_short(data: List[Dict]) -> bool:
    result = await arctic_delta_predict(data, current_position='none')
    return result['success'] and result['action'] == 3


async def arctic_delta_cover(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await arctic_delta_predict(data, current_position='short', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 4


# ============================================================================
# PERMAFROST-THETA (Contrarian)
# ============================================================================

async def permafrost_theta_predict(
    data: List[Dict],
    current_position: str = 'none',
    position_size: float = 0.0,
    unrealized_pnl: float = 0.0

    ) -> Dict:
    """Permafrost-Theta: Contrarian trader"""
    success, action, error = await _load_and_predict_async(
        'Permafrost-Theta', data, 'contrarian', current_position, position_size, unrealized_pnl
    )
    
    action_names = ['BUY', 'HOLD', 'SELL', 'SHORT', 'COVER']
    return {
        'success': success,
        'action': action,
        'action_name': action_names[action],
        'error': error
    }

async def permafrost_theta_buy(data: List[Dict]) -> bool:
    result = await permafrost_theta_predict(data, current_position='none')
    return result['success'] and result['action'] == 0


async def permafrost_theta_sell(data: List[Dict], position_size: float = 0.5, unrealized_pnl: float = 0.0) -> bool:
    result = await permafrost_theta_predict(data, current_position='long', position_size=position_size, unrealized_pnl=unrealized_pnl)
    return result['success'] and result['action'] == 2


async def permafrost_theta_short(data: List[Dict]) -> bool:
    result = await permafrost_theta_predict(data, current_position='none')
    return result['success'] and result['action'] == 3


from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
import json
from .models import SnowAIForwardTestingModel

@csrf_exempt
@require_http_methods(["POST"])
def save_snowai_forward_testing_model_endpoint(request):
    """
    Endpoint to save cleaned model code for forward testing.
    Accepts: model_id, cleaned_code, notes (optional)
    Returns: Success/error message with model details
    """
    try:
        # Parse JSON data from request body
        data = json.loads(request.body)
        
        # Extract required fields
        model_id = data.get('model_id')
        cleaned_code = data.get('cleaned_model_code')
        notes = data.get('notes', '')
        
        # Validate required fields
        if not model_id or not cleaned_code:
            return JsonResponse({
                'status': 'error',
                'message': 'Missing required fields: model_id and cleaned_model_code are required'
            }, status=400)
        
        # Check if model with this ID already exists
        existing_model = SnowAIForwardTestingModel.objects.filter(model_id=model_id).first()
        
        if existing_model:
            # Update existing model
            existing_model.cleaned_model_code = cleaned_code
            existing_model.notes = notes
            existing_model.save()
            
            return JsonResponse({
                'status': 'success',
                'message': 'Model updated successfully',
                'model_id': model_id,
                'created_at': existing_model.created_at.isoformat(),
                'last_updated': existing_model.last_updated.isoformat()
            }, status=200)
        else:
            # Create new model
            new_model = SnowAIForwardTestingModel.objects.create(
                model_id=model_id,
                cleaned_model_code=cleaned_code,
                notes=notes
            )
            
            return JsonResponse({
                'status': 'success',
                'message': 'Model saved successfully',
                'model_id': new_model.model_id,
                'created_at': new_model.created_at.isoformat(),
                'last_updated': new_model.last_updated.isoformat()
            }, status=201)
            
    except json.JSONDecodeError:
        print(f'Error in save snowai forward testing model endpoint: Invalid JSON in request body')
        return JsonResponse({
            'status': 'error',
            'message': 'Invalid JSON in request body'
        }, status=400)
    
    except Exception as e:
        print(f'Error in save snowai forward testing model endpoint: {e}')

        return JsonResponse({
            'status': 'error',
            'message': f'An error occurred: {str(e)}'
        }, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def retrieve_snowai_forward_testing_model_endpoint(request, model_id):
    """
    Endpoint to retrieve a saved model by its ID.
    Returns: Model details including cleaned code
    """
    try:
        model = SnowAIForwardTestingModel.objects.get(model_id=model_id)
        
        return JsonResponse({
            'status': 'success',
            'model': {
                'model_id': model.model_id,
                'cleaned_model_code': model.cleaned_model_code,
                'created_at': model.created_at.isoformat(),
                'last_updated': model.last_updated.isoformat(),
                'notes': model.notes
            }
        }, status=200)
        
    except SnowAIForwardTestingModel.DoesNotExist:
        print(f'Error in retrieve_snowai_forward_testing_model_endpoint: Model with ID {model_id} not found')

        return JsonResponse({
            'status': 'error',
            'message': f'Model with ID {model_id} not found'
        }, status=404)
    
    except Exception as e:
        print(f'Error in retrieve_snowai_forward_testing_model_endpoint: An error occurred: {str(e)}')

        return JsonResponse({
            'status': 'error',
            'message': f'An error occurred: {str(e)}'
        }, status=500)


@csrf_exempt
@require_http_methods(["GET"])
def list_all_snowai_forward_testing_models_endpoint(request):
    """
    Endpoint to list all saved models (without full code content).
    Returns: List of all models with basic metadata
    """
    try:
        models = SnowAIForwardTestingModel.objects.all()
        
        models_list = [{
            'model_id': model.model_id,
            'created_at': model.created_at.isoformat(),
            'last_updated': model.last_updated.isoformat(),
            'code_length': len(model.cleaned_model_code),
            'has_notes': bool(model.notes)
        } for model in models]
        
        return JsonResponse({
            'status': 'success',
            'count': len(models_list),
            'models': models_list
        }, status=200)
        
    except Exception as e:
        print(f'Error in list_all_snowai_forward_testing_models_endpoint: An error occurred: {str(e)}')

        return JsonResponse({
            'status': 'error',
            'message': f'An error occurred: {str(e)}'
        }, status=500)


@csrf_exempt
@require_http_methods(["DELETE"])
def delete_snowai_forward_testing_model_endpoint(request, model_id):
    """
    Endpoint to delete a saved model by its ID.
    Returns: Success/error message
    """
    try:
        model = SnowAIForwardTestingModel.objects.get(model_id=model_id)
        model.delete()
        
        return JsonResponse({
            'status': 'success',
            'message': f'Model {model_id} deleted successfully'
        }, status=200)
        
    except SnowAIForwardTestingModel.DoesNotExist:
        
        print(f'Error in delete_snowai_forward_testing_model_endpoint: Model with ID {model_id} not found')


        return JsonResponse({
            'status': 'error',
            'message': f'Model with ID {model_id} not found'
        }, status=404)
    
    except Exception as e:
        print(f'Error in delete_snowai_forward_testing_model_endpoint: An error occurred: {str(e)}')

        return JsonResponse({
            'status': 'error',
            'message': f'An error occurred: {str(e)}'
        }, status=500)
    

def execute_all_forward_tests():
    """
    Execute forward tests for all active models
    Runs every minute via scheduler
    """
    try:
        
        active_models = ActiveForwardTestModel.objects.filter(is_active=True)
        
        # Check if NYSE is open for stock models
        nyse_open = new_york_session()
        
        for model in active_models:
            try:
                # Skip US stock models if NYSE is closed
                if not nyse_open and is_us_stock(model.asset):
                    continue
                
                execute_forward_test(model.id)
            except Exception as e:
                print(f"Error executing model {model.id}: {e}")
    
    except Exception as e:
        print(f"Error in execute_all_forward_tests: {e}")


def is_us_stock(symbol):
    """Check if symbol is a US stock"""
    return not (symbol.endswith('=X') or symbol.endswith('=F') or symbol.startswith('^'))

def execute_forward_test(model_id):
    """
    Execute forward test for a single model
    """
    try:
        model = ActiveForwardTestModel.objects.get(id=model_id)
        
        if not model.is_active:
            print(f"Model {model.name} is inactive, skipping")
            return
        
        # Get market data using yfinance
        dataset = get_market_data(model.asset, model.interval)
        
        if dataset is None or len(dataset) == 0:
            print(f"No data available for {model.asset}")
            return
        
        print(f" Got {len(dataset)} candles for {model.name} ({model.asset})")
        
        # Check if we have open positions
        open_positions = Position.objects.filter(model=model, is_open=True)
        print(f"Open positions: {len(open_positions)}")
        
        # Always check TP/SL for existing positions first
        if len(open_positions) > 0:
            check_and_close_positions(model, open_positions)
            # Refresh open positions after potential closes
            open_positions = Position.objects.filter(model=model, is_open=True)
        
        # Prepare namespace for code execution
        namespace = prepare_namespace(model, dataset)
        
        # Execute the model code
        try:
            exec(model.model_code, namespace)
            
            # Check if model code called position.close()
            position_proxy = namespace.get('position')
            if position_proxy and position_proxy.should_close_position():
                # Close all open positions
                current_price = dataset['Close'].iloc[-1]
                for pos in open_positions:
                    pos.close_position(current_price)
                    print(f"Closed position via position.close() for {model.name} at {current_price}")
                
                # Refresh open positions after closes
                open_positions = Position.objects.filter(model=model, is_open=True)
            
            # Now check if we should open a new position
            return_statement = namespace.get('return_statement', None)
            print(f"Return statement: {return_statement}")
            
            if return_statement in ['buy', 'sell'] and len(open_positions) < model.num_positions:
                # Open new position
                current_price = dataset['Close'].iloc[-1]
                # position_size = model.current_equity / current_price / model.num_positions
                # Calculate position size based on risk per trade
                risk_per_trade = 0.02  # Risk 2% of equity per trade
                risk_amount = model.current_equity * risk_per_trade
                
                # Calculate distance to stop loss in dollars
                if return_statement == 'buy':
                    sl_distance = current_price - sl_price
                else:  # sell
                    sl_distance = sl_price - current_price
                
                # Position size = Risk amount / Stop loss distance
                if sl_distance > 0:
                    position_size = risk_amount / sl_distance
                else:
                    # Fallback if SL distance is 0 or negative
                    position_size = (model.current_equity * 0.05) / current_price  # Use 5% of equity
                
                print(f" OPENING {return_statement.upper()} for {model.name} at ${current_price}")
                
                # Get TP/SL from namespace (in case set_take_profit/set_stop_loss were called)
                tp_value = namespace.get('_take_profit', model.take_profit)
                tp_type = namespace.get('_take_profit_type', model.take_profit_type)
                sl_value = namespace.get('_stop_loss', model.stop_loss)
                sl_type = namespace.get('_stop_loss_type', model.stop_loss_type)
                
                # Calculate TP/SL prices
                tp_price, sl_price = calculate_tp_sl(
                    current_price,
                    return_statement,
                    tp_value,
                    tp_type,
                    sl_value,
                    sl_type
                )
                
                position = Position.objects.create(
                    model=model,
                    position_type=return_statement.upper(),
                    entry_price=current_price,
                    size=position_size,
                    take_profit_price=tp_price,
                    stop_loss_price=sl_price,
                )
                
                model.last_run = timezone.now()
                model.save()
                
                print(f" Opened {return_statement} position for {model.name} at {current_price}")
            elif len(open_positions) >= model.num_positions:
                print(f"Max positions reached for {model.name}")
        
        except Exception as e:
            print(f"Error executing model code for {model.name}: {e}")
            import traceback
            traceback.print_exc()
    
    except Exception as e:
        print(f"Error in execute_forward_test: {e}")
        import traceback
        traceback.print_exc()

@csrf_exempt
@require_http_methods(["GET", "POST"])
def snowai_models_list(request):
    """List all active forward test models or create new one"""
    
    if request.method == 'GET':
        models = ActiveForwardTestModel.objects.all()
        data = []
        
        for model in models:
            positions = Position.objects.filter(model=model)
            
            data.append({
                'id': model.id,
                'name': model.name,
                'asset': model.asset,
                'interval': model.interval,
                'is_active': model.is_active,
                'initial_equity': model.initial_equity,
                'current_equity': model.current_equity,
                'total_trades': model.total_trades,
                'winning_trades': model.winning_trades,
                'losing_trades': model.losing_trades,
                'total_pnl': model.total_pnl,
                'win_rate': model.win_rate,
                'created_at': model.created_at.isoformat(),
                'last_run': model.last_run.isoformat() if model.last_run else None,
            })
        
        return JsonResponse(data, safe=False)
    
    elif request.method == 'POST':
        try:
            data = json.loads(request.body)
            
            model = ActiveForwardTestModel.objects.create(
                name=data['name'],
                asset=data['asset'],
                interval=data['interval'],
                model_code=data['model_code'],
                initial_equity=data.get('initial_equity', 10000),
                current_equity=data.get('initial_equity', 10000),
                num_positions=data.get('num_positions', 1),
                take_profit=data.get('take_profit', 5),
                take_profit_type=data.get('take_profit_type', 'PERCENTAGE'),
                stop_loss=data.get('stop_loss', 3),
                stop_loss_type=data.get('stop_loss_type', 'PERCENTAGE'),
            )
            
            # Initialize equity curve
            model.equity_curve = json.dumps([model.initial_equity])
            model.save()
            
            # No need to add scheduler job - execute_all_forward_tests handles all models
            print(f"Created model {model.id}, will be picked up by scheduler automatically")
            
            return JsonResponse({
                'id': model.id,
                'message': 'Model created successfully'
            })
        
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=400)


@csrf_exempt
@require_http_methods(["GET", "PUT", "DELETE"])
def snowai_model_detail(request, model_id):
    """Get model details, update model, or delete model"""
    
    try:
        model = ActiveForwardTestModel.objects.get(id=model_id)
    except ActiveForwardTestModel.DoesNotExist:
        return JsonResponse({'error': 'Model not found'}, status=404)
    
    if request.method == 'GET':
        positions = Position.objects.filter(model=model)
        
        positions_data = [{
            'type': pos.position_type,
            'entry_price': pos.entry_price,
            'exit_price': pos.exit_price,
            'entry_time': pos.entry_time.isoformat(),
            'exit_time': pos.exit_time.isoformat() if pos.exit_time else None,
            'pnl': pos.pnl,
            'is_open': pos.is_open,
            'take_profit_price': pos.take_profit_price,
            'stop_loss_price': pos.stop_loss_price,
        } for pos in positions]
        
        return JsonResponse({
            'id': model.id,
            'name': model.name,
            'asset': model.asset,
            'interval': model.interval,
            'model_code': model.model_code,
            'is_active': model.is_active,
            'initial_equity': model.initial_equity,
            'current_equity': model.current_equity,
            'num_positions': model.num_positions,
            'take_profit': model.take_profit,
            'take_profit_type': model.take_profit_type,
            'stop_loss': model.stop_loss,
            'stop_loss_type': model.stop_loss_type,
            'total_trades': model.total_trades,
            'winning_trades': model.winning_trades,
            'losing_trades': model.losing_trades,
            'total_pnl': model.total_pnl,
            'win_rate': model.win_rate,
            'equity_curve': json.loads(model.equity_curve),
            'positions': positions_data,
            'created_at': model.created_at.isoformat(),
        })
    
    elif request.method == 'PUT':
        try:
            data = json.loads(request.body)
            
            # Update model fields
            model.name = data.get('name', model.name)
            model.asset = data.get('asset', model.asset)
            model.interval = data.get('interval', model.interval)
            model.model_code = data.get('model_code', model.model_code)
            model.initial_equity = data.get('initial_equity', model.initial_equity)
            model.num_positions = data.get('num_positions', model.num_positions)
            model.take_profit = data.get('take_profit', model.take_profit)
            model.take_profit_type = data.get('take_profit_type', model.take_profit_type)
            model.stop_loss = data.get('stop_loss', model.stop_loss)
            model.stop_loss_type = data.get('stop_loss_type', model.stop_loss_type)
            model.is_active = data.get('is_active', model.is_active)
            
            model.save()
            
            return JsonResponse({
                'id': model.id,
                'message': 'Model updated successfully'
            })
        
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=400)
    
    elif request.method == 'DELETE':
        # Stop scheduler job if exists
        try:
            from apscheduler.schedulers.background import BackgroundScheduler
            scheduler = BackgroundScheduler()
            scheduler.remove_job(f'model_{model_id}')
        except:
            pass
        
        model.delete()
        return JsonResponse({'message': 'Model deleted successfully'})


@csrf_exempt
def snowai_available_models(request):
    """Get all available model codes from SnowAIForwardTestingModel"""
    models = SnowAIForwardTestingModel.objects.all()
    
    data = [{
        'model_id': model.model_id,
        'cleaned_model_code': model.cleaned_model_code,
        'created_at': model.created_at.isoformat(),
        'notes': model.notes,
    } for model in models]
    
    return JsonResponse(data, safe=False)


@csrf_exempt
def snowai_fetch_chart_data_with_positions_endpoint(request, model_id):
    """
    Fetch yfinance price data and overlay positions for charting
    This has a unique name to avoid conflicts with other yfinance fetching functions
    """
    try:
        model = ActiveForwardTestModel.objects.get(id=model_id)
        positions = Position.objects.filter(model=model).order_by('entry_time')
        
        # Fetch price data from yfinance
        ticker = yf.Ticker(model.asset)
        
        # Determine period based on interval
        if model.interval in ['5m', '15m']:
            period = '7d'
        elif model.interval in ['1h', '4h']:
            period = '1mo'
        elif model.interval in ['1d']:
            period = '3mo'
        elif model.interval in ['1wk']:
            period = '1y'
        else:
            period = '3mo'
        
        df = ticker.history(period=period, interval=model.interval)
        
        if len(df) == 0:
            return JsonResponse({'error': 'No price data available'}, status=404)
        
        # Prepare chart data with OHLC
        chart_data = []
        for idx, row in df.iterrows():
            data_point = {
                'time': idx.strftime('%Y-%m-%d %H:%M') if model.interval in ['5m', '15m', '1h', '4h'] else idx.strftime('%Y-%m-%d'),
                'open': float(row['Open']),
                'high': float(row['High']),
                'low': float(row['Low']),
                'close': float(row['Close']),
                'volume': float(row['Volume']),
            }
            
            # Check if any positions were entered at this time
            for pos in positions:
                entry_time_str = pos.entry_time.strftime('%Y-%m-%d %H:%M') if model.interval in ['5m', '15m', '1h', '4h'] else pos.entry_time.strftime('%Y-%m-%d')
                
                if entry_time_str == data_point['time']:
                    if pos.position_type == 'BUY':
                        data_point['buy_entry'] = pos.entry_price
                    else:
                        data_point['sell_entry'] = pos.entry_price
                
                # Check if position was exited at this time
                if pos.exit_time:
                    exit_time_str = pos.exit_time.strftime('%Y-%m-%d %H:%M') if model.interval in ['5m', '15m', '1h', '4h'] else pos.exit_time.strftime('%Y-%m-%d')
                    
                    if exit_time_str == data_point['time']:
                        if pos.position_type == 'BUY':
                            data_point['buy_exit'] = pos.exit_price
                        else:
                            data_point['sell_exit'] = pos.exit_price
            
            chart_data.append(data_point)
        
        return JsonResponse(chart_data, safe=False)
    
    except ActiveForwardTestModel.DoesNotExist:
        return JsonResponse({'error': 'Model not found'}, status=404)
    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


def check_and_close_positions(model, open_positions):
    """Check if TP or SL has been hit and close positions"""
    dataset = get_market_data(model.asset, model.interval)
    
    if dataset is None:
        return
    
    current_price = dataset['Close'].iloc[-1]
    
    for position in open_positions:
        should_close = False
        
        if position.position_type == 'BUY':
            if current_price >= position.take_profit_price:
                should_close = True
                print(f"TP hit for {model.name} at {current_price}")
            elif current_price <= position.stop_loss_price:
                should_close = True
                print(f"SL hit for {model.name} at {current_price}")
        
        else:  # SELL (short)
            if current_price <= position.take_profit_price:
                should_close = True
                print(f"TP hit for {model.name} at {current_price}")
            elif current_price >= position.stop_loss_price:
                should_close = True
                print(f"SL hit for {model.name} at {current_price}")
        
        if should_close:
            position.close_position(current_price)


def get_market_data(asset, interval):
    """Fetch market data using yfinance"""
    try:
        ticker = yf.Ticker(asset)
        
        # Determine period based on interval
        if interval in ['5m', '15m']:
            period = '7d'
        elif interval in ['1h', '4h']:
            period = '1mo'
        elif interval in ['1d']:
            period = '1y'
        elif interval in ['1wk']:
            period = '5y'
        else:
            period = '1y'
        
        df = ticker.history(period=period, interval=interval)
        
        if len(df) == 0:
            return None
        
        return df
    
    except Exception as e:
        print(f"Error fetching data for {asset}: {e}")
        return None


def calculate_tp_sl(entry_price, position_type, tp_value, tp_type, sl_value, sl_type):
    """Calculate take profit and stop loss prices"""
    
    if position_type == 'buy':
        if tp_type == 'PERCENTAGE':
            tp_price = entry_price * (1 + tp_value / 100)
        else:
            tp_price = entry_price + tp_value
        
        if sl_type == 'PERCENTAGE':
            sl_price = entry_price * (1 - sl_value / 100)
        else:
            sl_price = entry_price - sl_value
    
    else:  # sell (short)
        if tp_type == 'PERCENTAGE':
            tp_price = entry_price * (1 - tp_value / 100)
        else:
            tp_price = entry_price - tp_value
        
        if sl_type == 'PERCENTAGE':
            sl_price = entry_price * (1 + sl_value / 100)
        else:
            sl_price = entry_price + sl_value
    
    return tp_price, sl_price
    

def prepare_namespace(model, dataset):
    """Prepare namespace with all trading functions"""
    
    # Create a position proxy object that can be called from model code
    class PositionProxy:
        def __init__(self, model):
            self.model = model
            self._should_close = False
        
        def close(self):
            """Mark that position should be closed"""
            self._should_close = True
        
        def should_close_position(self):
            return self._should_close
    
    position_proxy = PositionProxy(model)
    
    # TP/SL functions that can be called from model code
    def set_take_profit(number, type_of_setting='PERCENTAGE'):
        """Set take profit - stores in namespace for position creation"""
        namespace['_take_profit'] = number
        namespace['_take_profit_type'] = type_of_setting.upper()
    
    def set_stop_loss(number, type_of_setting='PERCENTAGE'):
        """Set stop loss - stores in namespace for position creation"""
        namespace['_stop_loss'] = number
        namespace['_stop_loss_type'] = type_of_setting.upper()

    current_open_positions = Position.objects.filter(model=model, is_open=True).count()

    
    namespace = {
        'interval': model.interval,
        'num_positions': current_open_positions,
        'dataset': dataset,
        'asset': model.asset,
        'position': position_proxy,
        'set_take_profit': set_take_profit,
        'set_stop_loss': set_stop_loss,
        '_take_profit': model.take_profit,
        '_take_profit_type': model.take_profit_type,
        '_stop_loss': model.stop_loss,
        '_stop_loss_type': model.stop_loss_type,
        # All your trading functions
        'is_support_level': is_support_level,
        'is_resistance_level': is_resistance_level,
        'is_uptrend': is_uptrend,
        'is_downtrend': is_downtrend,
        'is_ranging_market': is_ranging_market,
        'is_bullish_candle': is_bullish_candle,
        'is_bearish_candle': is_bearish_candle,
        'is_bullish_engulfing': is_bullish_engulfing,
        'is_bearish_engulfing': is_bearish_engulfing,
        'is_morning_star': is_morning_star,
        'is_evening_star': is_evening_star,
        'is_three_white_soldiers': is_three_white_soldiers,
        'is_three_black_crows': is_three_black_crows,
        'is_morning_doji_star': is_morning_doji_star,
        'is_evening_doji_star': is_evening_doji_star,
        'is_rising_three_methods': is_rising_three_methods,
        'is_falling_three_methods': is_falling_three_methods,
        'is_hammer': is_hammer,
        'is_hanging_man': is_hanging_man,
        'is_inverted_hammer': is_inverted_hammer,
        'is_shooting_star': is_shooting_star,
        'is_bullish_kicker': is_bullish_kicker,
        'is_bearish_kicker': is_bearish_kicker,
        'is_bullish_harami': is_bullish_harami,
        'is_bearish_harami': is_bearish_harami,
        'is_bullish_three_line_strike': is_bullish_three_line_strike,
        'is_bearish_three_line_strike': is_bearish_three_line_strike,
        'moving_average': moving_average,
        'bbands': bbands,
        'momentum': momentum,
        'rsi': rsi,
        'is_asian_range_buy': is_asian_range_buy,
        'is_asian_range_sell': is_asian_range_sell,
        'is_fibonacci_level': is_fibonacci_level,
        'is_ote_buy': is_ote_buy,
        'is_ote_sell': is_ote_sell,
        'is_bullish_orderblock': is_bullish_orderblock,
        'is_bearish_orderblock': is_bearish_orderblock,
        'is_bullish_weekly_profile': is_bullish_weekly_profile,
        'is_bearish_weekly_profile': is_bearish_weekly_profile,
        'buy_hold': buy_hold,
        'buy_hold_regime': buy_hold_regime,
        'is_bullish_bias': is_bullish_bias,
        'is_bearish_bias': is_bearish_bias,
        'is_high_volume': is_high_volume,
        'is_low_volume': is_low_volume,
        'is_stable_market': is_stable_market,
        'is_choppy_market': is_choppy_market,
        'is_volatile_market': is_volatile_market,
        'new_york_session': new_york_session,
        'london_session': london_session,
        'asian_session': asian_session,
        'snow_alpha_buy': snow_alpha_buy,
        'snow_alpha_short': snow_alpha_short,
        'ice_beta_buy': ice_beta_buy,
        'ice_beta_short': ice_beta_short,
        'frost_gamma_buy': frost_gamma_buy,
        'frost_gamma_short': frost_gamma_short,
        'glacier_x_buy': glacier_x_buy,
        'glacier_x_short': glacier_x_short,
        'avalanche_z_buy': avalanche_z_buy,
        'avalanche_z_short': avalanche_z_short,
        'polar_prime_buy': polar_prime_buy,
        'polar_prime_short': polar_prime_short,
        'blizzard_omega_buy': blizzard_omega_buy,
        'blizzard_omega_short': blizzard_omega_short,
        'tundra_sigma_buy': tundra_sigma_buy,
        'tundra_sigma_short': tundra_sigma_short,
        'arctic_delta_buy': arctic_delta_buy,
        'arctic_delta_short': arctic_delta_short,
        'permafrost_theta_buy': permafrost_theta_buy,
        'permafrost_theta_short': permafrost_theta_short,
    }
    
    return namespace
    

scheduler.add_job(
    execute_all_forward_tests,
    trigger=IntervalTrigger(minutes=1),
    id='snowai_forward_test_job',
    name='Execute all SnowAI forward tests every minute',
    replace_existing=True
)


# ============================================================================
# DEBUG ENDPOINT: Browser-Friendly GET Request
# ============================================================================
@csrf_exempt
@require_http_methods(["GET"])
def snowai_debug_predictions_simple(request):
    """
    DEBUG: Test all agents on sample data (no request body needed)
    GET /api/snowai-trading-weights/debug-predictions-simple/
    
    Just open this URL in your browser!
    """
    try:
        # Sample uptrending data (like a stock going up)
        sample_data = [
            {"close": 150.0, "open": 149.5, "high": 151.0, "low": 149.0, "timestamp": 1},
            {"close": 151.0, "open": 150.0, "high": 152.0, "low": 149.5, "timestamp": 2},
            {"close": 150.5, "open": 151.0, "high": 151.5, "low": 150.0, "timestamp": 3},
            {"close": 152.0, "open": 150.5, "high": 153.0, "low": 150.0, "timestamp": 4},
            {"close": 151.5, "open": 152.0, "high": 152.5, "low": 151.0, "timestamp": 5},
            {"close": 153.0, "open": 151.5, "high": 154.0, "low": 151.0, "timestamp": 6},
            {"close": 152.5, "open": 153.0, "high": 153.5, "low": 152.0, "timestamp": 7},
            {"close": 154.0, "open": 152.5, "high": 155.0, "low": 152.0, "timestamp": 8},
            {"close": 153.5, "open": 154.0, "high": 154.5, "low": 153.0, "timestamp": 9},
            {"close": 155.0, "open": 153.5, "high": 156.0, "low": 153.0, "timestamp": 10},
            {"close": 154.5, "open": 155.0, "high": 155.5, "low": 154.0, "timestamp": 11},
            {"close": 156.0, "open": 154.5, "high": 157.0, "low": 154.0, "timestamp": 12},
            {"close": 155.5, "open": 156.0, "high": 156.5, "low": 155.0, "timestamp": 13},
            {"close": 157.0, "open": 155.5, "high": 158.0, "low": 155.0, "timestamp": 14},
            {"close": 156.5, "open": 157.0, "high": 157.5, "low": 156.0, "timestamp": 15},
            {"close": 158.0, "open": 156.5, "high": 159.0, "low": 156.0, "timestamp": 16},
            {"close": 157.5, "open": 158.0, "high": 158.5, "low": 157.0, "timestamp": 17},
            {"close": 159.0, "open": 157.5, "high": 160.0, "low": 157.0, "timestamp": 18},
            {"close": 158.5, "open": 159.0, "high": 159.5, "low": 158.0, "timestamp": 19},
            {"close": 160.0, "open": 158.5, "high": 161.0, "low": 158.0, "timestamp": 20},
            {"close": 161.5, "open": 160.0, "high": 162.0, "low": 159.5, "timestamp": 21},
            {"close": 162.0, "open": 161.5, "high": 163.0, "low": 161.0, "timestamp": 22},
            {"close": 163.5, "open": 162.0, "high": 164.0, "low": 161.5, "timestamp": 23},
            {"close": 164.0, "open": 163.5, "high": 165.0, "low": 163.0, "timestamp": 24},
            {"close": 165.5, "open": 164.0, "high": 166.0, "low": 163.5, "timestamp": 25},
            {"close": 166.0, "open": 165.5, "high": 167.0, "low": 165.0, "timestamp": 26},
            {"close": 167.5, "open": 166.0, "high": 168.0, "low": 165.5, "timestamp": 27},
            {"close": 168.0, "open": 167.5, "high": 169.0, "low": 167.0, "timestamp": 28},
            {"close": 169.5, "open": 168.0, "high": 170.0, "low": 167.5, "timestamp": 29},
            {"close": 170.0, "open": 169.5, "high": 171.0, "low": 169.0, "timestamp": 30},
            {"close": 171.5, "open": 170.0, "high": 172.0, "low": 169.5, "timestamp": 31},
            {"close": 172.0, "open": 171.5, "high": 173.0, "low": 171.0, "timestamp": 32},
            {"close": 173.5, "open": 172.0, "high": 174.0, "low": 171.5, "timestamp": 33},
            {"close": 174.0, "open": 173.5, "high": 175.0, "low": 173.0, "timestamp": 34},
            {"close": 175.5, "open": 174.0, "high": 176.0, "low": 173.5, "timestamp": 35},
            {"close": 176.0, "open": 175.5, "high": 177.0, "low": 175.0, "timestamp": 36},
            {"close": 177.5, "open": 176.0, "high": 178.0, "low": 175.5, "timestamp": 37},
            {"close": 178.0, "open": 177.5, "high": 179.0, "low": 177.0, "timestamp": 38},
            {"close": 179.5, "open": 178.0, "high": 180.0, "low": 177.5, "timestamp": 39},
            {"close": 180.0, "open": 179.5, "high": 181.0, "low": 179.0, "timestamp": 40},
            {"close": 181.5, "open": 180.0, "high": 182.0, "low": 179.5, "timestamp": 41},
            {"close": 182.0, "open": 181.5, "high": 183.0, "low": 181.0, "timestamp": 42},
            {"close": 183.5, "open": 182.0, "high": 184.0, "low": 181.5, "timestamp": 43},
            {"close": 184.0, "open": 183.5, "high": 185.0, "low": 183.0, "timestamp": 44},
            {"close": 185.5, "open": 184.0, "high": 186.0, "low": 183.5, "timestamp": 45},
            {"close": 186.0, "open": 185.5, "high": 187.0, "low": 185.0, "timestamp": 46},
            {"close": 187.5, "open": 186.0, "high": 188.0, "low": 185.5, "timestamp": 47},
            {"close": 188.0, "open": 187.5, "high": 189.0, "low": 187.0, "timestamp": 48},
            {"close": 189.5, "open": 188.0, "high": 190.0, "low": 187.5, "timestamp": 49},
            {"close": 190.0, "open": 189.5, "high": 191.0, "low": 189.0, "timestamp": 50},
        ]
        
        current_position = 'none'
        position_size = 0.0
        unrealized_pnl = 0.0
        
        # Test all agents
        agents_to_test = [
            ('Snow-Alpha', 'standard', 'balanced'),
            ('Ice-Beta', 'short-term', 'scalper'),
            ('Frost-Gamma', 'long-term', 'trend'),
            ('Glacier-X', 'momentum', 'momentum'),
            ('Avalanche-Z', 'volatility', 'aggressive'),
            ('Polar-Prime', 'risk-aware', 'conservative'),
            ('Blizzard-Omega', 'momentum', 'aggressive'),
            ('Tundra-Sigma', 'mean-reversion', 'balanced'),
            ('Arctic-Delta', 'volatility', 'aggressive'),
            ('Permafrost-Theta', 'contrarian', 'contrarian')
        ]
        
        results = []
        
        for agent_name, state_features, personality in agents_to_test:
            # Load weights
            from asgiref.sync import sync_to_async
            
            def get_weights(name):
                try:
                    from .models import SnowAITradingWeights
                    obj = SnowAITradingWeights.objects.get(snow_agent_name=name)
                    return obj.snow_weights_data
                except Exception as e:
                    logger.error(f"Error loading weights for {name}: {e}")
                    return None
            
            weights = get_weights(agent_name)
            
            if not weights:
                results.append({
                    'agent': agent_name,
                    'error': 'No weights found in database'
                })
                continue
            
            # Calculate features
            inputs = calculate_model_inputs(
                sample_data,
                state_features=state_features,
                position_size=position_size,
                unrealized_pnl=unrealized_pnl
            )
            
            # Get Q-values
            q_values = forward_pass(inputs, weights)
            
            # Get action (no personality bias)
            action_no_bias = get_best_action(q_values, current_position, 'balanced')
            
            # Get action (with personality bias)
            action_with_bias = get_best_action(q_values, current_position, personality)
            
            action_names = ['BUY', 'HOLD', 'SELL', 'SHORT', 'COVER']
            
            # Find which Q-value is highest
            max_q = max(q_values)
            max_q_index = q_values.index(max_q)
            
            results.append({
                'agent': agent_name,
                'state_features': state_features,
                'personality': personality,
                'inputs': [round(x, 4) for x in inputs],
                'q_values': [round(q, 4) for q in q_values],
                'q_values_labeled': {
                    action_names[i]: round(q, 4) 
                    for i, q in enumerate(q_values)
                },
                'highest_q_action': action_names[max_q_index],
                'highest_q_value': round(max_q, 4),
                'action_no_bias': action_names[action_no_bias],
                'action_with_bias': action_names[action_with_bias],
                'weights_sample': {
                    'w1_first_row_first_5': [round(x, 4) for x in weights['w1'][0][:5]],
                    'b3_output_biases': [round(x, 4) for x in weights['b3']]
                }
            })
        
        # Summary statistics
        unique_inputs = len(set(str(r['inputs']) for r in results if 'inputs' in r))
        unique_q_values = len(set(str(r['q_values']) for r in results if 'q_values' in r))
        unique_actions_no_bias = len(set(r['action_no_bias'] for r in results if 'action_no_bias' in r))
        unique_actions_with_bias = len(set(r['action_with_bias'] for r in results if 'action_with_bias' in r))
        
        return JsonResponse({
            'success': True,
            'scenario': 'Strong uptrend (150 -> 190 over 50 candles)',
            'current_price': sample_data[-1]['close'],
            'price_change': f"+{((sample_data[-1]['close'] - sample_data[0]['close']) / sample_data[0]['close'] * 100):.2f}%",
            'summary': {
                'agents_tested': len(agents_to_test),
                'agents_with_weights': len([r for r in results if 'error' not in r]),
                'unique_input_combinations': unique_inputs,
                'unique_q_value_combinations': unique_q_values,
                'unique_actions_without_bias': unique_actions_no_bias,
                'unique_actions_with_bias': unique_actions_with_bias,
                'diagnosis': {
                    'inputs_different': unique_inputs > 1,
                    'q_values_different': unique_q_values > 1,
                    'actions_different_without_bias': unique_actions_no_bias > 1,
                    'actions_different_with_bias': unique_actions_with_bias > 1
                }
            },
            'results': results
        }, json_dumps_params={'indent': 2})
        
    except Exception as e:
        import traceback
        return JsonResponse({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }, status=500, json_dumps_params={'indent': 2})


@csrf_exempt
@require_http_methods(["GET"])
def snowai_debug_weight_shapes(request):
    """
    DEBUG: Check the shapes of stored weights
    GET /api/snowai-trading-weights/debug-weight-shapes/
    """
    try:
        from .models import SnowAITradingWeights
        import numpy as np
        
        # Get one agent's weights
        obj = SnowAITradingWeights.objects.first()
        
        if not obj:
            return JsonResponse({
                'success': False,
                'error': 'No weights found in database'
            })
        
        weights = obj.snow_weights_data
        
        # Get actual shapes
        w1_shape = np.array(weights['w1']).shape
        b1_shape = np.array(weights['b1']).shape
        w2_shape = np.array(weights['w2']).shape
        b2_shape = np.array(weights['b2']).shape
        w3_shape = np.array(weights['w3']).shape
        b3_shape = np.array(weights['b3']).shape
        
        # Sample values
        w1_sample = np.array(weights['w1'])
        
        return JsonResponse({
            'success': True,
            'agent': obj.snow_agent_name,
            'actual_shapes': {
                'w1': list(w1_shape),
                'b1': list(b1_shape),
                'w2': list(w2_shape),
                'b2': list(b2_shape),
                'w3': list(w3_shape),
                'b3': list(b3_shape),
            },
            'w1_is_2d': len(w1_shape) == 2,
            'w1_first_dimension': int(w1_shape[0]),
            'w1_second_dimension': int(w1_shape[1]) if len(w1_shape) > 1 else None,
            'w1_first_row_first_5': [float(x) for x in w1_sample[0][:5]] if len(w1_shape) == 2 else None,
            'expected_shapes_if_input_first': {
                'w1': '[7, 32] means [input_size, hidden1_size]',
                'w2': '[32, 16] means [hidden1_size, hidden2_size]',
                'w3': '[16, 5] means [hidden2_size, output_size]'
            },
            'expected_shapes_if_hidden_first': {
                'w1': '[32, 7] means [hidden1_size, input_size] - needs transpose',
                'w2': '[16, 32] means [hidden2_size, hidden1_size] - needs transpose',
                'w3': '[5, 16] means [output_size, hidden2_size] - needs transpose'
            }
        }, json_dumps_params={'indent': 2})
        
    except Exception as e:
        import traceback
        return JsonResponse({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }, status=500)
        
    
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression


def _calculate_trend_metrics(data):
    """
    Internal function to calculate trend characteristics
    
    Args:
        data (pd.DataFrame): DataFrame with 'Close' column and datetime index
        
    Returns:
        dict: Trend metrics or None if calculation fails
    """
    try:
        if len(data) < 20:
            return None
        
        # Calculate returns
        data_copy = data.copy()
        data_copy['returns'] = data_copy['Close'].pct_change()
        
        # Linear regression for trend
        prices = data_copy['Close'].values
        X = np.arange(len(prices)).reshape(-1, 1)
        y = prices.reshape(-1, 1)
        
        model = LinearRegression()
        model.fit(X, y)
        
        slope = model.coef_[0][0]
        r_squared = model.score(X, y)
        
        # Calculate trend strength (normalized slope)
        avg_price = np.mean(prices)
        trend_strength = (slope * len(prices)) / avg_price if avg_price != 0 else 0
        
        # Calculate directional consistency
        returns = data_copy['returns'].dropna()
        if len(returns) > 0:
            positive_ratio = (returns > 0).sum() / len(returns)
        else:
            positive_ratio = 0.5
        
        # Price momentum (current vs start)
        price_change_pct = (prices[-1] - prices[0]) / prices[0] if prices[0] != 0 else 0
        
        # Calculate moving average position
        if len(data_copy) >= 20:
            ma_20 = data_copy['Close'].rolling(window=20).mean().iloc[-1]
            current_price = data_copy['Close'].iloc[-1]
            ma_position = (current_price - ma_20) / ma_20 if ma_20 != 0 else 0
        else:
            ma_position = 0
        
        return {
            'slope': slope,
            'r_squared': r_squared,
            'trend_strength': trend_strength,
            'positive_ratio': positive_ratio,
            'price_change_pct': price_change_pct,
            'ma_position': ma_position
        }
        
    except Exception as e:
        return None


def is_uptrend(data):
    """
    Check if market is in an uptrend
    
    Args:
        data (pd.DataFrame): DataFrame with 'Close' column and datetime index
        
    Returns:
        bool: True if uptrend detected
    """
    try:
        metrics = _calculate_trend_metrics(data)
        if metrics is None:
            return False
        
        # Uptrend conditions
        is_uptrend_detected = (
            metrics['slope'] > 0.0005 and
            metrics['r_squared'] > 0.5 and
            metrics['positive_ratio'] > 0.60 and
            metrics['price_change_pct'] > 0.02 and
            metrics['ma_position'] > -0.02
        )
        
        return is_uptrend_detected
        
    except Exception:
        return False


def is_downtrend(data):
    """
    Check if market is in a downtrend
    
    Args:
        data (pd.DataFrame): DataFrame with 'Close' column and datetime index
        
    Returns:
        bool: True if downtrend detected
    """
    try:
        metrics = _calculate_trend_metrics(data)
        if metrics is None:
            return False
        
        # Downtrend conditions
        is_downtrend_detected = (
            metrics['slope'] < -0.0005 and
            metrics['r_squared'] > 0.5 and
            metrics['positive_ratio'] < 0.40 and
            metrics['price_change_pct'] < -0.02 and
            metrics['ma_position'] < 0.02
        )
        
        return is_downtrend_detected
        
    except Exception:
        return False


def is_ranging_market(data):
    """
    Check if market is ranging (sideways, no clear trend)
    
    Args:
        data (pd.DataFrame): DataFrame with 'Close' column and datetime index
        
    Returns:
        bool: True if ranging market detected
    """
    try:
        metrics = _calculate_trend_metrics(data)
        if metrics is None:
            return False
        
        # Ranging conditions: weak trend, low directional bias
        is_ranging_detected = (
            abs(metrics['trend_strength']) < 0.03 and
            (metrics['r_squared'] < 0.5 or
             (0.45 <= metrics['positive_ratio'] <= 0.55))
        )
        
        return is_ranging_detected
        
    except Exception:
        return False


# Your stock universe
STOCK_UNIVERSE = [
    # Tech Giants
    'AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN', 'NVDA', 
    'TSLA', 'META', 'AMD', 'INTC', 'ORCL', 'CSCO',
    'ADBE', 'CRM', 'AVGO', 'QCOM', 'TXN', 'AMAT',
    'LRCX', 'KLAC', 'SNPS', 'CDNS', 'MRVL', 'NXPI',
    
    # Financial Services
    'JPM', 'BAC', 'WFC', 'C', 'GS', 'MS', 'BLK',
    'SCHW', 'AXP', 'SPGI', 'CME', 'ICE', 'MCO',
    'BK', 'USB', 'PNC', 'TFC', 'COF',
    
    # Healthcare & Pharma
    'JNJ', 'LLY', 'UNH', 'PFE', 'ABBV', 'MRK', 'TMO',
    'ABT', 'DHR', 'BMY', 'AMGN', 'GILD', 'CVS',
    'CI', 'ELV', 'HUM', 'VRTX', 'REGN', 'ISRG',
    
    # Consumer Discretionary
    'HD', 'MCD', 'NKE', 'SBUX', 'TJX',
    'LOW', 'BKNG', 'MAR', 'CMG', 'F', 'GM', 'ABNB',
    
    # Consumer Staples
    'WMT', 'PG', 'KO', 'PEP', 'COST', 'PM', 'MO',
    'MDLZ', 'CL', 'KMB', 'GIS', 'KHC', 'STZ',
    
    # Energy
    'XOM', 'CVX', 'COP', 'EOG', 'SLB', 'MPC', 'PSX',
    'VLO', 'OXY', 'HAL', 'DVN', 'HES', 'BKR',
    
    # Industrials
    'BA', 'HON', 'UNP', 'CAT', 'GE', 'RTX', 'LMT',
    'UPS', 'DE', 'MMM', 'GD', 'NOC', 'FDX', 'CSX',
    
    # Telecom & Media
    'T', 'VZ', 'CMCSA', 'NFLX', 'DIS', 'TMUS', 'CHTR',
    
    # Payment Processors
    'V', 'MA', 'PYPL', 'ADP', 'FISV', 'FIS',
    
    # E-commerce & Retail
    'SHOP', 'MELI', 'EBAY', 'ETSY', 'TGT', 'ROST',
    
    # Software & Cloud
    'NOW', 'INTU', 'WDAY', 'PANW', 'CRWD', 'ZS',
    'DDOG', 'NET', 'SNOW', 'PLTR', 'TEAM',
    
    # Biotech
    'BIIB', 'MRNA', 'BNTX', 'SGEN', 'ALNY', 'BGNE',
    
    # Chinese ADRs
    'BABA', 'JD', 'PDD', 'BIDU', 'NIO', 'XPEV', 'LI',
    
    # EVs & Battery
    'RIVN', 'LCID', 'ALB', 'SQM',
]

# Model code templates
UPTREND_MODEL_CODE = """set_take_profit(number=4, type_of_setting='PERCENTAGE')
set_stop_loss(number=2, type_of_setting='PERCENTAGE')
if num_positions == 0:
    if is_stable_market(data=dataset, lookback_period=25):
        if buy_hold(dataset=dataset):
            if is_uptrend(data=dataset):
                return_statement = 'buy'"""

DOWNTREND_MODEL_CODE = """set_take_profit(number=4, type_of_setting='PERCENTAGE')
set_stop_loss(number=2, type_of_setting='PERCENTAGE')
if num_positions == 0:
    if is_stable_market(data=dataset, lookback_period=25):
        if sell_hold(dataset=dataset):
            if is_downtrend(data=dataset):
                return_statement = 'sell'"""


def scan_and_deploy_stocks():
    """
    Scan all stocks in universe for stability and trends
    Auto-create/remove forward testing models based on conditions
    """    
    # Check if NYSE is open using existing function
    if not new_york_session():
        print(f" NYSE is closed - skipping stock scan at {datetime.now()}")
        return
    
    print(f" Starting stock scan at {datetime.now()} (NYSE OPEN)")
    
    deployed_count = 0
    removed_count = 0
    
    for symbol in STOCK_UNIVERSE:
        try:
            # Fetch 4H data (same as what models will trade on)
            ticker = yf.Ticker(symbol)
            data = ticker.history(period='1mo', interval='4h')
            
            if len(data) < 50:
                print(f" Insufficient data for {symbol}")
                continue
            
            # Check if stock is stable using 4H data
            is_stable = is_stable_market(data=data, lookback_period=25)
            
            if not is_stable:
                # Check if we have an auto-deployed model for this stock and remove it
                remove_auto_model_if_exists(symbol, reason="no longer stable")
                continue
            
            # Stock is stable - check trend on 4H data
            is_up = is_uptrend(data=data)
            is_down = is_downtrend(data=data)
            
            if is_up:
                # Deploy uptrend model
                result = deploy_or_update_model(
                    symbol=symbol,
                    trend='uptrend',
                    model_code=UPTREND_MODEL_CODE
                )
                if result == 'deployed':
                    deployed_count += 1
                    print(f" Deployed UPTREND model for {symbol}")
                elif result == 'reactivated':
                    deployed_count += 1
                    print(f" Reactivated UPTREND model for {symbol}")
                elif result == 'updated':
                    print(f" Updated model for {symbol} to UPTREND")
                    
            elif is_down:
                # Deploy downtrend model
                result = deploy_or_update_model(
                    symbol=symbol,
                    trend='downtrend',
                    model_code=DOWNTREND_MODEL_CODE
                )
                if result == 'deployed':
                    deployed_count += 1
                    print(f" Deployed DOWNTREND model for {symbol}")
                elif result == 'reactivated':
                    deployed_count += 1
                    print(f" Reactivated DOWNTREND model for {symbol}")
                elif result == 'updated':
                    print(f" Updated model for {symbol} to DOWNTREND")
            else:
                # Stable but no clear trend - remove if exists
                remove_auto_model_if_exists(symbol, reason="no clear trend")
                
        except Exception as e:
            print(f" Error scanning {symbol}: {e}")
            continue
    
    print(f" Scan complete: {deployed_count} deployed, {removed_count} removed")


def deploy_or_update_model(symbol, trend, model_code):
    """
    Deploy a new model or update existing one if trend changed
    Reactivates paused models if conditions are met again
    
    Returns:
        str: 'deployed', 'updated', 'reactivated', or 'exists'
    """
    from .models import ActiveForwardTestModel
    
    # Check if auto-deployed model already exists (active or paused)
    existing = ActiveForwardTestModel.objects.filter(
        asset=symbol,
        name__startswith='[AUTO'
    ).first()
    
    if existing:
        # Reactivate if it was paused
        if not existing.is_active:
            existing.is_active = True
            existing.model_code = model_code
            existing.name = f"[AUTO] {symbol} - {trend.upper()}"
            existing.save()
            return 'reactivated'
        
        # Check if trend changed
        current_trend = 'uptrend' if 'is_uptrend' in existing.model_code else 'downtrend'
        
        if current_trend != trend:
            # Trend changed - update model code
            existing.model_code = model_code
            existing.name = f"[AUTO] {symbol} - {trend.upper()}"
            existing.save()
            return 'updated'
        else:
            # Same trend, model already exists
            return 'exists'
    else:
        # Create new model
        model = ActiveForwardTestModel.objects.create(
            name=f"[AUTO] {symbol} - {trend.upper()}",
            asset=symbol,
            interval='4h',
            model_code=model_code,
            initial_equity=10000.0,
            current_equity=10000.0,
            num_positions=1,
            take_profit=4.0,
            take_profit_type='PERCENTAGE',
            stop_loss=2.0,
            stop_loss_type='PERCENTAGE',
            is_active=True
        )
        
        # Initialize equity curve
        model.equity_curve = json.dumps([model.initial_equity])
        model.save()
        
        return 'deployed'


def remove_auto_model_if_exists(symbol, reason="conditions not met"):
    """
    Deactivate auto-deployed model if it exists (preserves trading history)
    """
    from .models import ActiveForwardTestModel, Position
    
    existing = ActiveForwardTestModel.objects.filter(
        asset=symbol,
        name__startswith='[AUTO]'
    ).first()
    
    if existing and existing.is_active:
        # Close any open positions first
        open_positions = Position.objects.filter(model=existing, is_open=True)
        
        if len(open_positions) > 0:
            # Get current price to close positions
            try:
                ticker = yf.Ticker(symbol)
                data = ticker.history(period='1d', interval='4h')
                if len(data) > 0:
                    current_price = data['Close'].iloc[-1]
                    
                    for pos in open_positions:
                        pos.close_position(current_price)
                        print(f" Closed position for {symbol} at ${current_price}")
            except:
                pass
        
        # Deactivate the model instead of deleting
        existing.is_active = False
        existing.name = f"[AUTO-PAUSED] {symbol} - {reason}"
        existing.save()
        print(f" Deactivated auto-model for {symbol}: {reason}")
        return True
    
    return False


def manual_trigger_stock_scan():
    """
    Manually trigger a stock scan (can be called via admin or API)
    """
    scan_and_deploy_stocks()
    return "Stock scan completed"


# Schedule the stock scanner to run every 10 minutes
scheduler.add_job(
    scan_and_deploy_stocks,
    trigger=IntervalTrigger(minutes=10),
    id='auto_stock_scanner_job',
    name='Scan stocks and auto-deploy trading models',
    replace_existing=True
    )


# LEGODI BACKEND CODE
def send_simple_message():
    # Replace with your Mailgun domain and API key
    domain = os.environ['MAILGUN_DOMAIN']
    api_key = os.environ['MAILGUN_API_KEY']

    # Mailgun API endpoint for sending messages
    url = f"https://api.mailgun.net/v3/{domain}/messages"

    # Email details
    sender = f"Excited User <postmaster@{domain}>"
    recipients = ["motingwetlotlo@yahoo.com"]
    subject = "Hello from Mailgun"
    text = "Testing some Mailgun awesomeness!"

    # Send the email
    response = requests.post(url, auth=("api", api_key), data={
        "from": sender,
        "to": recipients,
        "subject": subject,
        "text": text
    })

    # Return the response content as a JSON object
    return {
        "status_code": response.status_code,
        "response_content": response.content.decode("utf-8")
    }


def contact_us(request):
    if request.method == "POST":
        # Get form data from request body
        data = json.loads(request.body)
        first_name = data.get("firstName")
        last_name = data.get("lastName")
        email = data.get("email")
        message = data.get("message")
        
        # Save form data to the ContactUs model
        contact_us_entry = ContactUs.objects.create(
            first_name=first_name,
            last_name=last_name,
            email=email,
            message=message
        )
        return JsonResponse({"message": "Email sent successfully and saved to database!"})
    else:
        return JsonResponse({"error": "Method not allowed"}, status=405)


def book_order(request):
    if request.method == "POST":
        # Get form data from request body
        try:
            data = json.loads(request.body)
            first_name = data.get("first_name")
            last_name = data.get("last_name")
            email = data.get("email")
            interested_product = data.get("interested_product")
            number_of_units = int(data.get("number_of_units"))
            phone_number = data.get("phone_number")

            # Save form data to the BookOrder model
            book_order_entry = BookOrder.objects.create(
                first_name=first_name,
                last_name=last_name,
                email=email,
                interested_product=interested_product,
                phone_number=phone_number,
                number_of_units=number_of_units
            )
            return JsonResponse({"message": "Order booked successfully!"})
        except Exception as e:
            print(f'Exception occured: {e}')
            return JsonResponse({'error': str(e)})
    else:
        return JsonResponse({"error": "Method not allowed"}, status=405)

# Legodi Tech Registration and Login
from rest_framework import generics

class UserRegistrationView(generics.CreateAPIView):
    queryset = CustomUser.objects.all()
    serializer_class = CustomUserSerializer


def user_login(request):
    if request.method == 'POST':
        email = request.POST.get('email')
        password = request.POST.get('password')

        user = authenticate(request, username=email, password=password)
        if user:
            # User is authenticated
            login(request, user)
            # Generate and return an authentication token (e.g., JWT)
            return JsonResponse({'message': 'Login successful', 'token': 'your_token_here'})
        else:
            return JsonResponse({'message': 'Invalid credentials'}, status=400)
    else:
        return JsonResponse({'message': 'Invalid request method'}, status=405)


from django.middleware.csrf import get_token
from django.http import JsonResponse

def get_csrf_token(request):
    try:
        csrf_token = get_token(request)
        return JsonResponse({'csrfToken': csrf_token})
    except Exception as e:
        return JsonResponse({'error': str(e)})

